{
	"nodes":[
		{"id":"f6da8b1d468646d5","type":"text","text":"# Biometrics\n\nPer **biometrics** si intende la **capacità di riconoscere una persona dai suoi tratti somatici e/o comportamentali**, detti ***tratti biometrici*** (***TB***).\n\nÈ un'**alternativa all'autenticazione tramite oggetto** (e.g. \"Il tuo contatto è l'uomo con la valigetta nera\") **o  conoscenza** (e.g. \"Il tuo contatto conosce la password\"), ed è un buon **punto d'incontro tra facilità di utilizzo** (non richiede oggetti o memoria) **e precisione**.","x":-390,"y":860,"width":589,"height":261,"color":"6"},
		{"id":"efcdc18499da9576","type":"text","text":"# Premesse alla Biometrica: Classi e Pattern Recognition\n\nGli oggetti rappresentati nelle immagini possono essere raggruppati in diverse ***classi*** (e.g. fiori, volti, paesaggi), i cui elementi presentano caratteristiche uniche. Guardando questo insieme di caratteristiche, un osservatore è (idealmente) in grado di dire che \"questa immagine rappresenta un cane\".\n\nEntrando più nel dettaglio (i.e. osservando caratteristiche più complesse) potremmo addirittura scendere al livello delle ***sottoclassi***, e distinguere un pastore tedesco da un labrador.\n\nSe l'osservatore è un sistema informatico concettualmente cambia poco. La singola caratteristica che considero si chiama ***feature***, l'insieme delle caratteristiche utili a discriminare tra diverse classi si chiama ***feature vector*** (o ***pattern***), e il compito di riconoscere a quale (sotto)classe appartiene l'immagine in input viene eseguito da un modello di ***machine learning supervisionato*** (in ***classificazione***) detto ***pattern recognition***.\n\nDetto questo, ***la biometrica è un modello di pattern recognition in cui ogni individuo è una classe a sé stante***.\n\n\"Scusa, ma allora fa tutto l'algoritmo?\"\nSì, ma lo devi progettare. Ci sono tre questioni principali alle quali devi rispondere.\n\n- Pur vero che ***è il modello ad estrarre i pattern***, ***tu devi riconoscere quali dati grezzi è meglio fornirgli***.\n\t- Se provi ad identificare le persone dalla foto della spalla, il modello ce la metterà tutta ad estrarre i pattern migliori per discriminare tra le varie persone, ma se i dati grezzi non offrono nulla di significativo lui non troverà niente di significativo per discriminare, producendo pessime performance;\n\t- Questo include la ***diversificazione*** dei dati grezzi. Se ho scelto la faccia come dato grezzo ma ogni persona è rappresentata da una singola foto frontale con perfetta illuminazione e senza espressioni, il modello potrebbe avere qualche difficoltà se poi gli chiedo di riconoscere qualcuno che ride di profilo.\n- Devi anche ***metterlo nelle migliori condizioni di estrarre i pattern dai dati grezzi***.\n\t- Potrei fare una semplice Random Forest e lasciare tutto all'algoritmo. Oppure ha più senso dividere il programma in moduli e dedicarne uno al processo di ***features extraction***, il cui output viene poi dato in input del modulo che fa i calcoli sulle features.\n- Dati i pattern, devi dire al modello come discriminare tra di essi, cioè ***definire la metrica di valutazione***.\n- Dati i risultati del modello, sta a te ***definire la tolleranza*** di accettazione o rifiuto tramite delle threshold.\n\nTutto questo si applica a qualsiasi tipo di input, sia esso un'immagine, un audio, i dati di un accelerometro, ...","x":-554,"y":-200,"width":918,"height":839,"color":"4"},
		{"id":"4c0ac5a15b1f23ad","type":"text","text":"# Feature Extraction Module\n\nSe il FEM è al lavoro significa che ho già deciso quali sono i migliori dati grezzi da dare in pasto al modello (i.e. ho scelto di usare foto di volti, o fingerprint). A questo punto, a seconda di quello che ho scelto, esistono diversi metodi per l'estrazione delle features da quel particolare tipo di dati grezzi.\n\n- ***Algoritmi Classici*** - Utilizzano modelli geometrici e analisi matematica per estrarre le informazioni rilevanti;\n\t- Algoritmi come ***SIFT*** (**Scale-Invariant Feature Transform**) e ***SURF*** (**Speeded-Up Robust Features**) partono con un set di punti chiave di un oggetto (e.g. set di punti chiave di un occhio con relative distanze), cercano localmente nell'immagine delle zone interessanti a cui agganciare questi punti chiave (e.g. forti variazioni di intensità di colore dei pixel) e provano a fare una misura di match, restituendo tutte le ***porzioni locali*** di immagine in cui hanno fatto best match;\n\t- ***HOG*** (**Histogram of Oriented Gradients**) calcola il gradiente e restituisce un template che in pratica è l'***intera immagine*** con i contorni evidenziati;\n\t- Alcuni scelgono le features che meglio separano le classi (***riduzione dimensionale***). Esempi sono ***LDA*** (**Linear Discriminant Analysis**) e ***PCA*** (**Principal Component Analysis**).\n\t- Sempre utile la ***trasformata di Fourier*** per i segnali audio.\n- ***Algoritmi Statistici*** - Autoesplicativo, esempi sono ***LBP*** (**Local Binary Patterns**) per il riconoscimento facciale e la ***Wavelet Transform*** per la riduzione del rumore e la scomposizione dei segnali;\n- ***Machine Learning*** - Possono tornare utili algoritmi di ML classico come ***Random Forest*** e ***SVM*** (**Support Vector Machines**), soprattutto in caso di dati strutturati;\n- ***Neural Networks*** - Una NN che spara in output un FV. In questo caso ***non ho idea di cosa rappresentino i dati che estraggo***. E che mi importa? Che quando vado a fare il matching in teoria dovrei scegliere la metrica più adatta al tipo di dato in input, ma se non so qual è devo andare a tentativi.","x":-555,"y":3148,"width":918,"height":666,"color":"4"},
		{"id":"c3c219f1c9689029","type":"text","text":"# Schema Logico-Implementativo\n\nUna volta scelto il tipo dei dati grezzi, le fasi concettuali vengono generalmente implementate tramite 4 ***moduli***.\n\n- ***Dataset*** - I dati biometrici grezzi;\n- ***Feature Extraction Module*** - Dati i dati dei sensori, ne estrae le features per come previsto dal modello e le usa per costruire il feature vector (vero? o non necessariamente il template è un FV? o non sempre il FV è un vettore nel senso stretto del termine?);\n\t- serve compatibilità con i dati dei sensori? penso dipenda da come è scritto il codice... Di certo se il modello si aspetta la foto di un volto non ha senso dargli una foto di una macchina (proverà comunque a estrarne delle features per come è addestrato a fare, con risultati imprevedibili). Magari se lo si associa ad un sistema che riconosce i volti in una foto in cui questi sono nascosti...\n- ***Matching Module*** - Fa i calcoli e restituisce i match con i vari template. Questo può essere fatto con\n\t- ***Distanza*** - Se è \"piccola\" il match è \"buono\", e.g. distanza euclidea tra due FV;\n\t- ***Similarità*** - Più legata all'orientazione dei FV, in genere si calcola il coseno dell'angolo che formano. Contrariamente alla distanza, il match è \"buono\" se la similarità $\\to1$.\n- ***Decision Module*** - Dati i risultati del processo di matching, sceglie cosa farci sulla base delle politiche e delle threshold (che va scelta simulando, in base alla convenienza che in genere è minimizzare false acceptance rate\n\n\nquesti moduli funzionano in riconoscimento, in enrollment mi fermo al feature extraction?","x":-555,"y":2340,"width":918,"height":570,"color":"3"},
		{"id":"eed580368731a96b","type":"text","text":"# Schema Concettuale\n\nL'idea generale di un ***Biometric System*** (***BS***) è semplice, e consta di due semplici step.\n\n- ***Enrollment*** - ***Costruisce un DB*** catturando i dati biometrici grezzi (detti ***sample***, e.g. la foto del volto), estraendone le ***features*** (e.g. la distanza tra gli zigomi, dalla punta del naso al mento, ...) ed associando loro un'identità (e.g. \"Questo è Andrea\"). I ***template*** così ottenuti vengono raccolti nella ***gallery***.\n\t- Ovviamente è una fase preventiva all'azione, ma potrei continuare a raccogliere nuovi dati per il mio DB anche mentre sono in fase di riconoscimento.\n- ***Riconoscimento*** - Prendo il modello con il suo DB, gli fornisco gli opportuni dati grezzi in modo che lui possa estrarne le features (***probe***) e confrontarle con ciò che ha nel DB. Due scenari:\n\t- ***Verifica*** - L'utente dichiara di essere registrato. Il sistema confronta i suoi TB con quelli dell'utente che dice di essere (Controllo 1:1);\n\t\t- FaceID di iPhone è un ***identity claim*** implicito.\n\t- ***Identificazione*** - L'utente non dichiara niente, sta al sistema capire chi è. Per fare ciò deve confrontare i suoi TB con tutti quelli presenti nel DB (Controllo 1:N). A questo punto, anche qui, due scenari:\n\t\t- \"Questo tizio è sicuramente uno di quelli che hai nel DB\". Anche se la similarità è bassa, restituisco comunque il best match tra quelli che ho (***closed-set***), anche se così rischio l'errore;\n\t\t- \"Non è detto che tu sappia chi è\". Se ottengo una similarità bassa potrei scegliere di restituire un messaggio del tipo \"`Reject - Secondo me questo non sta nel DB`\" (***open-set***).\n\nTutto questo avviene sotto l'***assunzione che ogni persona sia unica***. Questo dipende dalle features. Se si usa il DNA è ovvio, ma in generale la fase di recognition non otterrà mai un match al $100\\%$ con la gallery, per motivi\n\n- ambientali, e.g. illuminazione e/o angolazione;\n- fisiologici, e.g. tengo la bocca aperta, rido, ...\n\nPer questo motivo, il modello deve prevedere una ***tolleranza*** entro cui gestire match multipli.","x":-555,"y":1360,"width":918,"height":718,"color":"4"},
		{"id":"8818849b9b5e0e2d","type":"text","text":"# Perché la Modularità? Perché non una NN?\n\nCosa mi impedisce di usare direttamente una NN in classificazione? Niente, se vuoi fallo.\n\nPerò devi rinunciare ad un sacco di cose:\n\n- Non sai quali features sta guardando la NN, quindi non ne hai il controllo;\n- Non è semplice ottimizzare la metrica che preferisci (e.g. FAR, FRR);\n- Non è generalizzabile (in quanto non modulare), cioè ti serve una NN per la verifica, una per l'identificazione, una per l'open-set, ...\n\nIn pratica, lasciare ad un unico monolitico modello di NN l'intero compito del riconoscimento biometrico è in generale una cattiva idea.","x":-1761,"y":1546,"width":760,"height":346,"color":"4"},
		{"id":"cd2095e4c31cfc60","type":"text","text":"# How To Features\n\nAnzitutto, cosa devo salvare nella gallery? Cioè, con quali criteri scelgo le features (i.e. i dati grezzi da cui poi il modello estrarrà delle buone features)?\n\nDevo scegliere ***TB in grado di identificare univocamente il generico essere umano***, pertanto devono essere\n\n- ***universali*** (i.e. posseduti da tutti, salvo rare eccezioni);\n- ***permanenti*** (i.e. che non possono cambiare nel tempo);\n- ***misurabili*** dai sensori (i.e. quantificabili in numeri);\n- la cui misura è ***accettabile*** dall'utente (i.e. che rispettino la privacy);\n- ***non-eludibili*** (i.e. non facilmente imitabili da artefatti).\n\nSimili TB sono detti ***strong features*** (***SF***), come ***impronte digitali***, ***retina*** e ***orecchie***.\n\nViceversa, un TB come il colore dei capelli è una ***weak feature*** (***WF***), in quanto non universale (impossibile distinguere tra le persone calve), non permanente (posso fare una tinta, o invecchiare, o usare una parrucca) e soprattutto non univoca.\n\nCi sono delle vie di mezzo? Ovviamente sì. I ***tratti comportamentali*** come ***gait*** (andamento della camminata), ***scrittura*** (in particolare la ***dinamica di movimento*** del polso e del braccio) e la ***voce*** sembrano da un lato una SF, ma dall'altro possono cambiare con l'età o con le condizioni fisiche e/o ambientali. Sicuramente sono ***più difficili da riprodurre*** da parte di un attaccante, e danno il meglio di sé quando usate in combinazione con altri TB (***Multimodal Biometrics***).\n\n","x":-1761,"y":3148,"width":760,"height":666,"color":"4"},
		{"id":"350412f4fee5a3ef","type":"file","file":"CosineSimilarity.png","x":-1060,"y":4646,"width":399,"height":86},
		{"id":"334ff31ec21a80e3","type":"file","file":"DTW.png","x":-1001,"y":4173,"width":283,"height":400},
		{"id":"bcc60cdc04b5895d","type":"text","text":"# Matching Module \n\nDati due template da comparare, ci sono sostanzialmente due alternative:\n\n- I template sono vettori (i.e. FV) contenenti misure di features significative (e.g. distanze tra i punti chiave di un volto, coordinate e orientazione delle minutiae sulle fingerprints). In questo caso posso cercare la ***minima distanza***.\n\t- Un esempio banale è la ***distanza euclidea*** (***L2***), o la ***distanza Manhattan*** (***L1***);\n\t- Uno meno banale è la ***distanza di Bhattacharyya***, che misura la ***sovrapposizione tra due pdf o istogrammi***.\n\t- Per dati dinamici (e.g. due tracce audio) si usa il ***Dynamic Time Warping*** (***DTW***), che con un processo non lineare confronta meglio due forme d'onda sfasate.\n- I template sono funzioni, istogrammi, oggetti per i quali non è ovvio definire una distanza o per i quali non è rilevante nel confronto. In questo caso si cerca la ***massima similarity***.\n\t- L'esempio più comune è la ***cosine similarity*** $S_C$, che guarda l'angolo individuato dai due vettori. $S_C$ è massima quando $\\cos\\vartheta$ vale $1$;\n\t- La ***Pearson Correlation*** valuta la ***correlazione lineare tra pdf e istogrammi***, quindi è sensibile al disallineamento (contrariamente al DTW).\n\nTutto questo assume che io sappia cosa rappresenta il template, ma questo non è vero se esso è generato da una NN. E quindi? Le provo tutte e vedo che succede!","x":-555,"y":4173,"width":918,"height":559,"color":"4"},
		{"id":"f1175d1af9fe2990","type":"text","text":"# Decision Module\n\nsulla base della threshold decido","x":-555,"y":5001,"width":918,"height":480},
		{"id":"221e8d65df6b0ef2","type":"text","text":"- **Watchlist**: Confronto mirato per verificare se una persona è in una lista ristretta di interesse.","x":-555,"y":6098,"width":250,"height":207},
		{"id":"b81e5ec8e65b5242","type":"text","text":"# Ma è una buona feature?\n\nAnche se lo sembra, ci sono alcuni dettagli da tenere in considerazione.\n\n- ***Wide Intra-Class Variations*** - La stessa persona può produrre sample molto diversi. Devo essere bravo a costruire una gallery rappresentativa di tutte le possibili varianti in cui la persona può presentarsi (e.g. bocca aperta, profilo, occhiali, ma anche parlare con il raffreddore);\n- ***Small Inter-Class Variations*** - Persone diverse possono produrre sample simili (e.g. gemelli, padre-figlio, ma anche sosia);\n- ***Noise*** - Questo riguarda più il modello, che dovrebbe essere robusto rispetto al rumore nei dati grezzi (e.g. cicatrici sulle dita, illuminazione non omogenea);\n- ***Non-Universality*** - Anche se scelgo un tratto abbastanza universale, devo sempre tenere in considerazione il fatto che qualcuno può non averlo;\n- ***Spoofing*** - Quanto è robusta rispetto ad chi prova ad impersonare un altro soggetto?\n\t- I tratti comportamentali sono in genere meno imitabili e/o falsificabili;\n\t- Se so come funziona il modello (e.g. prende la distanza tra i punti più luminosi e tra i punti più scuri) posso produrre un'immagine fittizia (***hand-crafted feature***) dalla quale il modello estrae esattamente quello che mi serve per superare il controllo (***white-box attack***, cioè conosco il funzionamento interno);\n\t\t- Questo è un grosso \"se\", perché in genere non so come funziona il modello (i.e. è una ***black-box***).\n\t- Può anche essere che l'attaccante non ci stia neanche davvero provando a fare spoofing, semplicemente i suoi tratti sono simili a quelli di un utente genuino (***zero-effort attack***).","x":-2771,"y":3148,"width":760,"height":666,"color":"4"},
		{"id":"19750c0603c5e6dd","type":"text","text":"The Biometric Consortium define biometrics as “automatic recognition of a person\naccording to discriminative characteristics”.","x":797,"y":-2815,"width":408,"height":215},
		{"id":"747086ac91be7c18","type":"text","text":"project + oral same session\n\nproject = build biometric system + performance evaluation, ma prima parla con lei. always mention the sources\n\nvene, retina, iride, orecchio sono tratti abbastanza univoci per fare pattern recognition\n\nfaccia importante, molte sue tecniche di pr si prendono per fare altro\n\nSVM utili. embeddings (set distanze tra punti) usati per fare training.\n\nbehavioural traits more difficult to reproduce for an attacker.\n\nverification - assess identity (i have claim of identity). dichiaro chi sono e l'algoritmo è comparativo con il DB di gente autorizzata ad entrare (e.g. quando sblocchi il telefono è un claim di identità implicito)\n- Questa roba deve essere sia strict che flessibile, perché se sono stanco e ho la faccia tirata non è che il telefono non si sblocca...\nidentification - (i dont). ho un DB pseudocompleto e riconduco l'input al best guess tra tutti quelli che ho.\n\ni tratti devono essere discriminativi. quelli che non lo sono (e.g. il colore dei capelli) sono considerabili \"soft biometrics\". assunzione che ogni persona sia unica (siccome non posso avere 100% accuracy questo in realtà non è vero, ma è una buona approx).\n\ndetto ciò, se individuo un buon tratto, quali features devo estrarne? mediapipe libreria per estrarre features. minuties? need triplets (x y orientazione).\n\nmanca un'intera lezione di 3h qui","x":1385,"y":-3040,"width":716,"height":766},
		{"id":"c6df0c468aac74b6","type":"text","text":"C'è tutta una pippa mentale sul tipo di utente che se hai voglia poi ti vedi\n\n- Cooperative: the user is interested in recognition (an impostor might try to be recognized as a legal user). \n- Non-cooperative: the user is indifferent or even adverse to recognition (an impostor might try to avoid being recognized)\n- Public/Private: users of the system are customers or employees of the entity installing the system\n- Used/Non used: frequency of use of the biometric system (more times a day, daily, weekly, monthly, occasionally …).\n- Aware/Not aware: the user is aware or not of the recognition process\n\ne poi sui setting\n\n- Controlled: capture settings can be controlled, distortions mostly avoided (e.g., for face, pose, illumination, and expression), defective templates can be rejected, and capture repeated\n- Uncontrolled/undercontrolled: capture settings cannot be controlled, template can present various levels of distortion, defective templates can be rejected, but capture cannot be repeated\n\nserve a qualcosa? ai posteri l'ardua sentenza","x":831,"y":-1928,"width":780,"height":663},
		{"id":"066eda0b63762e4f","type":"text","text":"# Performance Evaluation\nQuanto funziona bene questo giocattolo? Visto che di base è ML, dividiamo il dataset in\n\n- ***Training/Validation*** - I sample vengono divisi in modo pseudocasuale, eventualmente con l'aiuto di strumenti complessi come una ***K-Fold Cross-Validation*** per evitare bias.\n\t- Su questo set verrà eseguito il ***tuning*** dei parametri (i.e. viene appunto addestrato il modello) secondo le indicazioni delle metriche ottenute sul validation set;\n- ***Test*** - Viene usato per valutare la bontà del training. Dividiamo ulteriormente i sample in\n\t- ***Gallery*** - I dati di riferimento del sistema (e.g. una watchlist, i.e. la simulazione di quello che ho a disposizione nel momento in cui uso il modello), nella quale posso scegliere cosa includere (e.g. solo foto in ambiente controllato, immagini rappresentative del rumore) a seconda di quello che ci sia aspetta nella situazione reale. Il modello estrae le features dai soggetti labeled di questo set e prova a fare predizioni sulle probes.\n\t- ***Probe*** - La simulazione dei dati reali. Qui la scelta varia a seconda dell'applicazione:\n\t\t- Se sono in ***identification open-set*** (i.e. la probe potrebbe non essere nella Gallery) ha senso inserire sample di persone che non compaiono nella Gallery. Posso ovviamente anche non farlo, ma in generale le prestazioni di un simile modello dipendono dalla percentuale di soggetti non-identificabili (i.e. sorvegliare un aeroporto con una blacklist di poche persone costringe il modello ad incontrare un alto numero di individui che non compaiono nella Gallery).\n\t\t- Se sono in ***identification closed-set*** non ha senso mettere sample di persone che non compaiono nella Gallery. Il sistema proverà in ogni caso a dare una risposta al suo interno, producendo con tali sample un errore al $100\\%$;\n\t\t- Se sono in ***verifica*** questa scelta ovviamente non influenza le prestazioni.","x":2481,"y":2292,"width":796,"height":666,"color":"6"},
		{"id":"4acb6b0dcd714593","type":"text","text":"# Errori open-set\n\nIn questo caso ci sono due domande da porsi: Il soggetto è nel DB? Se sì, chi è?\n\nCom'è ovvio, due domande ci danno $4$ possibili scenari, di cui uno non si può mai verificare (se valuto che il soggetto non è nel DB non posso sapere chi è). Dal punto di vista del modello ci sono solo due casi, a cui seguono i sottocasi legati alle performance:\n\n- ***Alarm*** (`Il soggetto è nel DB`) - Il modello pensa di aver riconosciuto il soggetto.\n\t- Se il soggetto è davvero nel DB `Correct Detect`, dopodiché\n\t\t- se pensava fosse un altro (i.e. ha fatto `Correct Detect` per il motivo sbagliato) abbiamo `Incorrect Identification`, che si traduce in una ***FR***;\n\t\t- se ha indovinato anche l'identità `Correct Identification` (nel complesso abbiamo un ***Correct Result***). In questo caso si parla di ***Detect and Identification*** (***DI***).\n\t- Se il soggetto non è davvero nel DB abbiamo un ***falso allarme***, quindi sia una `Incorrect Detect` che una `Incorrect Identify`, che si traduce in una ***FA***.\n\t\t- Inviare $n\\gg1$ probe che non appartengono alla gallery permette di stimare il ***false alarm rate***. \n- ***No Alarm*** (`Il soggetto non è nel DB`) - Il modello non riconosce il soggetto.\n\t- Se davvero il soggetto non è nel DB, `Correct Result` i.e. ***GR***.\n\t- Se invece lo è, `Incorrect Result` i.e. ***FR***.\n\nA questo punto possiamo definire la CMC come ***Detect and Identification Rate at Rank*** $k$:$$DIR(t,k) = {|\\{p_j\\,:\\,rank(p_j)\\leq k, \\,s_{ij}\\geq t, \\,id(g_i)=id(p_j)\\}|\\over |P_G|}\\quad\\forall p_j\\in P_G=P_{\\text{Genuine}}$$Bene, a che serve? A calcolare il ***FRR***, in questo caso un ***False Negative Identification Rate*** (FNIR), come$$FRR(t) = 1 - DIR(t,1)$$cioè \"qual è la probabilità che il sistema non riconosca (al rank 1) un soggetto presente nella gallery?\". Analogamente posso chiedermi qual è la probabilità che il sistema accetti un soggetto non presente nella gallery (FAR, FPIR)$$FAR(t) = {|\\{p_j\\,:\\,\\max_i(s_{ij}\\geq t) \\}|\\over|P_N|}\\quad \\forall p_j\\in P_N \\,\\,\\wedge\\,\\, \\forall g_i\\in G$$Tutto questo mi è utile a calcolarmi la ***ROC***, che plotta come sempre FAR vs FRR.","x":3430,"y":5005,"width":960,"height":863,"color":"4"},
		{"id":"0029f2bd8b83473f","type":"text","text":"# How To Threshold (ROC)\n\nMigliore il modello, migliore la ROC (i.e. maggiore la AUC). Ma ogni possibile scelta di threshold è un compromesso. Come la scelgo? Abbiamo diversi scenari.\n\n- ***Minimizzare i falsi allarmi***, i.e. \"per dare l'`alarm` devi esserne proprio sicuro\";\n\t- Usi - Un evento che mobilita molte risorse, che crea panico o, o se non si vuole far scoprire con troppa facilità la presenza di un sistema di sicurezza;\n\t- Significa scegliere una threshold che sia ***più a sinistra possibile*** sulla ROC;\n- ***Massimizzare la DIR***, i.e. \"cerca di non farti sfuggire nessun potenziale elemento della Watchlist.\";\n\t- Usi - Ci sono due sotto-situazioni qui:\n\t\t- \"... ***anche a costo di produrre un gran numero di falsi allarmi***\", tipo ai controlli in aeroporto. Un falso allarme è gestibile, ed è preferibile ad una minaccia non vista;\n\t\t\t- Significa scegliere una threshold che sia ***più a destra possibile*** sulla ROC;\n\t\t- \"... ma cerca di non creare troppi falsi allarmi\", in situazioni in cui è difficile gestirli, come i controlli alla frontiera per trovare dei terroristi. Un falso allarme potrebbe creare panico.\n\t\t\t- Significa scegliere una threshold che sia ***a destra, ma non troppo***.\n- ***Bilanciare FAR e FRR***, i.e. \"ho tolleranza sia per i falsi positivi sia per i falsi negativi, basta che non siano troppi\".\n\t- Usi - Un sistema di sicurezza per l'accesso ad un edificio, in cui non è un problema qualche falso positivo (i.e. un utente autorizzato verrà riconosciuto da un umano) o qualche falso negativo (i.e. magari uno che entra per sbaglio cercando un bagno non mobilita la sicurezza, insomma basta che il rischio non sia troppo alto).\n\nCi sono situazioni in cui ***non è necessaria una threshold***: in un'investigazione, ad esempio, la polizia potrebbe voler vedere personalmente tutti i match score prodotti dal modello.","x":3430,"y":6001,"width":960,"height":641,"color":"4"},
		{"id":"b915a5ddb6f4acfa","type":"text","text":"# 2.1\n\naccuracy/loss va bene per training-validation (deep learning), ma quando vado in testing mi tocca usare le metriche più adatte per la biometrica\n\na volte i dataset contengono direttamente le distanze invece delle probes, ma è raro\na volte la gente è simpatica e costruisce un test set ad-hoc per pompare le performance, oppure lasciano che training e test sia overlappino per lo stesso motivo (...)\nse nel training sono sbilanciato (e.g. poche donne afroamericane di 20 anni) in testing faccio errori su quella categoria.\nreliability through good balancing, alcuni dataset suggeriscono opportune partizioni train-test.\nil training potrebbe non essere necessario (uso tipo tagli rettangolari?)\n\n\"Choice based on both subjects (a subject may not belong to2 he training set, to better test generalizability) and on samples (no overlap between TR and TS allowed!)\"\n\nfirst e second choice non sono alternative, ma consecutive (train/test prima e probe/galley poi).\nper probe/gallery devo poter usare la cross-validation (cerca appunti iocchi) (ma è parte del test? me sa). 70 train 10 validation 20 test\n\nnormalizzazione deve andare in \\[0,1\\] ma anche preservare le forme funzionali (similarity = 1-distance). \"easier to minimize distances than similarities\" (?) leggi bene probe vs gallery (slide 14)\n\nclosed set è importante solo l'ordine, negli altri casi anche i numeri\n\n\\[...\\]","x":9660,"y":4436,"width":812,"height":830},
		{"id":"237a1e0bfd57425c","type":"text","text":"in genere si sacrifica genuine acceptance per minimizzare false acceptance.\n\nse il TB cambia (e.g. invecchio) dovrei comunque essere in grado di riconoscere quel TB. fingerprints sono considerate universal and reliable ma possono esserci danni. voce non universale obv, cataratta per retina (o iride?), ...\n\nzero-error attack dichiaro di essere qualcun altro\nspoofing fare qualcosa to mislead recognition system (posso creare falso fingerprint con colle e simili, e seguono tecniche anti-spoofing). posso presentare la foto dell'iride di un'altra persona, o usare lenti a contatto.\n\nDNA univoco, ma se ricevo donazione di midollo potrebbe essere ambiguo.\nretina funziona bene. behavioral traits difficili da riprodurre. walking richiede strategia raffinata in modo molto specifico per ogni persona, difficile da riprodurre a lungo. signature, soprattutto la dinamica del polso che la fa.\n\nhand-crafted features = features... con cui creare il FV (si possono anche estrarre come embeddings, cioè il livello quasi-finale di una NN, anche detto latent space in quanto non interpretabile da un umano ma utilizzabile da una macchina)\n\nminutiae = bifurcation of lines, end of lines, ... (on fingerprint FP)\n\ncityblock (L1 distance) ce ne sono tipo mille, ma distinguo solo distanza e similarità. la covarianza è una misura di similarità, usata per FP. per confrontare gli istogrammi c'è la distanza di Bhattacharyya. in generale, ogni tipologia di gallery (i.e. tipo di template al suo interno) ha la sua misura ottimale. dyamic time warping per dati con accelerometri tipo, nel senso che confronta due funzioni considerando gli stretch e le fasi. in pratica prova a farle corrispondere. più è difficile più sono distanti. insomma, è la distanza da usare tra segnali(t).\n\nFP. in genere problema di orientazione e che spesso si trovano solo frammenti di FP. ci sono modi speciali di fare la distanza (NN).\n\nerrori di tipo I e II non sono ottimizzabili contemporaneamente, in genere serve trade-off. l'analisi degli errori si porta dietro tutti i problemi del training in ML.\n\nricorda che false rejection ha al denominatore il totale dei probes che should be accepted ($\\text{true but rejected}\\over\\text{TbR + True and Accepted}$) . FR e GA sum up to one (i.e. genuine probes). nei casi limite si lascia la scelta all'umano.\n\nil training è sempre supervised.\n\nmore template for same person to address intra-class variations. meglio pochi ma buoni che prendere 200 frame di un video in cui close frames sono simili e la risoluzione è bassa.","x":10099,"y":1560,"width":708,"height":1248},
		{"id":"6a9045ca7edf7d9f","type":"text","text":"# Spoofing\n\ndevo distinguere tra spoofing (provo ad impersonare qualcun altro) dal camouflage (nascondo i miei tratti biometrici, ad esempio facendomi tagli sulle dita)\n\nma se taglio un dito all'addetto della sicurezza e lo uso per aprire la porta? Ci sono delle tecniche per determinare se il pezzo di corpo che mostro è vivo o morto (e.g. micro variazioni di volume dovute al battito cardiaco)\n\nora, attacchi del genere sono attacchi diretti. posso però attaccare un qualsiasi punto dell'intero sistema (i.e. feature extractor, comparator, DB, ...)\n\ntexture = repeating pattern che copre la superficie. è stato uno dei primi task della computer vision (texture identification). texture of a mask is different from one of a real face.\n\nora, face recognition si basa solo sulla faccia, ignorando tutto il resto. se gli metto davanti una foto, ho automaticamente vinto. E quindi?\n\nse provo a fare replay attack facendo un video al video, ci sono due telecamere di mezzo che si sovrappongono ed esce fuori un effetto moire che posso riconoscere.\n\npoi camera termica (se sono freddo forse sono falso), reazioni involontarie e volontarie (\"alza un braccio\", oppure lo spaventi, fa ridere), ecc...\n\noptical flow? la roba per cui \"ruota la testa\" su una foto ti fa capire che è una foto. tipo.\n\nmedia pipe\n\nlocal binary pattern\n\n","x":10980,"y":2306,"width":660,"height":1201},
		{"id":"f9bb82c22ea2981e","type":"text","text":"# Errori closed-set\n\nDiamo per scontato che il soggetto sia nel DB. Ma chi è?\n\nValutare le performance di un siffatto sistema si traduce nel valutare\n$$CMC = CMS(k) = \\frac{|\\{ p_j \\in P \\, : \\, rank(p_j) \\leq k \\, \\wedge \\, id(p_j) = id(g_i) \\}|}{|P|}$$che fa le veci della curva ROC in open-set.","x":1341,"y":5005,"width":960,"height":258,"color":"4"},
		{"id":"0f97a59a582d24b7","type":"text","text":"# How To Threshold (ALL-VS-ALL)\n\nInvece di dividere (eventualmente più volte) il test in Probe e Gallery posso usare $D$ per effettuare un check ALL-VS-ALL: ogni sample fa sia da probe sia da elemento della Gallery, ignorando ovviamente gli elementi diagonali di confronto del sample con se stesso.\n\nÈ per costruzione una sorta di media delle possibili distribuzioni Genuine/Impostor, il che può essere un vantaggio per la generalizzabilità ma uno svantaggio per una task specifica.\n\n- Perché usarlo?\n\t- facile da programmare;\n\t- ogni probe risulta genuina solo alle varianti di se stessa (i.e. $A_2$ è genuina solo su $A_1$ e $A_3$), di conseguenza ne risulta un gran numero di impostori (i.e. $A_2$ è impostore su ogni $B_i$, ogni $C_i$, ...). Questo è utile come stress test se sto allenando un modello che avrà a che fare con un gran numero di tentativi di impostori.\n- Possibili problematiche?\n\t- potrebbe avere un alto costo computazionale;\n\t- fortemente aspecifico, tende ad appiattire ogni variabilità sia di applicazione che interna al dataset (e.g. se ho samples presi in momenti, condizioni e sensori diversi, questo metodo fa una media sommaria di tutte queste differenze che andrebbero invece incluse nella gallery per creare un campione rappresentativo).\n\nOgnuna delle $N$ righe effettua $N$ controlli, trovando\n\n- Impostore, se la label non corrisponde. Data la threshold decido se classificare questo matching come ***FA*** (il sistema accetta l'impostore) o come ***GR*** (lo rifiuta);\n- Genuino, se la label corrisponde. Questo succede solo con le proprie varianti, e anche qui a seconda della threshold e del punteggio distinguo tra ***GA*** ed ***FR***.\n\nRipetendo questo gioco al variare della threshold trovo quella ottimale.","x":1341,"y":5518,"width":960,"height":700,"color":"4"},
		{"id":"4bf5870defbcf5c9","type":"text","text":"# Errori in Verifica\n\nIn verifica, un soggetto viene accettato se la distanza tra la probe e i template associati all'***identità reclamata*** è minore di una certa ***threshold*** (e viceversa con la similarity). 4 casi:\n\n- ***Genuine Acceptance*** (***GA***) - il claim è vero, il sistema accetta;\n- ***False Rejection*** (***FR***) - il claim è vero, ma il sistema rifiuta (***Type-I Error***);\n\t- Gli si associa la FR Rate, ovvero la probabilità che un utente registrato sia rifiutato$$\\text{FRR}(t) = {\\text{\\#Claim di utenti registrati che vengono rifiutati}\\over\\text{\\#Claim di utenti registrati}}$$\n\t- Segue con analoghe definizioni che $\\text{GAR} = 1-\\text{FRR}$.\n- ***False Acceptance*** (***FA***) - il claim è falso, ma il sistema accetta (***Type-II Error***);\n\t- Qui si usa FA Rate, intuitivamente l'opposto della precedente$$\\text{FAR}(t) = {\\text{\\#Claim di impostori che vengono accettati}\\over\\text{\\#Claim di impostori}}$$\n- ***Genuine Rejection*** (***GR***) - il claim è falso, il sistema rifiuta.\n\t- Segue che $\\text{GRR} = 1-\\text{FAR}$.\n\nOvviamente il rischio maggiore è un Type-II, e in generale la valutazione delle performance si basa sul minimizzare gli errori, piuttosto che massimizzare le identificazioni corrette. In questo senso ci sono criteri come $\\text{ZeroFAR}$ (i.e. metto la threshold in modo che non ci sia alcun errore Type-II) e $\\text{ZeroFRR}$, ma anche valutazioni in cui accetto un bilanciamento in cui essi sono uguali ($\\text{EER}$, ***Equal Error Rate***) o trade-off come le curve ***ROC*** e ***DET***.\n\nFormalmente, sia `id(template)` la funzione che restituisce la ***ground truth*** (vera identità) del `template` che gli si passa. Sia $p_j$ la $j$-esima probe e $g_k$ il $k$-esimo template della gallery.$$\\text{FRR}(t) = \\frac{\\left| \\{ p_j : s_{xj} \\leq t, \\ \\text{id}(g_x) = \\text{id}(p_j) \\} \\right|}{\\left| \\{ p_j : \\text{id}(g_x) = \\text{id}(p_j) \\} \\right|}$$È banalmente la traduzione di quello che c'è scritto sopra, dove $s$ è la similarità e $t$ la threshold (identico, con la diseguaglianza al contrario, per la distanza). $g_x$ si intende ottenuto come output di una qualche funzione `TopMatch(p, identity)` che restituisce i template della gallery che *best-matchano* con `p`. Ciò detto il significato è chiaro:\n\n- Numeratore - Numero di probe $p_j$ la cui identità è la stessa del suo `TopMatch` che però avendo ottenuto un punteggio di similarità $s_{xj}$ minore di $t$ sono state rifiutate.\n- Denominatore - Numero di probe $p_j$ la cui identità è la stessa del suo `TopMatch`.\n\nDiscorso analogo per il gemello malvagio Type-II$$\\text{FRR}(t) = \\frac{\\left| \\{ p_j : s_{xj} \\geq t, \\ \\text{id}(g_x) \\neq \\text{id}(p_j) \\} \\right|}{\\left| \\{ p_j : \\text{id}(g_x) \\neq \\text{id}(p_j) \\} \\right|}$$A questo punto, come detto, uno in genere usa la ***ROC***.\n\nIl grafico ROC passa per $(0,0)$ e $(1,1)$: se metto la soglia troppo bassa rifiuto tutto (quindi sia $\\text{FAR}$ che $\\text{GAR}$ sono $0$), e viceversa. La ROC mi dice che ***potenzialmente posso scegliere una threshold lungo la curva***, quindi in generale più l'AUC è grande più sono contento, perché così posso fare scelte migliori. La scelta, tuttavia, è sempre un compromesso.","x":2482,"y":862,"width":796,"height":1216,"color":"4"},
		{"id":"bec8557c6aabbfee","type":"file","file":"SingleAndOverall.png","x":2483,"y":542,"width":795,"height":194},
		{"id":"987aa01415d17eed","type":"text","text":"# Overfitting?\n\nMa se il mio modello deve aspettarsi un gran numero di soggetti che non sono nella Gallery (e.g. la sorveglianza dell'aeroporto) non potrebbe avere senso \"ingegnerizzare\" anche la fase di training nello stesso spirito dello split Gallery/Probe?\n\n... ni. Ovviamente questa cosa introduce un ***bias intenzionale***, ma non necessariamente è una cosa negativa. Se trovi il giusto equilibrio potresti migliorare le performance per quella specifica applicazione, fermo restando che sicuramente perderai in generalizzabilità.\n\nIl rischio ovviamente è andare in ***overfitting***, se non trovi questo famoso \"giusto equilibrio\".","x":1039,"y":1928,"width":767,"height":300,"color":"4"},
		{"id":"f1084d1ab8558173","type":"file","file":"ALL-VS-ALL Pseudocode.png","x":2579,"y":5644,"width":600,"height":449},
		{"id":"298ea7f436357c7e","type":"text","text":"# Dataset, Gallery, Features, Templates...\n\n\nL'insieme delle features estratte dagli elementi della gallery prende il nome di ***template***.\n\nI template devono essere tali da\n\n- Non permettere la ricostruzione del dato grezzo originale, per questioni di privacy e sicurezza (in genere è comunque difficile fare reverse engineering);\n\n\nPosso continuare ad aggiungere dati alla gallery anche dopo che il modello ha finito il training\n\n- se riconosco un nuovo elemento e qualcuno si prende la briga di associargli una label (***supervised assignment***);\n- se una probe viene riconosciuta con abbastanza sicurezza (e.g. oltre una certa threshold) da indurre il modello ad aggiungerla in autonomia (***semi-supervised***).\n\nPosso inoltre scegliere di effettuare questa azione \"in diretta\" oppure \"ogni tot\".","x":-2266,"y":2340,"width":760,"height":570,"color":"2"},
		{"id":"71532c4f2c736877","type":"text","text":"# Metriche del Machine Learning\n\nHa senso ***affiancare*** alle misure di performance ad hoc per i BS quelle per il machine learning classico quali\n\n- ***Precision*** - Quante previsioni positive sono effettivamente corrette? $$\\text{Precision}={TP\\over TP+FP}$$Guarda la $\\text{Precision}$ se è importante ***evitare i falsi positivi***.\n- ***Recall*** - Quanti veri positivi riconosco come positivi?$$\\text{Recall}={TP\\over TP+FN}$$Guarda la $\\text{Recall}$ se è importante ***trovare tutti i positivi***.\n- ***F-Score*** - Quanto sono bilanciate $\\text{Precision}$ e $\\text{Recall}$? Un punteggio vicino a $0$ indica un forte sbilanciamento (i.e. una delle due è $\\sim0$) , un valore vicino a $1$ è auspicabile. A seconda di quanto peso dare a ciascuna, si usano diverse definizioni di $\\text{F-Score}$:\n\t- Se è più importante la $\\text{Precision}$ si usa (con una nomenclatura da Oscar)$$F_{0.5}={(1+0.25^2)\\cdot\\text{Precision}\\cdot\\text{Recall}\\over0.25\\cdot\\text{Precision}+\\text{Recall}}$$\n\t- Per un bilanciamento simmetrico si usa la ***media armonica***$$F_1=2\\cdot{\\text{Precision}\\cdot\\text{Recall}\\over\\text{Precision}+\\text{Recall}}$$\n\t- Se è più importante la $\\text{Recall}$ si usa$$F_2={(1+2^2)\\cdot\\text{Precision}\\cdot\\text{Recall}\\over4\\cdot\\text{Precision}+\\text{Recall}}$$","x":1039,"y":3103,"width":767,"height":756,"color":"4"},
		{"id":"25a2bf3a89b19ed9","type":"text","text":"# Distance Matrix\n\nData una divisione Probe/Gallery è possibile calcolare $D_{ij}=\\dist(\\text{probe}_i,\\text{gallery}_j)$. Una simile ***distance matrix*** è un check in cui per ogni probe sample si calcolano le distanze da tutti i gallery samples (***Probe-VS-Gallery***)\n\nOvviamente se sono in verifica non serve fare una cosa del genere: ogni probe sample ha una claimed identity associata, quindi confronto solo con quei sample della gallery.\n\nData la distance matrix $D$ e la threshold $t$, è facile verificare per ogni probe se\n\n- in ***verifica***, l'identità dichiarata ha un match $\\ge t$. Se è così, visto che ho le label sono in grado di dire se è stata una GA (user) o una FA (impostor). Viceversa, se ottengo un match $<t$, sono in grado di dire se è stata una GR (impostor) o una FR (user);\n- in ***identificazione*** ho diverse scelte.\n\t- In ***open-set***, se esiste almeno un match $\\ge t$, entro tale insieme posso ad esempio restituire il top match. Posso anche scegliere di restituire i primi $k$ match e valutare il sistema al rango $k$. In ogni caso, il sistema sputerà fuori un verdetto. Il primo check ovviamente è se il soggetto è o meno nella Gallery, dopodiché però devo anche predire correttamente la sua identità. Qua è un po' meno intuitivo definire i vari tipi di GA, FR e compagnia, quindi dedichiamo un rettangolino a parte;\n\t- In ***closed-set*** non c'è la threshold, ogni identificazione al rank $k$ incrementa ogni variabile $CMS(i\\ge k)$, i.e. se identifico al rank $k$ significa che ho identificato anche entro il rank $k+1$, $k+2$, ... Se identifico in posizione $1$, $CMS(1)$ è anche detto ***Recognition Rate*** (***RR***).\n\nÈ comune avere in gallery più di un sample per utente registrato. Questo in generale offre più possibili match, quindi riduce la FRR, ma può aumentare la FAR (ci sono anche più modi per un impostore di sembrare genuino).\n\nTutto questo gioco viene poi eventualmente ripetuto per diverse divisioni Probe/Gallery.","x":3430,"y":3148,"width":960,"height":666,"color":"4"},
		{"id":"fc3058e5571aad4f","type":"text","text":"# Usi principali della Distance Matrix\n\n- ***ALL-VS-ALL*** - Tendenzialmente in ***closed-set*** o se si hanno a disposizione pochi template. Ognuno di essi è confrontato con ogni altro template salvo se stesso. Non c'è alcuna divisione esplicita in Probe e Gallery, ogni template ha entrambi i ruoli;\n- ***ALL-VS-ALL-Gallery*** - Solo un subset dei template costituisce la gallery, ma ognuno di essi è usato come probe. Anche qui tendenzialmente ***closed-set***;\n- ***ALL-Probe-VS-ALL-Gallery*** - Quello che uno penserebbe sentendo la definizione di Distance Matrix. Divido i template in Probe e Gallery, quindi calcolo la matrice. Si può usare anche in ***open-set***.\n\n<span style=\"color:#FFA500\">Credo, eh! Non sono sicurissimo di ciò</span>\n\nSi possono poi usare altre tecniche più complesse come la stessa k-fold cross validation per dividere il test set in probe e gallery.\n\nTutto questo non ha molto senso in verifica (credo)","x":4620,"y":3148,"width":764,"height":666,"color":"3"},
		{"id":"21071ea6413c0796","type":"text","text":"# Un esempio di identificazione open-set\n\nNell'immagine, i numeri servono solo ad indicare l'ordine dei best match (i.e. non sono il punteggio di distanza).\n\n- La probe $P_1$ ha come identità $A$. Il sistema individua come top match il sample $A_2$ della Gallery. Se la distanza di tale match è minore della threshold $t$ scelta, avremo una $DI(1,t)$, i.e. una ***Detection and Identification at rank 1***, che è il miglior risultato possibile. Questo ci porta ad aumentare il counter delle buone notizie (i.e. `DI(1,t)++`);\n\t- Se però il valore del match è $\\ge t$, dal momento che $A_2$ è il top match nessun altro sample della Gallery può avere un punteggio migliore. Il sistema rifiuta, e di conseguenza abbiamo una ***FR***. Non serve tuttavia fare `FR++`: le false rejections possono essere dedotte dal DIR ($FR=N_{\\text{Genuine Users}}-DI(1,t)$, i.e. \"tutti quelli che dovevi identificare correttamente meno quelli che hai identificato correttamente\").\n- La probe $P_2$ ha come identità $D$, ma il sistema individua come top match il sample $B_2$. Siamo tutti d'accordo che il sistema ha sbagliato (i.e. ho una ***FR at rank 1***), ma se la distanza da $B_2$ e da $B_1$ è $<t$ posso controllare se lo è anche quella da $D_1$. In caso affermativo, posso comunque avere una $DI(3,t)$, i.e. ***identifico correttamente al rango 3***. Quanto è utile saperlo? Dipende da quanto sono permissivo con il mio sistema, i.e. entro a quale rank pretendo la risposta giusta;\n\t- Questo ci fa capire meglio per quale motivo non conto esplicitamente le FR: se mi interessa il rank $1$ le deduco da $DI(1,t)$, se mi interessa il rank $k$ le deduco da $DI(k,t)$ (i.e. $FR(k)=N_{\\text{Genuine Users}}-DI(k,t)$)\n- La probe $P_3$ è un impostore, quindi la casistica è più semplice: se il top match è $\\ge t$ avrò ***GR***, altrimenti ***FA***. Nota che questi devo per forza contarli, perché non posso dedurli in nessun altro modo.","x":3431,"y":4185,"width":960,"height":537,"color":"4"},
		{"id":"297b1973a951d1b5","type":"file","file":"EsempioIdentificazione.png","x":4501,"y":4327,"width":740,"height":253},
		{"id":"437ae081305ab212","type":"text","text":"# Errori in Identificazione\n\nQuando arriva una probe, vengono calcolati tutti i match score con gli elementi della gallery, quindi ordinati in ordine decrescente. Il giusto match potrebbe essere quello con il massimo score (quindi trovarsi in posizione $1$) o nella generica posizione $k$.\n\n$k$ è detto ***rango*** dell'identificazione. Con questa nozione, è chiaro che se richiediamo che la giusta identità venga restituita a rank $1$ le performance saranno peggiori rispetto ad una situazione più morbida in cui ci sta bene il rango $2$.\n\nIn generale, definiamo il ***Cumulative Match Score at Rank $k$*** (***CMS***) come la probabilità che la corretta identificazione avvenga entro la posizione $k$ della lista ordinata. Il plot delle varie CMS per i vari valori di $k$ è detta curva ***CMC*** (***Cumulative Match Characteristic***).\n\nLa definizione matematica di CMS è diversa nei casi di\n\n- ***open-set***, in cui è rilevante $CMS(k=1)$ per costruire la curva ROC;\n- ***closed-set***, in cui si usa direttamente $CMC$.","x":2482,"y":4213,"width":796,"height":480,"color":"4"},
		{"id":"f4896017c69ba8cb","type":"text","text":"# Breve nota storica\n\nNel 1882 il grande capo della polizia di Parigi Bertillon decise di iniziare a prendere varie misure biometriche del corpo dei detenuti (e.g. forma della mano, della testa, dettagli facciali).\n\nQuesta roba ebbe un sacco successo, tanto che nel 1896 anche l'FBI iniziò a schedare le persone e nel 1900 queste carte divennero regolamentate dalla legge francese.\n\nQualcuno però storse il naso. Per la privacy? No, perché questa roba era inefficiente. Tale Galton introdusse il concetto di ***minutiae***, ovvero piccolissimi tratti distintivi in grado di identificare in modo univoco una persona. Un esempio? Data una ***fingerprint***, le minutiae sono i punti in cui le linee terminano o si dividono.\n\n","x":1123,"y":791,"width":600,"height":399,"color":"4"},
		{"id":"c2ba3fbffc4679d8","type":"text","text":"# Sì, ma in pratica?\n\nData la gallery, faccio ***supervised training*** (le etichette sono dette ***ground truth***) che consiste nel loop\n\n- FEM\n- Matching\n- Decision\n- Evaluation\n\t- sulla base di quest'ultima modifico qualcosa in una delle fasi precedenti (pesi delle metriche, threshold, fine tuning se il FEM è una NN, ...) e ricomincio finché non ottengo risultati soddisfacenti.","x":-2600,"y":1440,"width":556,"height":371},
		{"id":"f4cc52883462f784","type":"text","text":"# Face","x":-6137,"y":975,"width":250,"height":60},
		{"id":"2f94a815f931defe","type":"text","text":"# Iris","x":-5730,"y":1946,"width":250,"height":60},
		{"id":"fadd10091b275ac7","type":"text","text":"# Fingerprint","x":-5656,"y":1362,"width":250,"height":91},
		{"id":"1ec09233624528e6","type":"text","text":"# Signature ","x":-7227,"y":1238,"width":257,"height":124},
		{"id":"c5ac94c35b35d4f6","type":"text","text":"# Voce - Gaussian Mixture Model (GMM)","x":-6730,"y":2116,"width":599,"height":140},
		{"id":"f7c635f63f3a96b7","type":"text","text":"# Retina","x":-6681,"y":940,"width":250,"height":60},
		{"id":"9e8b2a1513d0ae94","type":"file","file":"TraitsOverview.png","x":-6550,"y":1802,"width":400,"height":197},
		{"id":"f3212e371ea4f80e","type":"text","text":"# Anti-Spoofing\n\nmi sa che sta roba sta meglio vicino alle implementazioni, perché è specifica del tipo di feature","x":-7520,"y":1689,"width":422,"height":257},
		{"id":"08d4569a4c15afaf","type":"text","text":"# Implementazioni\n\n","x":-5825,"y":860,"width":589,"height":261,"color":"6"},
		{"id":"95d5a8fc66a6d083","type":"text","text":"# Idea progetto\n\nUna roba potenzialmente multimodale, in cui usiamo un tratto principale ed uno secondario.\n\nEsempio, per loggare basta parlare (voice scan). Se il punteggio di matching è\n\n- inferiore ad una low threshold: si viene subito rifiutati;\n- superiore ad una high threshold: si viene subito accettati;\n- compreso tra le due threshold: si procede con un secondo tratto biometrico di backup (e.g. hand scan). Combinando i risultati dei due matching dovrei essere in grado di ridurre l'errore ed essere più preciso nel decidere se accettare o meno.\n\t- a questo punto devo ricadere su un sistema classico a singola threshold\n\npossibili problemi:\n\n- esistono DB che hanno sia voce che mano della stessa persona? (suppongo di sì);\n- quanto è difficile combinare i due risultati a livello di matematica (probabilità/statistica)?\n- qualcuno fa già cose simili?","x":-4960,"y":-2373,"width":780,"height":500,"color":"5"},
		{"id":"d86bb13b73cf864e","type":"text","text":"# Progetto\n\n\nBuonasera professoressa, le scrivo a nome del gruppo ecc...\n\nPer il progetto dell'esame di Biometric Systems vorremmo combinare l'eye-tracking ed il riconoscimento vocale, aggiungendo una misura anti-spoofing.\n\nL'utente può scegliere (o meno) di dichiarare la propria identità (se si lavora in verifica o in identificazione), dopodiché il sistema propone su uno schermo una frase generata in modo casuale. L'utente dovrà quindi leggere ad alta voce quanto scritto.\n\nLa lettura consente di prendere i dati relativi al movimento oculare tramite una webcam, la voce viene catturata da un registratore.\n\nUn modulo Speech-to-Text controlla se quanto detto dalla voce corrisponde alla frase casuale generata, impedendo di usare voci registrate. In caso il controllo abbia esito positivo, il modello effettua i controlli sui dati biometrici.\n\nL'idea è realizzare un sistema economico a livello di sensori e low effort per l'utente.\n\nAl momento non abbiamo trovato un dataset che includa entrambe queste misure da parte della stessa persona, ma riteniamo plausibile addestrare il modello in modo accettabile anche prendendo i dati da dataset separati. Ne approfittiamo comunque per chiederLe se per caso Lei ne conoscesse qualcuno.\n\nIn alternativa, se utilizzare l'eye-tracking risulti troppo difficile, pensavamo di ripiegare su un face-scan mantenendo invariata la struttura del progetto.\n\nAttendiamo la sua opinione sulla fattibilità di questo progetto per procedere.\n\nCordiali saluti ecc...","x":-4960,"y":-1686,"width":708,"height":842,"color":"5"},
		{"id":"811c43decb94ee1d","type":"text","text":"# Progetto","x":-3849,"y":-1822,"width":446,"height":124,"color":"6"},
		{"id":"a3d2e59f3d6b2d43","type":"text","text":"### **Riconoscimento Facciale (Face Recognition)**\n\n- **FEM basati su Deep Learning**:\n    - **FaceNet**: Una delle architetture più popolari per l'estrazione di feature per il riconoscimento facciale. Utilizza una **loss triplet** per imparare una rappresentazione ottimale del volto, in modo che volti simili siano vicini nello spazio delle embedding e volti diversi siano lontani.\n    - **VGG-Face**: Basato sull'architettura VGG, questo modello è stato progettato per estrarre feature facciali rappresentative. Le sue embedding sono molto utilizzate per confronti facciali.\n    - **ArcFace**: Un modello avanzato che utilizza una particolare funzione di loss chiamata **Additive Angular Margin Loss** per migliorare la discriminazione tra diverse classi di volti, generando embedding molto efficaci per il confronto.\n- **FEM tradizionali**:\n    - **Local Binary Patterns (LBP)**: È una tecnica che cattura texture e caratteristiche locali del volto, molto usata nei primi metodi di riconoscimento facciale.\n    - **Histogram of Oriented Gradients (HOG)**: Analizza le direzioni dei gradienti nell'immagine per rappresentare le forme e i contorni del volto.\n\n### 2. **Impronte Digitali (Fingerprint Recognition)**\n\n- **FEM basati su Deep Learning**:\n    - **DeepPrint**: Un sistema che usa reti neurali profonde per estrarre feature dalle impronte digitali. Il modello estrae le caratteristiche importanti come i minuzie (biforcazioni, terminazioni) e genera embedding per il matching.\n- **FEM tradizionali**:\n    - **Minutiae-based Methods**: Sono i metodi più comuni per l'estrazione di feature dalle impronte digitali. Identificano le **minuzie**, come biforcazioni e terminazioni dei solchi, per rappresentare un'impronta in modo univoco.\n    - **Orientation Field**: Questo metodo estrae le direzioni principali dei solchi in diverse regioni dell'impronta, che possono poi essere confrontate tra diverse impronte.\n\n### 3. **Riconoscimento della Voce (Speaker Recognition)**\n\n- **FEM basati su Deep Learning**:\n    \n    - **x-vector**: È un modello di deep learning che estrae vettori (x-vectors) dalle caratteristiche vocali. Questi vettori sono utilizzati per il riconoscimento del parlante, basandosi sul timbro e altre caratteristiche della voce.\n    - **Deep Speaker**: Un altro modello basato su deep learning che si occupa di estrarre feature dalla voce. Il modello apprende a generare embedding vocali che possono essere usate per identificare un parlante.\n- **FEM tradizionali**:\n    \n    - **Mel Frequency Cepstral Coefficients (MFCC)**: Una delle tecniche più comuni per l'estrazione delle caratteristiche vocali. Estrae le **componenti spettrali** della voce che possono essere usate per confrontare i diversi speaker.\n    - **Linear Predictive Coding (LPC)**: Un altro metodo per rappresentare il segnale vocale, modellando come il tratto vocale filtra il segnale sonoro.\n\n### 4. **Riconoscimento dell'Iride (Iris Recognition)**\n\n- **FEM tradizionali**:\n    - **Daugman’s Algorithm**: Un algoritmo molto utilizzato per l'estrazione di feature dall'iride. Cattura l'informazione del pattern di texture dell'iride e la rappresenta con un **codice binario** (iris code).\n    - **Wavelet-based Methods**: Tecniche che utilizzano wavelet per analizzare il pattern di texture dell'iride a varie scale e livelli di risoluzione.\n\n### 5. **Riconoscimento della Camminata (Gait Recognition)**\n\n- **FEM tradizionali**:\n    - **Gait Energy Image (GEI)**: Un metodo che utilizza una serie di immagini silhouette della camminata di una persona per estrarre un'immagine energetica media che rappresenta il modello di camminata.\n    - **Motion History Image (MHI)**: Cattura il movimento attraverso una sequenza temporale di frame e rappresenta la variazione del movimento in una singola immagine.\n- **FEM basati su Deep Learning**:\n    - **GaitNet**: Un'architettura basata su reti neurali che apprende direttamente i modelli di camminata da sequenze video e genera embedding per distinguere tra le persone.\n\n### Conclusione:\n\nOltre alla rete neurale che puoi usare per estrarre feature, ci sono FEM specifici per ogni tipo di input (facce, impronte, voce, ecc.). Questi moduli possono essere sia **deep learning-based** che **metodi più tradizionali**, e la scelta dipende dal tipo di dati e dalle performance richieste per l'applicazione.","x":3600,"y":-1698,"width":1356,"height":1567},
		{"id":"59d4e4543428278c","type":"text","text":"# System Response Reliability (SRR)\n\nLa ***system response reliability*** si chiede quanto è affidabile la risposta sulla singola probe presentata al sistema","x":6000,"y":3148,"width":780,"height":666},
		{"id":"f9faa215d5fddabb","type":"text","text":"# Doddington Zoo\n\nUn fan de La Fattoria degli Animali o dei Pink Floyd si è divertito a trovare un corrispettivo animale alle varie tipologie di utente in un sistema biometrico. In ordine di quanto siamo contenti di un utente del genere troviamo\n\n- Gli utenti che massimizzano la ***Genuine Acceptance*** (***buon match con se stessi***) sono\n\t- ***Colomba*** - Basso match con gli altri utenti e difficilmente impersonabili;\n\t- ***Pecora*** - Basso match con gli altri utenti ma più facilmente impersonabili.\n- Gli utenti che massimizzano la ***False Rejection*** (***basso match con se stessi***) sono\n\t- ***Capra*** - matchano male con i loro stessi tratti (False Rejection).\n- Gli utenti che matchano abbastanza bene con se stessi, ma hanno un'alta ***False Acceptance*** (vengono scambiati per qualcun altro). Convenzionalmente abbiamo\n\t- ***Lupo*** - gli è facile impersonare altri utenti;\n\t- ***Agnello*** - è facile che un lupo passi per un agnello.\n- Gli utenti simpatici, ai bordi sbagliati del grafico.\n\t- ***Fantasma*** - non matchano con nulla, nemmeno con se stessi; \n\t- ***Camaleonte*** - matchano con qualsiasi cosa;\n\t- ***Verme*** - chissà perché si chiama così. Probabile che verrà accettato, ma non perché matcha con se stesso. Potrebbe indicare un problema strutturale del sistema.\n\nBello, e che ci faccio?\n\nSe in fase di Enrollment riesco a stimare il tipo di soggetto, posso anche associargli una threshold ad-hoc in modo che non faccia troppi danni.","x":6000,"y":1440,"width":780,"height":661,"color":"4"},
		{"id":"8b674582fcf6a3f1","type":"text","text":"# Slides L 3\n\ndoddington zoo è utile perché se stimo in enrollment il tipo di soggetto posso anche associargli una threshold consigliata\n\neliminare i lupi -> aumentare le performance\n\ntutto questo in verification, perché in identification in generale non so chi sia la probe\n\nroll è facile da compensare, per la simmetria\npitch meno, per lo stesso motivo\nyaw facilissima\n\n","x":7120,"y":1040,"width":660,"height":339},
		{"id":"e8a2dd3299cc2c92","type":"text","text":"# Evaluation Reliability\n\nMetriche come FAR ed FRR non sono sufficienti a valutare la bontà di un modello, in quanto molto dipende dalla ***qualità*** del dataset e delle probes che gli vengono sottoposte.\n\nIn che senso? Se prendiamo la faccia, una buona metrica per valutare quanto il modello riesce ad estrarre correttamente le features possono essere gli angoli roll-pitch-yaw. Un'immagine frontale avrà una qualità ottimale, una dall'alto in cui si vedono solo i capelli un po' meno.\n\nQuesto per dire che una buona ***misura di qualità*** permette di migliorare le performance scartando il minor numero possibile di dati (e.g. vorrei chiederti meno volte possibile \"`scusa, non ho capito bene, puoi rimettere il dito sul lettore di impronte?`\").\n\nCi sono due punti critici in cui la qualità dei dati è un problema.\n\n- ***Singolo Template*** - Non riuscire ad estrarre \"buone\" features è un problema \"ovvio\", ma è limitato al singolo sample. Ho diverse metriche per valutare questo (e.g. ***UIQI***, ***SE***, ...);\n- ***Modello*** - Il modello può non essere in grado di separare bene le classi, e pertanto diverse risposte possono avere diversi livelli di ***System Response Reliability*** (***SRR***).\n\nTutto questo tenendo presente in ogni caso che possono esserci singoli utenti che nonostante tutto per qualche motivo sconosciuto creano problemi (cfr. ***Doddington Zoo***).","x":6000,"y":2355,"width":780,"height":541,"color":"6"},
		{"id":"028aba75caedfa66","type":"text","text":"# Stimare la Qualità del Singolo Template\n\nCome se fa a dire quanto è bella una singola immagine? Diversi metodi:\n\n- stimare la qualità media sull'intero dataset (BANCA Database)\n- margins based on error estimation (?)\n\nPer valutare la qualità dei template (prendiamo come esempio la faccia) potrei\n\n- Roll pitch yaw (SP)\n- omogeneità dei livelli di grigio (SI)\n- simmetria della faccia (SY)\n\nCome metodi più generali abbiamo\n\n- Universal Image Quality Index (UIQI): any image distortion as a combination of three factors: loss of correlation, luminance distortion, and contrast distortion\n- Sharpness Estimation Quality Index (SE): In order to estimate the sharpness of an image I of x × y pixels, we compute the mean of intensity differences between adjacent pixels, taken in both the vertical and horizontal directions:","x":7040,"y":2355,"width":780,"height":541},
		{"id":"9b9d2f3c4858c4e8","x":8060,"y":2769,"width":880,"height":254,"color":"4","type":"text","text":"# Misure di Qualità (Face-recognition)\n\nDiverse misure di qualità (SP, SI, ...) vengono usate su diversi dataset per face recognition (FERET, ...).\n\n- A sinistra, la pdf di ciascuna di esse su ciascun dataset. Prevedibilmente sono gaussiane;\n- A destra, il rate di immagini \"adeguate\" (i.e. di qualità sufficiente) data la scelta della threshold sulla misura di qualità. Prevedibilmente, più metto la soglia alta e meno immagini sono accettabili.\n\nUna buona misura di qualità dovrebbe avere una buona decrease di EER scartando meno sample possibili."},
		{"id":"5a7ff4295e8a033f","type":"file","file":"QualityMeasuresComparisonII.png","x":8540,"y":2162,"width":400,"height":387},
		{"id":"ac00a7e9478802aa","type":"file","file":"QualityMeasuresComparison.png","x":8060,"y":2162,"width":400,"height":387}
	],
	"edges":[
		{"id":"fb0a974e652c8a2b","fromNode":"f6da8b1d468646d5","fromSide":"bottom","toNode":"eed580368731a96b","toSide":"top"},
		{"id":"4baf0ebd927b08c9","fromNode":"efcdc18499da9576","fromSide":"bottom","toNode":"f6da8b1d468646d5","toSide":"top"},
		{"id":"b400fc2395efec78","fromNode":"eed580368731a96b","fromSide":"bottom","toNode":"c3c219f1c9689029","toSide":"top"},
		{"id":"ebc0862f7de85daa","fromNode":"066eda0b63762e4f","fromSide":"top","toNode":"4bf5870defbcf5c9","toSide":"bottom","label":"Verifica"},
		{"id":"ed69b891c135d5d4","fromNode":"066eda0b63762e4f","fromSide":"bottom","toNode":"437ae081305ab212","toSide":"top","label":"Identificazione"},
		{"id":"4c9a045aaf3a43ca","fromNode":"4c0ac5a15b1f23ad","fromSide":"bottom","toNode":"bcc60cdc04b5895d","toSide":"top"},
		{"id":"5a39fbc5b7553d67","fromNode":"bcc60cdc04b5895d","fromSide":"bottom","toNode":"f1175d1af9fe2990","toSide":"top"},
		{"id":"51a76407d854014f","fromNode":"c3c219f1c9689029","fromSide":"bottom","toNode":"4c0ac5a15b1f23ad","toSide":"top","label":"Il Modello"},
		{"id":"f8dc485d09a4f976","fromNode":"4c0ac5a15b1f23ad","fromSide":"left","toNode":"cd2095e4c31cfc60","toSide":"right"},
		{"id":"b17ec63688d0500f","fromNode":"cd2095e4c31cfc60","fromSide":"left","toNode":"b81e5ec8e65b5242","toSide":"right"},
		{"id":"f2f94e8cb219c5ae","fromNode":"eed580368731a96b","fromSide":"left","toNode":"8818849b9b5e0e2d","toSide":"right"},
		{"id":"cbdeab744f155415","fromNode":"bcc60cdc04b5895d","fromSide":"left","toNode":"334ff31ec21a80e3","toSide":"right"},
		{"id":"3dd75ff78f2450e2","fromNode":"bcc60cdc04b5895d","fromSide":"left","toNode":"350412f4fee5a3ef","toSide":"right"},
		{"id":"976c9ae939d13916","fromNode":"4bf5870defbcf5c9","fromSide":"top","toNode":"bec8557c6aabbfee","toSide":"bottom"},
		{"id":"e893d87074d91d10","fromNode":"437ae081305ab212","fromSide":"bottom","toNode":"4acb6b0dcd714593","toSide":"top"},
		{"id":"b1ecb18eb75f723f","fromNode":"437ae081305ab212","fromSide":"bottom","toNode":"f9bb82c22ea2981e","toSide":"top"},
		{"id":"0ed9b73d1ff6a25a","fromNode":"4acb6b0dcd714593","fromSide":"bottom","toNode":"0029f2bd8b83473f","toSide":"top"},
		{"id":"7cce9724a77c9257","fromNode":"c3c219f1c9689029","fromSide":"left","toNode":"cd2095e4c31cfc60","toSide":"top"},
		{"id":"ca0c810475b5eedf","fromNode":"066eda0b63762e4f","fromSide":"right","toNode":"25a2bf3a89b19ed9","toSide":"top"},
		{"id":"fb8ab18290c1e1ec","fromNode":"25a2bf3a89b19ed9","fromSide":"bottom","toNode":"21071ea6413c0796","toSide":"top"},
		{"id":"c45e8cead0d4b621","fromNode":"4acb6b0dcd714593","fromSide":"top","toNode":"21071ea6413c0796","toSide":"bottom"},
		{"id":"0e55f198c108c70f","fromNode":"437ae081305ab212","fromSide":"right","toNode":"21071ea6413c0796","toSide":"left"},
		{"id":"919fbe53e63e0385","fromNode":"21071ea6413c0796","fromSide":"right","toNode":"297b1973a951d1b5","toSide":"left"},
		{"id":"6cea3212c2bddb7b","fromNode":"297b1973a951d1b5","fromSide":"left","toNode":"21071ea6413c0796","toSide":"right"},
		{"id":"f67e1ba9d1cf65b2","fromNode":"25a2bf3a89b19ed9","fromSide":"bottom","toNode":"437ae081305ab212","toSide":"top"},
		{"id":"8f86f5502c2d0bf1","fromNode":"f9bb82c22ea2981e","fromSide":"bottom","toNode":"0f97a59a582d24b7","toSide":"top"},
		{"id":"3795f87ae965b8e0","fromNode":"25a2bf3a89b19ed9","fromSide":"right","toNode":"fc3058e5571aad4f","toSide":"left"},
		{"id":"b3d7e4cf4ca16e16","fromNode":"066eda0b63762e4f","fromSide":"right","toNode":"e8a2dd3299cc2c92","toSide":"left","label":"Affidabilità del Modello"},
		{"id":"298b6ea06db7d3db","fromNode":"e8a2dd3299cc2c92","fromSide":"top","toNode":"f9faa215d5fddabb","toSide":"bottom"},
		{"id":"00cb4b1c0b9631a7","fromNode":"c3c219f1c9689029","fromSide":"left","toNode":"8818849b9b5e0e2d","toSide":"bottom"},
		{"id":"d49effe909732c4f","fromNode":"c3c219f1c9689029","fromSide":"right","toNode":"066eda0b63762e4f","toSide":"left","label":"Valutare e Migliorare il Modello"},
		{"id":"d01d943071d0aeab","fromNode":"066eda0b63762e4f","fromSide":"left","toNode":"987aa01415d17eed","toSide":"bottom"},
		{"id":"9e8297080a2edbfc","fromNode":"0f97a59a582d24b7","fromSide":"right","toNode":"f1084d1ab8558173","toSide":"left"},
		{"id":"66028ade3a85c84b","fromNode":"c3c219f1c9689029","fromSide":"left","toNode":"298ea7f436357c7e","toSide":"right"},
		{"id":"eff160d06aa6c526","fromNode":"066eda0b63762e4f","fromSide":"left","toNode":"71532c4f2c736877","toSide":"top"},
		{"id":"09fce5d0787f7422","fromNode":"f4896017c69ba8cb","fromSide":"left","toNode":"f6da8b1d468646d5","toSide":"right"},
		{"id":"8c015246e4917d24","fromNode":"f6da8b1d468646d5","fromSide":"left","toNode":"08d4569a4c15afaf","toSide":"right"},
		{"id":"90933766cb01c0a7","fromNode":"e8a2dd3299cc2c92","fromSide":"bottom","toNode":"59d4e4543428278c","toSide":"top"},
		{"id":"3306ef2e12b3bba0","fromNode":"e8a2dd3299cc2c92","fromSide":"right","toNode":"028aba75caedfa66","toSide":"left"},
		{"id":"d7cb9b0bdbe37a25","fromNode":"9b9d2f3c4858c4e8","fromSide":"top","toNode":"ac00a7e9478802aa","toSide":"bottom"},
		{"id":"0b99adcf1aa69466","fromNode":"9b9d2f3c4858c4e8","fromSide":"top","toNode":"5a7ff4295e8a033f","toSide":"bottom"},
		{"id":"38a5bf2acdb320ce","fromNode":"028aba75caedfa66","fromSide":"right","toNode":"9b9d2f3c4858c4e8","toSide":"left"}
	]
}