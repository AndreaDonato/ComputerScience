{
	"nodes":[
		{"id":"4b10e86ce85f5765","type":"text","text":"","x":-12745,"y":-1509,"width":250,"height":60},
		{"id":"f6da8b1d468646d5","type":"text","text":"# Biometrics\n\nPer **biometrics** si intende la **capacità di riconoscere una persona dai suoi tratti somatici e/o comportamentali**, detti ***tratti biometrici*** (***TB***).\n\nÈ un'**alternativa all'autenticazione tramite oggetto** (e.g. \"Il tuo contatto è l'uomo con la valigetta nera\") **o  conoscenza** (e.g. \"Il tuo contatto conosce la password\"), ed è un buon **punto d'incontro tra facilità di utilizzo** (non richiede oggetti o memoria) **e precisione**.","x":-390,"y":860,"width":589,"height":261,"color":"6"},
		{"id":"efcdc18499da9576","type":"text","text":"# Premesse alla Biometrica: Classi e Pattern Recognition\n\nGli oggetti rappresentati nelle immagini possono essere raggruppati in diverse ***classi*** (e.g. fiori, volti, paesaggi), i cui elementi presentano caratteristiche uniche. Guardando questo insieme di caratteristiche, un osservatore è (idealmente) in grado di dire che \"questa immagine rappresenta un cane\".\n\nEntrando più nel dettaglio (i.e. osservando caratteristiche più complesse) potremmo addirittura scendere al livello delle ***sottoclassi***, e distinguere un pastore tedesco da un labrador.\n\nSe l'osservatore è un sistema informatico concettualmente cambia poco. La singola caratteristica che considero si chiama ***feature***, l'insieme delle caratteristiche utili a discriminare tra diverse classi si chiama ***feature vector*** (o ***pattern***), e il compito di riconoscere a quale (sotto)classe appartiene l'immagine in input viene eseguito da un modello di ***machine learning supervisionato*** (in ***classificazione***) detto ***pattern recognition***.\n\nDetto questo, ***la biometrica è un modello di pattern recognition in cui ogni individuo è una classe a sé stante***.\n\n\"Scusa, ma allora fa tutto l'algoritmo?\"\nSì, ma lo devi progettare. Ci sono tre questioni principali alle quali devi rispondere.\n\n- Pur vero che ***è il modello ad estrarre i pattern***, ***tu devi riconoscere quali dati grezzi è meglio fornirgli***.\n\t- Se provi ad identificare le persone dalla foto della spalla, il modello ce la metterà tutta ad estrarre i pattern migliori per discriminare tra le varie persone, ma se i dati grezzi non offrono nulla di significativo lui non troverà niente di significativo per discriminare, producendo pessime performance;\n\t- Questo include la ***diversificazione*** dei dati grezzi. Se ho scelto la faccia come dato grezzo ma ogni persona è rappresentata da una singola foto frontale con perfetta illuminazione e senza espressioni, il modello potrebbe avere qualche difficoltà se poi gli chiedo di riconoscere qualcuno che ride di profilo.\n- Devi anche ***metterlo nelle migliori condizioni di estrarre i pattern dai dati grezzi***.\n\t- Potrei fare una semplice Random Forest o Rete Neurale in classificazione e lasciare tutto all'algoritmo. Oppure ha più senso dividere il programma in moduli e dedicarne uno al processo di ***feature extraction***, il cui output viene poi dato in input del modulo che fa i calcoli sulle features.\n- Date le features devi dire al modello come discriminare tra esse, cioè ***definire la metrica di valutazione***.\n- Dati i risultati del modello, sta a te ***definire la tolleranza*** di accettazione o rifiuto tramite delle ***threshold***.\n\nTutto questo si applica a qualsiasi tipo di input, sia esso un'immagine, un audio, i dati di un accelerometro, ...","x":-554,"y":-200,"width":918,"height":839,"color":"4"},
		{"id":"4c0ac5a15b1f23ad","type":"text","text":"# Feature Extraction Module\n\nSe il FEM è al lavoro significa che ho già deciso quali sono i migliori dati grezzi da dare in pasto al modello (i.e. ho scelto di usare foto di volti, o fingerprint). A questo punto, a seconda di quello che ho scelto, esistono diversi metodi per l'estrazione delle features da quel particolare tipo di dati grezzi.\n\n- ***Algoritmi Classici*** - Utilizzano modelli geometrici e analisi matematica per estrarre le informazioni rilevanti;\n\t- Algoritmi come ***SIFT*** (**Scale-Invariant Feature Transform**) e ***SURF*** (**Speeded-Up Robust Features**) partono con un set di punti chiave di un oggetto (e.g. set di punti chiave di un occhio con relative distanze), cercano localmente nell'immagine delle zone interessanti a cui agganciare questi punti chiave (e.g. forti variazioni di intensità di colore dei pixel) e provano a fare una misura di match, restituendo tutte le ***porzioni locali*** di immagine in cui hanno fatto best match;\n\t- ***HOG*** (**Histogram of Oriented Gradients**) calcola il gradiente e restituisce un template che in pratica è l'***intera immagine*** con i contorni evidenziati;\n\t- Alcuni scelgono le features che meglio separano le classi (***riduzione dimensionale***). Esempi sono ***LDA*** (**Linear Discriminant Analysis**) e ***PCA*** (**Principal Component Analysis**).\n\t- Sempre utile la ***trasformata di Fourier*** per i segnali audio.\n- ***Algoritmi Statistici*** - Autoesplicativo, esempi sono ***LBP*** (**Local Binary Patterns**) per il riconoscimento facciale e la ***Wavelet Transform*** per la riduzione del rumore e la scomposizione dei segnali;\n- ***Machine Learning*** - Possono tornare utili algoritmi di ML classico come ***Random Forest*** e ***SVM*** (**Support Vector Machines**), soprattutto in caso di dati strutturati;\n- ***Neural Networks*** - Una NN che spara in output un FV. In questo caso ***non ho idea di cosa rappresentino i dati che estraggo***. E che mi importa? Che quando vado a fare il matching in teoria dovrei scegliere la metrica più adatta al tipo di dato in input, ma se non so qual è devo andare a tentativi.","x":-555,"y":3148,"width":918,"height":666,"color":"4"},
		{"id":"c3c219f1c9689029","type":"text","text":"# Schema Logico-Implementativo\n\nUna volta scelto il tipo dei dati grezzi, le fasi concettuali vengono generalmente implementate tramite 4 ***moduli***.\n\n- ***Dataset*** - I dati biometrici grezzi;\n- ***Feature Extraction Module*** - Dati i dati dei sensori, ne estrae le features per come previsto dal modello e le usa per costruire il feature vector;\n\t- Serve ovviamente compatibilità con i dati dei sensori. Se il modello si aspetta la foto di un volto non ha senso dargli una foto di una mano (proverà comunque a estrarne delle features per come è addestrato a fare, con risultati imprevedibili). Alcuni FEM come InsightFace hanno ad esempio un preventivo algoritmo di face localization prima di estrarre le features dall'immagine: se il volto non viene individuato, il FEM restituisce un errore.\n- ***Matching Module*** - Fa i calcoli e restituisce i match con i vari template. Questo può essere fatto con\n\t- ***Distanza*** - Se è \"piccola\" il match è \"buono\", e.g. distanza euclidea tra due FV;\n\t- ***Similarità*** - Più legata all'orientazione dei FV, in genere si calcola il coseno dell'angolo che formano. Contrariamente alla distanza, il match è \"buono\" se la similarità $\\to1$.\n- ***Decision Module*** - Dati i risultati del processo di matching, sceglie cosa farci sulla base delle politiche e delle threshold (che va scelta facendo fine tuning in base a cosa mi interessa ottenere (in genere l'obiettivo è  minimizzare il False Acceptance Rate).\n\nChiaramente li uso tutti insieme solo in fase di riconoscimento, in enrollment mi fermo al FEM.","x":-555,"y":2340,"width":918,"height":570,"color":"4"},
		{"id":"eed580368731a96b","type":"text","text":"# Schema Concettuale\n\nL'idea generale di un ***Biometric System*** (***BS***) è semplice, e consta di due semplici step.\n\n- ***Enrollment*** - ***Costruisce un DB*** catturando i dati biometrici grezzi (detti ***sample***, e.g. la foto del volto), estraendone le ***features*** (e.g. la distanza tra gli zigomi, dalla punta del naso al mento, ...) ed associando loro un'identità (e.g. \"Questo è Andrea\"). I ***template*** così ottenuti vengono raccolti nella ***gallery***.\n\t- Ovviamente è una fase preventiva all'azione, ma potrei continuare a raccogliere nuovi dati per il mio DB anche mentre sono in fase di riconoscimento.\n- ***Riconoscimento*** - Prendo il modello con il suo DB, gli fornisco gli opportuni dati grezzi in modo che lui possa estrarne le features (***probe***) e confrontarle con ciò che ha nel DB. Due scenari:\n\t- ***Verifica*** - L'utente dichiara di essere registrato. Il sistema confronta i suoi TB con quelli dell'utente che dice di essere (Controllo `1:1`);\n\t\t- FaceID di iPhone è un ***identity claim*** implicito.\n\t- ***Identificazione*** - L'utente non dichiara niente, sta al sistema capire chi è. Per fare ciò deve confrontare i suoi TB con tutti quelli presenti nel DB (Controllo `1:N`). A questo punto, anche qui, due scenari:\n\t\t- \"Questo tizio è sicuramente uno di quelli che hai nel DB\". Anche se la similarità è bassa, restituisco comunque il best match tra quelli che ho (***closed-set***), anche se così rischio l'errore;\n\t\t- \"Non è detto che tu sappia chi è\". Se ottengo una similarità bassa potrei scegliere di restituire un messaggio del tipo \"`Reject - Secondo me questo non sta nel DB`\" (***open-set***).\n\nTutto questo avviene sotto l'***assunzione che ogni persona sia unica***. Questo dipende dalle features. Se si usa il DNA è ovvio, ma in generale la fase di recognition non otterrà mai un match al $100\\%$ con la gallery, per motivi\n\n- ambientali, e.g. illuminazione e/o angolazione;\n- fisiologici, e.g. tengo la bocca aperta, rido, ...\n\nPer questo motivo, il modello deve prevedere una ***tolleranza*** entro cui gestire match multipli.","x":-555,"y":1360,"width":918,"height":718,"color":"4"},
		{"id":"8818849b9b5e0e2d","type":"text","text":"# Perché la Modularità? Perché non una NN?\n\nCosa mi impedisce di usare direttamente una NN in classificazione? Niente, se vuoi fallo.\n\nPerò devi rinunciare ad un sacco di cose:\n\n- Non sai quali features sta guardando la NN, quindi non ne hai il controllo;\n- Non è semplice ottimizzare la metrica che preferisci (e.g. FAR, FRR);\n- Non è generalizzabile (in quanto non modulare), cioè ti serve una NN per la verifica, una per l'identificazione, una per l'open-set, ...\n\nIn pratica, lasciare ad un unico monolitico modello di NN l'intero compito del riconoscimento biometrico è in generale una cattiva idea.","x":-1761,"y":1546,"width":760,"height":346,"color":"4"},
		{"id":"cd2095e4c31cfc60","type":"text","text":"# How To Features\n\nAnzitutto, cosa devo salvare nella gallery? Cioè, con quali criteri scelgo le features (i.e. i dati grezzi da cui poi il modello estrarrà delle buone features)?\n\nDevo scegliere ***TB in grado di identificare univocamente il generico essere umano***, pertanto devono essere\n\n- ***universali*** (i.e. posseduti da tutti, salvo rare eccezioni);\n- ***permanenti*** (i.e. che non possono cambiare nel tempo);\n- ***misurabili*** dai sensori (i.e. quantificabili in numeri);\n- la cui misura è ***accettabile*** dall'utente (i.e. che rispettino la privacy);\n- ***non-eludibili*** (i.e. non facilmente imitabili da artefatti).\n\nSimili TB sono detti ***strong features*** (***SF***), come ***impronte digitali***, ***retina*** e ***orecchie***.\n\nViceversa, un TB come il colore dei capelli è una ***weak feature*** (***WF***), in quanto non universale (impossibile distinguere tra le persone calve), non permanente (posso fare una tinta, o invecchiare, o usare una parrucca) e soprattutto non univoca.\n\nCi sono delle vie di mezzo? Ovviamente sì. I ***tratti comportamentali*** come ***gait*** (andamento della camminata), ***scrittura*** (in particolare la ***dinamica di movimento*** del polso e del braccio) e la ***voce*** sembrano da un lato una SF, ma dall'altro possono cambiare con l'età o con le condizioni fisiche e/o ambientali. Sicuramente sono ***più difficili da riprodurre*** da parte di un attaccante, e danno il meglio di sé quando usate in combinazione con altri TB (***Multimodal Biometrics***).\n\n","x":-1761,"y":3148,"width":760,"height":666,"color":"4"},
		{"id":"350412f4fee5a3ef","type":"file","file":"CosineSimilarity.png","x":-1060,"y":4646,"width":399,"height":86},
		{"id":"334ff31ec21a80e3","type":"file","file":"DTW.png","x":-1001,"y":4173,"width":283,"height":400},
		{"id":"bcc60cdc04b5895d","type":"text","text":"# Matching Module \n\nDati due template da comparare, ci sono sostanzialmente due alternative:\n\n- I template sono vettori (i.e. FV) contenenti misure di features significative (e.g. distanze tra i punti chiave di un volto, coordinate e orientazione delle minutiae sulle fingerprints). In questo caso posso cercare la ***minima distanza***.\n\t- Un esempio banale è la ***distanza euclidea*** (***L2***), o la ***distanza Manhattan*** (***L1***);\n\t- Uno meno banale è la ***distanza di Bhattacharyya***, che misura la ***sovrapposizione tra due pdf o istogrammi***.\n\t- Per dati dinamici (e.g. due tracce audio) si usa il ***Dynamic Time Warping*** (***DTW***), che con un processo non lineare confronta meglio due forme d'onda sfasate.\n- I template sono funzioni, istogrammi, oggetti per i quali non è ovvio definire una distanza o per i quali non è rilevante nel confronto. In questo caso si cerca la ***massima similarity***.\n\t- L'esempio più comune è la ***cosine similarity*** $S_C$, che guarda l'angolo individuato dai due vettori. $S_C$ è massima quando $\\cos\\vartheta$ vale $1$;\n\t- La ***Pearson Correlation*** valuta la ***correlazione lineare tra pdf e istogrammi***, quindi è sensibile al disallineamento (contrariamente al DTW).\n\nTutto questo assume che io sappia cosa rappresenta il template, ma questo non è vero se esso è generato da una NN. E quindi? Le provo tutte e vedo che succede!","x":-555,"y":4173,"width":918,"height":559,"color":"4"},
		{"id":"f1175d1af9fe2990","type":"text","text":"# Decision Module\n\nSulla base della threshold prendo la decisione.\n\nQuesta cosa è ovvia per un modello con un singolo tratto biometrico, altrimenti cfr. ***Multibiometrics***.","x":-555,"y":5001,"width":918,"height":139,"color":"4"},
		{"id":"221e8d65df6b0ef2","type":"text","text":"- **Watchlist**: Confronto mirato per verificare se una persona è in una lista ristretta di interesse.","x":-555,"y":6098,"width":250,"height":207},
		{"id":"b81e5ec8e65b5242","type":"text","text":"# Ma è una buona feature?\n\nAnche se lo sembra, ci sono alcuni dettagli da tenere in considerazione.\n\n- ***Wide Intra-Class Variations*** - La stessa persona può produrre sample molto diversi. Devo essere bravo a costruire una gallery rappresentativa di tutte le possibili varianti in cui la persona può presentarsi (e.g. bocca aperta, profilo, occhiali, ma anche parlare con il raffreddore);\n- ***Small Inter-Class Variations*** - Persone diverse possono produrre sample simili (e.g. gemelli, padre-figlio, ma anche sosia);\n- ***Noise*** - Questo riguarda più il modello, che dovrebbe essere robusto rispetto al rumore nei dati grezzi (e.g. cicatrici sulle dita, illuminazione non omogenea);\n- ***Non-Universality*** - Anche se scelgo un tratto abbastanza universale, devo sempre tenere in considerazione il fatto che qualcuno può non averlo;\n- ***Spoofing*** - Quanto è robusta rispetto ad chi prova ad impersonare un altro soggetto?\n\t- I tratti comportamentali sono in genere meno imitabili e/o falsificabili;\n\t- Se so come funziona il modello (e.g. prende la distanza tra i punti più luminosi e tra i punti più scuri) posso produrre un'immagine fittizia (***hand-crafted feature***) dalla quale il modello estrae esattamente quello che mi serve per superare il controllo (***white-box attack***, cioè conosco il funzionamento interno);\n\t\t- Questo è un grosso \"se\", perché in genere non so come funziona il modello (i.e. è una ***black-box***).\n\t- Può anche essere che l'attaccante non ci stia neanche davvero provando a fare spoofing, semplicemente i suoi tratti sono simili a quelli di un utente genuino (***zero-effort attack***).","x":-2771,"y":3148,"width":760,"height":666,"color":"4"},
		{"id":"19750c0603c5e6dd","type":"text","text":"The Biometric Consortium define biometrics as “automatic recognition of a person\naccording to discriminative characteristics”.","x":797,"y":-2815,"width":408,"height":215},
		{"id":"747086ac91be7c18","type":"text","text":"project + oral same session\n\nproject = build biometric system + performance evaluation, ma prima parla con lei. always mention the sources\n\nvene, retina, iride, orecchio sono tratti abbastanza univoci per fare pattern recognition\n\nfaccia importante, molte sue tecniche di pr si prendono per fare altro\n\nSVM utili. embeddings (set distanze tra punti) usati per fare training.\n\nbehavioural traits more difficult to reproduce for an attacker.\n\nverification - assess identity (i have claim of identity). dichiaro chi sono e l'algoritmo è comparativo con il DB di gente autorizzata ad entrare (e.g. quando sblocchi il telefono è un claim di identità implicito)\n- Questa roba deve essere sia strict che flessibile, perché se sono stanco e ho la faccia tirata non è che il telefono non si sblocca...\nidentification - (i dont). ho un DB pseudocompleto e riconduco l'input al best guess tra tutti quelli che ho.\n\ni tratti devono essere discriminativi. quelli che non lo sono (e.g. il colore dei capelli) sono considerabili \"soft biometrics\". assunzione che ogni persona sia unica (siccome non posso avere 100% accuracy questo in realtà non è vero, ma è una buona approx).\n\ndetto ciò, se individuo un buon tratto, quali features devo estrarne? mediapipe libreria per estrarre features. minuties? need triplets (x y orientazione).\n\nmanca un'intera lezione di 3h qui","x":1385,"y":-3040,"width":716,"height":766},
		{"id":"c6df0c468aac74b6","type":"text","text":"C'è tutta una pippa mentale sul tipo di utente che se hai voglia poi ti vedi\n\n- Cooperative: the user is interested in recognition (an impostor might try to be recognized as a legal user). \n- Non-cooperative: the user is indifferent or even adverse to recognition (an impostor might try to avoid being recognized)\n- Public/Private: users of the system are customers or employees of the entity installing the system\n- Used/Non used: frequency of use of the biometric system (more times a day, daily, weekly, monthly, occasionally …).\n- Aware/Not aware: the user is aware or not of the recognition process\n\ne poi sui setting\n\n- Controlled: capture settings can be controlled, distortions mostly avoided (e.g., for face, pose, illumination, and expression), defective templates can be rejected, and capture repeated\n- Uncontrolled/undercontrolled: capture settings cannot be controlled, template can present various levels of distortion, defective templates can be rejected, but capture cannot be repeated\n\nserve a qualcosa? ai posteri l'ardua sentenza","x":831,"y":-1928,"width":780,"height":663},
		{"id":"066eda0b63762e4f","type":"text","text":"# Performance Evaluation\nQuanto funziona bene questo giocattolo? Visto che di base è ML, dividiamo il dataset in\n\n- ***Training/Validation*** - I sample vengono divisi in modo pseudocasuale, eventualmente con l'aiuto di strumenti complessi come una ***K-Fold Cross-Validation*** per evitare bias.\n\t- Su questo set verrà eseguito il ***tuning*** dei parametri (i.e. viene appunto addestrato il modello) secondo le indicazioni delle metriche ottenute sul validation set;\n- ***Test*** - Viene usato per valutare la bontà del training. Dividiamo ulteriormente i sample in\n\t- ***Gallery*** - I dati di riferimento del sistema (e.g. una watchlist, i.e. la simulazione di quello che ho a disposizione nel momento in cui uso il modello), nella quale posso scegliere cosa includere (e.g. solo foto in ambiente controllato, immagini rappresentative del rumore) a seconda di quello che ci sia aspetta nella situazione reale. Il modello estrae le features dai soggetti labeled di questo set e prova a fare predizioni sulle probes.\n\t- ***Probe*** - La simulazione dei dati reali. Qui la scelta varia a seconda dell'applicazione:\n\t\t- Se sono in ***identification open-set*** (i.e. la probe potrebbe non essere nella Gallery) ha senso inserire sample di persone che non compaiono nella Gallery. Posso ovviamente anche non farlo, ma in generale le prestazioni di un simile modello dipendono dalla percentuale di soggetti non-identificabili (i.e. sorvegliare un aeroporto con una blacklist di poche persone costringe il modello ad incontrare un alto numero di individui che non compaiono nella Gallery).\n\t\t- Se sono in ***identification closed-set*** non ha senso mettere sample di persone che non compaiono nella Gallery. Il sistema proverà in ogni caso a dare una risposta al suo interno, producendo con tali sample un errore al $100\\%$;\n\t\t- Se sono in ***verifica*** questa scelta ovviamente non influenza le prestazioni.","x":2481,"y":2292,"width":796,"height":666,"color":"6"},
		{"id":"4acb6b0dcd714593","type":"text","text":"# Errori open-set\n\nIn questo caso ci sono due domande da porsi: Il soggetto è nel DB? Se sì, chi è?\n\nCom'è ovvio, due domande ci danno $4$ possibili scenari, di cui uno non si può mai verificare (se valuto che il soggetto non è nel DB non posso sapere chi è). Dal punto di vista del modello ci sono solo due casi, a cui seguono i sottocasi legati alle performance:\n\n- ***Alarm*** (`Il soggetto è nel DB`) - Il modello pensa di aver riconosciuto il soggetto.\n\t- Se il soggetto è davvero nel DB `Correct Detect`, dopodiché\n\t\t- se pensava fosse un altro (i.e. ha fatto `Correct Detect` per il motivo sbagliato) abbiamo `Incorrect Identification`, che si traduce in una ***FR***;\n\t\t- se ha indovinato anche l'identità `Correct Identification` (nel complesso abbiamo un ***Correct Result***). In questo caso si parla di ***Detect and Identification*** (***DI***).\n\t- Se il soggetto non è davvero nel DB abbiamo un ***falso allarme***, quindi sia una `Incorrect Detect` che una `Incorrect Identify`, che si traduce in una ***FA***.\n\t\t- Inviare $n\\gg1$ probe che non appartengono alla gallery permette di stimare il ***false alarm rate***. \n- ***No Alarm*** (`Il soggetto non è nel DB`) - Il modello non riconosce il soggetto.\n\t- Se davvero il soggetto non è nel DB, `Correct Result` i.e. ***GR***.\n\t- Se invece lo è, `Incorrect Result` i.e. ***FR***.\n\nA questo punto possiamo definire la CMC come ***Detect and Identification Rate at Rank*** $k$:$$DIR(t,k) = {|\\{p_j\\,:\\,rank(p_j)\\leq k, \\,s_{ij}\\geq t, \\,id(g_i)=id(p_j)\\}|\\over |P_G|}\\quad\\forall p_j\\in P_G=P_{\\text{Genuine}}$$Bene, a che serve? A calcolare il ***FRR***, in questo caso un ***False Negative Identification Rate*** (FNIR), come$$FRR(t) = 1 - DIR(t,1)$$cioè \"qual è la probabilità che il sistema non riconosca (al rank 1) un soggetto presente nella gallery?\". Analogamente posso chiedermi qual è la probabilità che il sistema accetti un soggetto non presente nella gallery (FAR, FPIR)$$FAR(t) = {|\\{p_j\\,:\\,\\max_i(s_{ij}\\geq t) \\}|\\over|P_N|}\\quad \\forall p_j\\in P_N \\,\\,\\wedge\\,\\, \\forall g_i\\in G$$Tutto questo mi è utile a calcolarmi la ***ROC***, che plotta come sempre FAR vs FRR.","x":3430,"y":5005,"width":960,"height":863,"color":"4"},
		{"id":"0029f2bd8b83473f","type":"text","text":"# How To Threshold (ROC)\n\nMigliore il modello, migliore la ROC (i.e. maggiore la AUC). Ma ogni possibile scelta di threshold è un compromesso. Come la scelgo? Abbiamo diversi scenari.\n\n- ***Minimizzare i falsi allarmi***, i.e. \"per dare l'`alarm` devi esserne proprio sicuro\";\n\t- Usi - Un evento che mobilita molte risorse, che crea panico o, o se non si vuole far scoprire con troppa facilità la presenza di un sistema di sicurezza;\n\t- Significa scegliere una threshold che sia ***più a sinistra possibile*** sulla ROC;\n- ***Massimizzare la DIR***, i.e. \"cerca di non farti sfuggire nessun potenziale elemento della Watchlist.\";\n\t- Usi - Ci sono due sotto-situazioni qui:\n\t\t- \"... ***anche a costo di produrre un gran numero di falsi allarmi***\", tipo ai controlli in aeroporto. Un falso allarme è gestibile, ed è preferibile ad una minaccia non vista;\n\t\t\t- Significa scegliere una threshold che sia ***più a destra possibile*** sulla ROC;\n\t\t- \"... ma cerca di non creare troppi falsi allarmi\", in situazioni in cui è difficile gestirli, come i controlli alla frontiera per trovare dei terroristi. Un falso allarme potrebbe creare panico.\n\t\t\t- Significa scegliere una threshold che sia ***a destra, ma non troppo***.\n- ***Bilanciare FAR e FRR***, i.e. \"ho tolleranza sia per i falsi positivi sia per i falsi negativi, basta che non siano troppi\".\n\t- Usi - Un sistema di sicurezza per l'accesso ad un edificio, in cui non è un problema qualche falso positivo (i.e. un utente autorizzato verrà riconosciuto da un umano) o qualche falso negativo (i.e. magari uno che entra per sbaglio cercando un bagno non mobilita la sicurezza, insomma basta che il rischio non sia troppo alto).\n\nCi sono situazioni in cui ***non è necessaria una threshold***: in un'investigazione, ad esempio, la polizia potrebbe voler vedere personalmente tutti i match score prodotti dal modello.","x":3430,"y":6001,"width":960,"height":641,"color":"4"},
		{"id":"b915a5ddb6f4acfa","type":"text","text":"# 2.1\n\naccuracy/loss va bene per training-validation (deep learning), ma quando vado in testing mi tocca usare le metriche più adatte per la biometrica\n\na volte i dataset contengono direttamente le distanze invece delle probes, ma è raro\na volte la gente è simpatica e costruisce un test set ad-hoc per pompare le performance, oppure lasciano che training e test sia overlappino per lo stesso motivo (...)\nse nel training sono sbilanciato (e.g. poche donne afroamericane di 20 anni) in testing faccio errori su quella categoria.\nreliability through good balancing, alcuni dataset suggeriscono opportune partizioni train-test.\nil training potrebbe non essere necessario (uso tipo tagli rettangolari?)\n\n\"Choice based on both subjects (a subject may not belong to2 he training set, to better test generalizability) and on samples (no overlap between TR and TS allowed!)\"\n\nfirst e second choice non sono alternative, ma consecutive (train/test prima e probe/galley poi).\nper probe/gallery devo poter usare la cross-validation (cerca appunti iocchi) (ma è parte del test? me sa). 70 train 10 validation 20 test\n\nnormalizzazione deve andare in \\[0,1\\] ma anche preservare le forme funzionali (similarity = 1-distance). \"easier to minimize distances than similarities\" (?) leggi bene probe vs gallery (slide 14)\n\nclosed set è importante solo l'ordine, negli altri casi anche i numeri\n\n\\[...\\]","x":9660,"y":4436,"width":812,"height":830},
		{"id":"237a1e0bfd57425c","type":"text","text":"in genere si sacrifica genuine acceptance per minimizzare false acceptance.\n\nse il TB cambia (e.g. invecchio) dovrei comunque essere in grado di riconoscere quel TB. fingerprints sono considerate universal and reliable ma possono esserci danni. voce non universale obv, cataratta per retina (o iride?), ...\n\nzero-error attack dichiaro di essere qualcun altro\nspoofing fare qualcosa to mislead recognition system (posso creare falso fingerprint con colle e simili, e seguono tecniche anti-spoofing). posso presentare la foto dell'iride di un'altra persona, o usare lenti a contatto.\n\nDNA univoco, ma se ricevo donazione di midollo potrebbe essere ambiguo.\nretina funziona bene. behavioral traits difficili da riprodurre. walking richiede strategia raffinata in modo molto specifico per ogni persona, difficile da riprodurre a lungo. signature, soprattutto la dinamica del polso che la fa.\n\nhand-crafted features = features... con cui creare il FV (si possono anche estrarre come embeddings, cioè il livello quasi-finale di una NN, anche detto latent space in quanto non interpretabile da un umano ma utilizzabile da una macchina)\n\nminutiae = bifurcation of lines, end of lines, ... (on fingerprint FP)\n\ncityblock (L1 distance) ce ne sono tipo mille, ma distinguo solo distanza e similarità. la covarianza è una misura di similarità, usata per FP. per confrontare gli istogrammi c'è la distanza di Bhattacharyya. in generale, ogni tipologia di gallery (i.e. tipo di template al suo interno) ha la sua misura ottimale. dyamic time warping per dati con accelerometri tipo, nel senso che confronta due funzioni considerando gli stretch e le fasi. in pratica prova a farle corrispondere. più è difficile più sono distanti. insomma, è la distanza da usare tra segnali(t).\n\nFP. in genere problema di orientazione e che spesso si trovano solo frammenti di FP. ci sono modi speciali di fare la distanza (NN).\n\nerrori di tipo I e II non sono ottimizzabili contemporaneamente, in genere serve trade-off. l'analisi degli errori si porta dietro tutti i problemi del training in ML.\n\nricorda che false rejection ha al denominatore il totale dei probes che should be accepted ($\\text{true but rejected}\\over\\text{TbR + True and Accepted}$) . FR e GA sum up to one (i.e. genuine probes). nei casi limite si lascia la scelta all'umano.\n\nil training è sempre supervised.\n\nmore template for same person to address intra-class variations. meglio pochi ma buoni che prendere 200 frame di un video in cui close frames sono simili e la risoluzione è bassa.","x":10099,"y":1560,"width":708,"height":1248},
		{"id":"6a9045ca7edf7d9f","type":"text","text":"# Spoofing\n\ndevo distinguere tra spoofing (provo ad impersonare qualcun altro) dal camouflage (nascondo i miei tratti biometrici, ad esempio facendomi tagli sulle dita)\n\nma se taglio un dito all'addetto della sicurezza e lo uso per aprire la porta? Ci sono delle tecniche per determinare se il pezzo di corpo che mostro è vivo o morto (e.g. micro variazioni di volume dovute al battito cardiaco)\n\nora, attacchi del genere sono attacchi diretti. posso però attaccare un qualsiasi punto dell'intero sistema (i.e. feature extractor, comparator, DB, ...)\n\ntexture = repeating pattern che copre la superficie. è stato uno dei primi task della computer vision (texture identification). texture of a mask is different from one of a real face.\n\nora, face recognition si basa solo sulla faccia, ignorando tutto il resto. se gli metto davanti una foto, ho automaticamente vinto. E quindi?\n\nse provo a fare replay attack facendo un video al video, ci sono due telecamere di mezzo che si sovrappongono ed esce fuori un effetto moire che posso riconoscere.\n\npoi camera termica (se sono freddo forse sono falso), reazioni involontarie e volontarie (\"alza un braccio\", oppure lo spaventi, fa ridere), ecc...\n\noptical flow? la roba per cui \"ruota la testa\" su una foto ti fa capire che è una foto. tipo.\n\nmedia pipe\n\nlocal binary pattern\n\n","x":10980,"y":2306,"width":660,"height":1201},
		{"id":"f9bb82c22ea2981e","type":"text","text":"# Errori closed-set\n\nDiamo per scontato che il soggetto sia nel DB. Ma chi è?\n\nValutare le performance di un siffatto sistema si traduce nel valutare\n$$CMC = CMS(k) = \\frac{|\\{ p_j \\in P \\, : \\, rank(p_j) \\leq k \\, \\wedge \\, id(p_j) = id(g_i) \\}|}{|P|}$$che fa le veci della curva ROC in open-set.","x":1341,"y":5005,"width":960,"height":258,"color":"4"},
		{"id":"0f97a59a582d24b7","type":"text","text":"# How To Threshold (ALL-VS-ALL)\n\nInvece di dividere (eventualmente più volte) il test in Probe e Gallery posso usare $D$ per effettuare un check ALL-VS-ALL: ogni sample fa sia da probe sia da elemento della Gallery, ignorando ovviamente gli elementi diagonali di confronto del sample con se stesso.\n\nÈ per costruzione una sorta di media delle possibili distribuzioni Genuine/Impostor, il che può essere un vantaggio per la generalizzabilità ma uno svantaggio per una task specifica.\n\n- Perché usarlo?\n\t- facile da programmare;\n\t- ogni probe risulta genuina solo alle varianti di se stessa (i.e. $A_2$ è genuina solo su $A_1$ e $A_3$), di conseguenza ne risulta un gran numero di impostori (i.e. $A_2$ è impostore su ogni $B_i$, ogni $C_i$, ...). Questo è utile come stress test se sto allenando un modello che avrà a che fare con un gran numero di tentativi di impostori.\n- Possibili problematiche?\n\t- potrebbe avere un alto costo computazionale;\n\t- fortemente aspecifico, tende ad appiattire ogni variabilità sia di applicazione che interna al dataset (e.g. se ho samples presi in momenti, condizioni e sensori diversi, questo metodo fa una media sommaria di tutte queste differenze che andrebbero invece incluse nella gallery per creare un campione rappresentativo).\n\nOgnuna delle $N$ righe effettua $N$ controlli, trovando\n\n- Impostore, se la label non corrisponde. Data la threshold decido se classificare questo matching come ***FA*** (il sistema accetta l'impostore) o come ***GR*** (lo rifiuta);\n- Genuino, se la label corrisponde. Questo succede solo con le proprie varianti, e anche qui a seconda della threshold e del punteggio distinguo tra ***GA*** ed ***FR***.\n\nRipetendo questo gioco al variare della threshold trovo quella ottimale.","x":1341,"y":5518,"width":960,"height":700,"color":"4"},
		{"id":"4bf5870defbcf5c9","type":"text","text":"# Errori in Verifica\n\nIn verifica, un soggetto viene accettato se la distanza tra la probe e i template associati all'***identità reclamata*** è minore di una certa ***threshold*** (e viceversa con la similarity). 4 casi:\n\n- ***Genuine Acceptance*** (***GA***) - il claim è vero, il sistema accetta;\n- ***False Rejection*** (***FR***) - il claim è vero, ma il sistema rifiuta (***Type-I Error***);\n\t- Gli si associa la FR Rate, ovvero la probabilità che un utente registrato sia rifiutato$$\\text{FRR}(t) = {\\text{\\#Claim di utenti registrati che vengono rifiutati}\\over\\text{\\#Claim di utenti registrati}}$$\n\t- Segue con analoghe definizioni che $\\text{GAR} = 1-\\text{FRR}$.\n- ***False Acceptance*** (***FA***) - il claim è falso, ma il sistema accetta (***Type-II Error***);\n\t- Qui si usa FA Rate, intuitivamente l'opposto della precedente$$\\text{FAR}(t) = {\\text{\\#Claim di impostori che vengono accettati}\\over\\text{\\#Claim di impostori}}$$\n- ***Genuine Rejection*** (***GR***) - il claim è falso, il sistema rifiuta.\n\t- Segue che $\\text{GRR} = 1-\\text{FAR}$.\n\nOvviamente il rischio maggiore è un Type-II, e in generale la valutazione delle performance si basa sul minimizzare gli errori, piuttosto che massimizzare le identificazioni corrette. In questo senso ci sono criteri come $\\text{ZeroFAR}$ (i.e. metto la threshold in modo che non ci sia alcun errore Type-II) e $\\text{ZeroFRR}$, ma anche valutazioni in cui accetto un bilanciamento in cui essi sono uguali ($\\text{EER}$, ***Equal Error Rate***) o trade-off come le curve ***ROC*** e ***DET***.\n\nFormalmente, sia `id(template)` la funzione che restituisce la ***ground truth*** (vera identità) del `template` che gli si passa. Sia $p_j$ la $j$-esima probe e $g_k$ il $k$-esimo template della gallery.$$\\text{FRR}(t) = \\frac{\\left| \\{ p_j : s_{xj} \\leq t, \\ \\text{id}(g_x) = \\text{id}(p_j) \\} \\right|}{\\left| \\{ p_j : \\text{id}(g_x) = \\text{id}(p_j) \\} \\right|}$$È banalmente la traduzione di quello che c'è scritto sopra, dove $s$ è la similarità e $t$ la threshold (identico, con la diseguaglianza al contrario, per la distanza). $g_x$ si intende ottenuto come output di una qualche funzione `TopMatch(p, identity)` che restituisce i template della gallery che *best-matchano* con `p`. Ciò detto il significato è chiaro:\n\n- Numeratore - Numero di probe $p_j$ la cui identità è la stessa del suo `TopMatch` che però avendo ottenuto un punteggio di similarità $s_{xj}$ minore di $t$ sono state rifiutate.\n- Denominatore - Numero di probe $p_j$ la cui identità è la stessa del suo `TopMatch`.\n\nDiscorso analogo per il gemello malvagio Type-II$$\\text{FAR}(t) = \\frac{\\left| \\{ p_j : s_{xj} \\geq t, \\ \\text{id}(g_x) \\neq \\text{id}(p_j) \\} \\right|}{\\left| \\{ p_j : \\text{id}(g_x) \\neq \\text{id}(p_j) \\} \\right|}$$A questo punto, come detto, uno in genere usa la ***ROC***.\n\nIl grafico ROC passa per $(0,0)$ e $(1,1)$: se metto la soglia troppo bassa rifiuto tutto (quindi sia $\\text{FAR}$ che $\\text{GAR}$ sono $0$), e viceversa. La ROC mi dice che ***potenzialmente posso scegliere una threshold lungo la curva***, quindi in generale più l'AUC è grande più sono contento, perché così posso fare scelte migliori. La scelta, tuttavia, è sempre un compromesso.","x":2482,"y":862,"width":796,"height":1216,"color":"4"},
		{"id":"bec8557c6aabbfee","type":"file","file":"SingleAndOverall.png","x":2483,"y":542,"width":795,"height":194},
		{"id":"987aa01415d17eed","type":"text","text":"# Overfitting?\n\nMa se il mio modello deve aspettarsi un gran numero di soggetti che non sono nella Gallery (e.g. la sorveglianza dell'aeroporto) non potrebbe avere senso \"ingegnerizzare\" anche la fase di training nello stesso spirito dello split Gallery/Probe?\n\n... ni. Ovviamente questa cosa introduce un ***bias intenzionale***, ma non necessariamente è una cosa negativa. Se trovi il giusto equilibrio potresti migliorare le performance per quella specifica applicazione, fermo restando che sicuramente perderai in generalizzabilità.\n\nIl rischio ovviamente è andare in ***overfitting***, se non trovi questo famoso \"giusto equilibrio\".","x":1039,"y":1928,"width":767,"height":300,"color":"4"},
		{"id":"f1084d1ab8558173","type":"file","file":"ALL-VS-ALL Pseudocode.png","x":2579,"y":5644,"width":600,"height":449},
		{"id":"298ea7f436357c7e","type":"text","text":"# Dataset, Gallery, Features, Templates...\n\n\nL'insieme delle features estratte dagli elementi della gallery prende il nome di ***template***.\n\nI template devono essere tali da\n\n- Non permettere la ricostruzione del dato grezzo originale, per questioni di privacy e sicurezza (in genere è comunque difficile fare reverse engineering);\n\n\nPosso continuare ad aggiungere dati alla gallery anche dopo che il modello ha finito il training\n\n- se riconosco un nuovo elemento e qualcuno si prende la briga di associargli una label (***supervised assignment***);\n- se una probe viene riconosciuta con abbastanza sicurezza (e.g. oltre una certa threshold) da indurre il modello ad aggiungerla in autonomia (***semi-supervised***).\n\nPosso inoltre scegliere di effettuare questa azione \"in diretta\" oppure \"ogni tot\".\n\nNon ho ben capito dove vuole arrivare questa parte...","x":-2266,"y":2340,"width":760,"height":570,"color":"2"},
		{"id":"71532c4f2c736877","type":"text","text":"# Metriche del Machine Learning\n\nHa senso ***affiancare*** alle misure di performance ad hoc per i BS quelle per il machine learning classico quali\n\n- ***Precision*** - Quante previsioni positive sono effettivamente corrette? $$\\text{Precision}={TP\\over TP+FP}$$Guarda la $\\text{Precision}$ se è importante ***evitare i falsi positivi***.\n- ***Recall*** - Quanti veri positivi riconosco come positivi?$$\\text{Recall}={TP\\over TP+FN}$$Guarda la $\\text{Recall}$ se è importante ***trovare tutti i positivi***.\n- ***F-Score*** - Quanto sono bilanciate $\\text{Precision}$ e $\\text{Recall}$? Un punteggio vicino a $0$ indica un forte sbilanciamento (i.e. una delle due è $\\sim0$) , un valore vicino a $1$ è auspicabile. A seconda di quanto peso dare a ciascuna, si usano diverse definizioni di $\\text{F-Score}$:\n\t- Se è più importante la $\\text{Precision}$ si usa (con una nomenclatura da Oscar)$$F_{0.5}={(1+0.25^2)\\cdot\\text{Precision}\\cdot\\text{Recall}\\over0.25\\cdot\\text{Precision}+\\text{Recall}}$$\n\t- Per un bilanciamento simmetrico si usa la ***media armonica***$$F_1=2\\cdot{\\text{Precision}\\cdot\\text{Recall}\\over\\text{Precision}+\\text{Recall}}$$\n\t- Se è più importante la $\\text{Recall}$ si usa$$F_2={(1+2^2)\\cdot\\text{Precision}\\cdot\\text{Recall}\\over4\\cdot\\text{Precision}+\\text{Recall}}$$","x":1039,"y":3103,"width":767,"height":756,"color":"4"},
		{"id":"25a2bf3a89b19ed9","type":"text","text":"# Distance Matrix\n\nData una divisione Probe/Gallery è possibile calcolare $D_{ij}=\\dist(\\text{probe}_i,\\text{gallery}_j)$. Una simile ***distance matrix*** è un check in cui per ogni probe sample si calcolano le distanze da tutti i gallery samples (***Probe-VS-Gallery***)\n\nOvviamente se sono in verifica non serve fare una cosa del genere: ogni probe sample ha una claimed identity associata, quindi confronto solo con quei sample della gallery.\n\nData la distance matrix $D$ e la threshold $t$, è facile verificare per ogni probe se\n\n- in ***verifica***, l'identità dichiarata ha un match $\\ge t$. Se è così, visto che ho le label sono in grado di dire se è stata una GA (user) o una FA (impostor). Viceversa, se ottengo un match $<t$, sono in grado di dire se è stata una GR (impostor) o una FR (user);\n- in ***identificazione*** ho diverse scelte.\n\t- In ***open-set***, se esiste almeno un match $\\ge t$, entro tale insieme posso ad esempio restituire il top match. Posso anche scegliere di restituire i primi $k$ match e valutare il sistema al rango $k$. In ogni caso, il sistema sputerà fuori un verdetto. Il primo check ovviamente è se il soggetto è o meno nella Gallery, dopodiché però devo anche predire correttamente la sua identità. Qua è un po' meno intuitivo definire i vari tipi di GA, FR e compagnia, quindi dedichiamo un rettangolino a parte;\n\t- In ***closed-set*** non c'è la threshold, ogni identificazione al rank $k$ incrementa ogni variabile $CMS(i\\ge k)$, i.e. se identifico al rank $k$ significa che ho identificato anche entro il rank $k+1$, $k+2$, ... Se identifico in posizione $1$, $CMS(1)$ è anche detto ***Recognition Rate*** (***RR***).\n\nÈ comune avere in gallery più di un sample per utente registrato. Questo in generale offre più possibili match, quindi riduce la FRR, ma può aumentare la FAR (ci sono anche più modi per un impostore di sembrare genuino).\n\nTutto questo gioco viene poi eventualmente ripetuto per diverse divisioni Probe/Gallery.","x":3430,"y":3148,"width":960,"height":666,"color":"4"},
		{"id":"fc3058e5571aad4f","type":"text","text":"# Usi principali della Distance Matrix\n\n- ***ALL-VS-ALL*** - Tendenzialmente in ***closed-set*** o se si hanno a disposizione pochi template. Ognuno di essi è confrontato con ogni altro template salvo se stesso. Non c'è alcuna divisione esplicita in Probe e Gallery, ogni template ha entrambi i ruoli;\n- ***ALL-VS-ALL-Gallery*** - Solo un subset dei template costituisce la gallery, ma ognuno di essi è usato come probe. Anche qui tendenzialmente ***closed-set***;\n- ***ALL-Probe-VS-ALL-Gallery*** - Quello che uno penserebbe sentendo la definizione di Distance Matrix. Divido i template in Probe e Gallery, quindi calcolo la matrice. Si può usare anche in ***open-set***.\n\n<span style=\"color:#FFA500\">Credo, eh! Non sono sicurissimo di ciò</span>\n\nSi possono poi usare altre tecniche più complesse come la stessa k-fold cross validation per dividere il test set in probe e gallery.\n\nTutto questo non ha molto senso in verifica (credo)","x":4620,"y":3148,"width":764,"height":666,"color":"3"},
		{"id":"21071ea6413c0796","type":"text","text":"# Un esempio di identificazione open-set\n\nNell'immagine, i numeri servono solo ad indicare l'ordine dei best match (i.e. non sono il punteggio di distanza).\n\n- La probe $P_1$ ha come identità $A$. Il sistema individua come top match il sample $A_2$ della Gallery. Se la distanza di tale match è minore della threshold $t$ scelta, avremo una $DI(1,t)$, i.e. una ***Detection and Identification at rank 1***, che è il miglior risultato possibile. Questo ci porta ad aumentare il counter delle buone notizie (i.e. `DI(1,t)++`);\n\t- Se però il valore del match è $\\ge t$, dal momento che $A_2$ è il top match nessun altro sample della Gallery può avere un punteggio migliore. Il sistema rifiuta, e di conseguenza abbiamo una ***FR***. Non serve tuttavia fare `FR++`: le false rejections possono essere dedotte dal DIR ($FR=N_{\\text{Genuine Users}}-DI(1,t)$, i.e. \"tutti quelli che dovevi identificare correttamente meno quelli che hai identificato correttamente\").\n- La probe $P_2$ ha come identità $D$, ma il sistema individua come top match il sample $B_2$. Siamo tutti d'accordo che il sistema ha sbagliato (i.e. ho una ***FR at rank 1***), ma se la distanza da $B_2$ e da $B_1$ è $<t$ posso controllare se lo è anche quella da $D_1$. In caso affermativo, posso comunque avere una $DI(3,t)$, i.e. ***identifico correttamente al rango 3***. Quanto è utile saperlo? Dipende da quanto sono permissivo con il mio sistema, i.e. entro a quale rank pretendo la risposta giusta;\n\t- Questo ci fa capire meglio per quale motivo non conto esplicitamente le FR: se mi interessa il rank $1$ le deduco da $DI(1,t)$, se mi interessa il rank $k$ le deduco da $DI(k,t)$ (i.e. $FR(k)=N_{\\text{Genuine Users}}-DI(k,t)$)\n- La probe $P_3$ è un impostore, quindi la casistica è più semplice: se il top match è $\\ge t$ avrò ***GR***, altrimenti ***FA***. Nota che questi devo per forza contarli, perché non posso dedurli in nessun altro modo.","x":3431,"y":4185,"width":960,"height":537,"color":"4"},
		{"id":"437ae081305ab212","type":"text","text":"# Errori in Identificazione\n\nQuando arriva una probe, vengono calcolati tutti i match score con gli elementi della gallery, quindi ordinati in ordine decrescente. Il giusto match potrebbe essere quello con il massimo score (quindi trovarsi in posizione $1$) o nella generica posizione $k$.\n\n$k$ è detto ***rango*** dell'identificazione. Con questa nozione, è chiaro che se richiediamo che la giusta identità venga restituita a rank $1$ le performance saranno peggiori rispetto ad una situazione più morbida in cui ci sta bene il rango $2$.\n\nIn generale, definiamo il ***Cumulative Match Score at Rank $k$*** (***CMS***) come la probabilità che la corretta identificazione avvenga entro la posizione $k$ della lista ordinata. Il plot delle varie CMS per i vari valori di $k$ è detta curva ***CMC*** (***Cumulative Match Characteristic***).\n\nLa definizione matematica di CMS è diversa nei casi di\n\n- ***open-set***, in cui è rilevante $CMS(k=1)$ per costruire la curva ROC;\n- ***closed-set***, in cui si usa direttamente $CMC$.","x":2482,"y":4213,"width":796,"height":480,"color":"4"},
		{"id":"f4896017c69ba8cb","type":"text","text":"# Breve nota storica\n\nNel 1882 il grande capo della polizia di Parigi Bertillon decise di iniziare a prendere varie misure biometriche del corpo dei detenuti (e.g. forma della mano, della testa, dettagli facciali).\n\nQuesta roba ebbe un sacco successo, tanto che nel 1896 anche l'FBI iniziò a schedare le persone e nel 1900 queste carte divennero regolamentate dalla legge francese.\n\nQualcuno però storse il naso. Per la privacy? No, perché questa roba era inefficiente. Tale Galton introdusse il concetto di ***minutiae***, ovvero piccolissimi tratti distintivi in grado di identificare in modo univoco una persona. Un esempio? Data una ***fingerprint***, le minutiae sono i punti in cui le linee terminano o si dividono.\n\n","x":1123,"y":791,"width":600,"height":399,"color":"4"},
		{"id":"08d4569a4c15afaf","type":"text","text":"# Implementazioni\n\nI tratti biometrici hanno come variabili più importanti l'***accuracy***, la ***reliability*** e l'***acceptability***. Nessun tratto conosciuto riesce a massimizzare tutti e tre contemporaneamente. Un buon compromesso è generalmente la faccia.\n\nLe tecniche utilizzate per la faccia sono abbastanza generalizzabili da essere usate potenzialmente per qualsiasi cosa.","x":-8065,"y":860,"width":645,"height":261,"color":"6"},
		{"id":"a3d2e59f3d6b2d43","type":"text","text":"### **Riconoscimento Facciale (Face Recognition)**\n\n- **FEM basati su Deep Learning**:\n    - **FaceNet**: Una delle architetture più popolari per l'estrazione di feature per il riconoscimento facciale. Utilizza una **loss triplet** per imparare una rappresentazione ottimale del volto, in modo che volti simili siano vicini nello spazio delle embedding e volti diversi siano lontani.\n    - **VGG-Face**: Basato sull'architettura VGG, questo modello è stato progettato per estrarre feature facciali rappresentative. Le sue embedding sono molto utilizzate per confronti facciali.\n    - **ArcFace**: Un modello avanzato che utilizza una particolare funzione di loss chiamata **Additive Angular Margin Loss** per migliorare la discriminazione tra diverse classi di volti, generando embedding molto efficaci per il confronto.\n- **FEM tradizionali**:\n    - **Local Binary Patterns (LBP)**: È una tecnica che cattura texture e caratteristiche locali del volto, molto usata nei primi metodi di riconoscimento facciale.\n    - **Histogram of Oriented Gradients (HOG)**: Analizza le direzioni dei gradienti nell'immagine per rappresentare le forme e i contorni del volto.\n\n### 2. **Impronte Digitali (Fingerprint Recognition)**\n\n- **FEM basati su Deep Learning**:\n    - **DeepPrint**: Un sistema che usa reti neurali profonde per estrarre feature dalle impronte digitali. Il modello estrae le caratteristiche importanti come i minuzie (biforcazioni, terminazioni) e genera embedding per il matching.\n- **FEM tradizionali**:\n    - **Minutiae-based Methods**: Sono i metodi più comuni per l'estrazione di feature dalle impronte digitali. Identificano le **minuzie**, come biforcazioni e terminazioni dei solchi, per rappresentare un'impronta in modo univoco.\n    - **Orientation Field**: Questo metodo estrae le direzioni principali dei solchi in diverse regioni dell'impronta, che possono poi essere confrontate tra diverse impronte.\n\n### 3. **Riconoscimento della Voce (Speaker Recognition)**\n\n- **FEM basati su Deep Learning**:\n    \n    - **x-vector**: È un modello di deep learning che estrae vettori (x-vectors) dalle caratteristiche vocali. Questi vettori sono utilizzati per il riconoscimento del parlante, basandosi sul timbro e altre caratteristiche della voce.\n    - **Deep Speaker**: Un altro modello basato su deep learning che si occupa di estrarre feature dalla voce. Il modello apprende a generare embedding vocali che possono essere usate per identificare un parlante.\n- **FEM tradizionali**:\n    \n    - **Mel Frequency Cepstral Coefficients (MFCC)**: Una delle tecniche più comuni per l'estrazione delle caratteristiche vocali. Estrae le **componenti spettrali** della voce che possono essere usate per confrontare i diversi speaker.\n    - **Linear Predictive Coding (LPC)**: Un altro metodo per rappresentare il segnale vocale, modellando come il tratto vocale filtra il segnale sonoro.\n\n### 4. **Riconoscimento dell'Iride (Iris Recognition)**\n\n- **FEM tradizionali**:\n    - **Daugman’s Algorithm**: Un algoritmo molto utilizzato per l'estrazione di feature dall'iride. Cattura l'informazione del pattern di texture dell'iride e la rappresenta con un **codice binario** (iris code).\n    - **Wavelet-based Methods**: Tecniche che utilizzano wavelet per analizzare il pattern di texture dell'iride a varie scale e livelli di risoluzione.\n\n### 5. **Riconoscimento della Camminata (Gait Recognition)**\n\n- **FEM tradizionali**:\n    - **Gait Energy Image (GEI)**: Un metodo che utilizza una serie di immagini silhouette della camminata di una persona per estrarre un'immagine energetica media che rappresenta il modello di camminata.\n    - **Motion History Image (MHI)**: Cattura il movimento attraverso una sequenza temporale di frame e rappresenta la variazione del movimento in una singola immagine.\n- **FEM basati su Deep Learning**:\n    - **GaitNet**: Un'architettura basata su reti neurali che apprende direttamente i modelli di camminata da sequenze video e genera embedding per distinguere tra le persone.\n\n### Conclusione:\n\nOltre alla rete neurale che puoi usare per estrarre feature, ci sono FEM specifici per ogni tipo di input (facce, impronte, voce, ecc.). Questi moduli possono essere sia **deep learning-based** che **metodi più tradizionali**, e la scelta dipende dal tipo di dati e dalle performance richieste per l'applicazione.","x":3600,"y":-1698,"width":1356,"height":1567},
		{"id":"59d4e4543428278c","type":"text","text":"# System Response Reliability (SRR)\n\nLa ***system response reliability*** si chiede quanto è affidabile la risposta sulla singola probe presentata al sistema. Quando provo ad identificare una classe è possibile che questa sia circondata da una nuvola di altre classi. Se la nuvola è densa, la confusione tra due risposte è molto probabile, viceversa le classi sono ben separate.$$\\f_1={d_2-d_1\\over d_{max}}$$valuta la differenza normalizzata le due distanza del best match e del secondo best match. Si capisce che se le distanze sono simili il numeratore tende a 0, viceversa più sono distanti più la frazione tende ad 1. Posso valutare la densità della nuvola di candidati vicino al best match:$$\\f_2=1-{|N_b|\\over|G|}$$dove $|N_b|$ è il numero di candidati entro un certo range dal best match e $|G|$ è la dimensione dell'intera galleria. Anche qui, valori \"bassi\" indicano che \"vicino\" al best match ci sono \"quasi tutti\" gli elementi di $G$, mentre valori prossimi a $1$ significano \"quasi nessuno\".\n\nBene, e che ci faccio? Essendo una misura di qualità di separazione delle classi, devo stabilire una soglia $\\f_k$ tale che se il best match ottiene un punteggio superiore viene accettato, viceversa viene rifiutato. Questo porta potenzialmente ancora a FA e FR. La miglior scelta di $\\f_k$ minimizza le FA e cerca di minimizzare le FR.\n\n<span style=\"color:#FFA500\">Qua la definizione di SRR non ha senso per me...</span>","x":6000,"y":3148,"width":780,"height":666,"color":"3"},
		{"id":"f9faa215d5fddabb","type":"text","text":"# Doddington Zoo\n\nUn fan de La Fattoria degli Animali o dei Pink Floyd si è divertito a trovare un corrispettivo animale alle varie tipologie di utente in un sistema biometrico. In ordine di quanto siamo contenti di un utente del genere troviamo\n\n- Gli utenti che massimizzano la ***Genuine Acceptance*** (***buon match con se stessi***) sono\n\t- ***Colomba*** - Basso match con gli altri utenti e difficilmente impersonabili;\n\t- ***Pecora*** - Basso match con gli altri utenti ma più facilmente impersonabili.\n- Gli utenti che massimizzano la ***False Rejection*** (***basso match con se stessi***) sono\n\t- ***Capra*** - matchano male con i loro stessi tratti (False Rejection).\n- Gli utenti che matchano abbastanza bene con se stessi, ma hanno un'alta ***False Acceptance*** (vengono scambiati per qualcun altro). Convenzionalmente abbiamo\n\t- ***Lupo*** - gli è facile impersonare altri utenti;\n\t- ***Agnello*** - è facile che un lupo passi per un agnello.\n- Gli utenti simpatici, ai bordi sbagliati del grafico.\n\t- ***Fantasma*** - non matchano con nulla, nemmeno con se stessi; \n\t- ***Camaleonte*** - matchano con qualsiasi cosa;\n\t- ***Verme*** - chissà perché si chiama così. Probabile che verrà accettato, ma non perché matcha con se stesso. Potrebbe indicare un problema strutturale del sistema.\n\nBello, e che ci faccio?\n\nSe in fase di Enrollment riesco a stimare il tipo di soggetto, posso anche associargli una threshold ad-hoc in modo che non faccia troppi danni.","x":6000,"y":1440,"width":780,"height":661,"color":"4"},
		{"id":"8b674582fcf6a3f1","type":"text","text":"# Slides L 3\n\ndoddington zoo è utile perché se stimo in enrollment il tipo di soggetto posso anche associargli una threshold consigliata\n\neliminare i lupi -> aumentare le performance\n\ntutto questo in verification, perché in identification in generale non so chi sia la probe\n\nroll è facile da compensare, per la simmetria\npitch meno, per lo stesso motivo\nyaw facilissima\n\n","x":7120,"y":1040,"width":660,"height":339},
		{"id":"e8a2dd3299cc2c92","type":"text","text":"# Evaluation Reliability\n\nMetriche come FAR ed FRR non sono sufficienti a valutare la bontà di un modello, in quanto molto dipende dalla ***qualità*** del dataset e delle probes che gli vengono sottoposte.\n\nIn che senso? Se prendiamo la faccia, una buona metrica per valutare quanto il modello riesce ad estrarre correttamente le features possono essere gli angoli roll-pitch-yaw. Un'immagine frontale avrà una qualità ottimale, una dall'alto in cui si vedono solo i capelli un po' meno.\n\nQuesto per dire che una buona ***misura di qualità*** permette di migliorare le performance scartando il minor numero possibile di dati (e.g. vorrei chiederti meno volte possibile \"`scusa, non ho capito bene, puoi rimettere il dito sul lettore di impronte?`\").\n\nCi sono due punti critici in cui la qualità dei dati è un problema.\n\n- ***Singolo Template*** - Non riuscire ad estrarre \"buone\" features è un problema \"ovvio\", ma è limitato al singolo sample. Ho diverse metriche per valutare questo (e.g. ***UIQI***, ***SE***, ...);\n- ***Modello*** - Il modello può non essere in grado di separare bene le classi, e pertanto diverse risposte possono avere diversi livelli di ***System Response Reliability*** (***SRR***).\n\nTutto questo tenendo presente in ogni caso che possono esserci singoli utenti che nonostante tutto per qualche motivo sconosciuto creano problemi (cfr. ***Doddington Zoo***).","x":6000,"y":2355,"width":780,"height":541,"color":"6"},
		{"id":"028aba75caedfa66","type":"text","text":"# Stimare la Qualità del Singolo Template\n\nCome se fa a dire quanto è _bella_ una singola immagine? Diversi metodi:\n\n- Per situazioni specifiche come la ***faccia***, anzitutto individuo il contorno del volto, poi piazzo i ***landmarks*** (i.e. punti sul contorno, occhi, naso, ...), poi ho più scelte, ad esempio\n\t- ***Selfie Photo*** (***SP***) - Un'immagine frontale può essere considerata ottimale. Abbiamo tre angoli per discostarci: ***Roll*** (lungo $x$ uscente dal foglio, i.e. l'immagine ruota \"a destra e sinistra\"), ***Pitch*** (in $\\tD$ è l'asse $y$, in $\\dD$ è il classico asse $x$, i.e la testa si inclina in avanti o indietro) e ***Yaw*** (lungo $z$ che in $\\dD$ sarebbe $y$, i.e. il movimento di \"scuotere la testa\"). In genere ogni singolo angolo deve essere $<30°$, complessivamente posso valutare la misura tramite una roba del tipo$$SP=\\a(1-roll)+\\b(1-yaw)+\\g(1-pitch)$$\n\t- ***Spectral Information*** (***SI***) - Valuta la presenza di zone troppo chiare o troppo scure. Un' alta ***omogeneità dei livelli di grigio*** implica (almeno entro questa approssimazione) contrasti naturali e non deformati. Possiamo valutarla facendo la deviazione standard dei livelli di grigio di tutti i pixel dell'immagine, ovvero$$SI = 1-F\\bigg(\\s\\big(\\{\\text{pixel}\\}_i\\big)\\bigg)$$dove $F$ è una qualche funzione che normalizza i valori in $[0,1]$. Bassa SI implica buona qualità, e viceversa;\n\t- ***Simmetria*** (***SY***) - Prendo i landmarks sul contorno del volto e valuto la differenza tra coppie di punti omologhi (e.g. livelli di grigio, gradienti con i punti circostanti, ...).\n\nCome metodi più generali abbiamo\n\n- ***Universal Image Quality Index*** (***UIQI***) - Una misura che, date due immagini $x$ ed $y$, è definita come$$Q={4\\,\\,\\meanx\\,\\meany\\,\\,\\s_{xy}\\over(\\s^2_x+\\s^2_y)(\\meanx^2+\\meany^2)}=4\\,{\\meanx\\,\\meany\\over\\meanx^2+\\meany^2}\\,{\\s_{xy}\\over\\s^2_x+\\s^2_y}$$\n\t- I valori medi $\\meanx$ e $\\meany$ tengono conto della ***luminanza media*** delle due immagini;\n\t- La ***correlazione*** e le due varianze valutano quanto le due ***distribuzioni*** di luminanza sono simili. \n- ***Sharpness Estimation Quality Index*** (***SE***) - Valuto l'intensità media del contrasto tra pixel vicini.\n\n\nOgni applicazione può scegliere tuttavia un diverso e specifico metodo, ad esempio\n\n- Stimare la ***qualità media sull'intero dataset***, valutando poi la qualità della singola immagine rispetto alla media (e.g. in BANCA Database è legata al blurring dell'immagine);\n- <span style=\"color:#FFA500\">margins based on error estimation (?) slide 29 del pacco 3.0</span>\n","x":7000,"y":2152,"width":940,"height":948,"color":"4"},
		{"id":"9b9d2f3c4858c4e8","type":"text","text":"# Misure di Qualità (Face-recognition)\n\nDiverse misure di qualità (SP, SI, ...) vengono usate su diversi dataset per face recognition (FERET, ...).\n\n- A sinistra, la pdf di ciascuna di esse su ciascun dataset. Prevedibilmente sono gaussiane;\n- A destra, il rate di immagini \"adeguate\" (i.e. di qualità sufficiente) data la scelta della threshold sulla misura di qualità. Prevedibilmente, più metto la soglia alta e meno immagini sono accettabili.\n\nUna buona misura di qualità dovrebbe avere una buona decrease di EER scartando meno sample possibili.","x":8180,"y":2769,"width":880,"height":254,"color":"4"},
		{"id":"ac00a7e9478802aa","type":"file","file":"QualityMeasuresComparison.png","x":8180,"y":2162,"width":400,"height":387},
		{"id":"5a7ff4295e8a033f","type":"file","file":"QualityMeasuresComparisonII.png","x":8660,"y":2162,"width":400,"height":387},
		{"id":"fd34db194919dca3","type":"text","text":"# Gamma, Contrasto, Luminance\n\nIl punto di questa roba è il seguente: l'occhio umano non risponde né in modo lineare rispetto alla luminosità di un immagine né in modo uniforme rispetto ai diversi colori dello spettro.\n\nQuindi, la prima cosa che fa uno schermo quando gli arrivano le informazioni di un'immagine (i.e. una matrice di pixel con le rispettive intensità RGB) è quello di applicare una <span style=\"color:#00b050\">***gamma-compression***</span>  i.e. una trasformazione non lineare definita come$$V_{out}=V_{in}^\\g\\quad\\text{(valori normalizzati)} \\quad\\iff\\quad V_{out}=\\bigg({V_{in}\\over255}\\bigg)^\\g\\times255\\quad\\text{(valori }\\in[0,255])$$che come effetto una modifica del <span style=\"color:#00b050\">***contrasto***</span> tra pixel più luminosi e meno luminosi.\n\nAttenzione: il <span style=\"color:#cd0a0a\">***contrasto***</span> dell'immagine è altra cosa rispetto alla <span style=\"color:#00b050\">***gamma***</span>! Mentre la <span style=\"color:#00b050\">***gamma***</span> modifica l'<span style=\"color:#00b050\">***intensità del singolo colore***</span> tramite una trasformazione non lineare avendo come effetto \"collaterale\" di aumentare il <span style=\"color:#00b050\">***contrasto***</span>, il <span style=\"color:#cd0a0a\">***contrasto***</span> è una trasformazione lineare che avvicina o allontana la <span style=\"color:#cd0a0a\">***luminosità complessiva***</span> del pixel rispetto ad un valore mediano di <span style=\"color:#cd0a0a\">***threshold***</span> che divide i pixel \"scuri\" da quelli \"chiari\".\n\nOra, tutto questo avviene per farci percepire le immagini più vivide in termini di contrasto. Tuttavia i recettori dell'occhio umano non sono ugualmente sensibili ai tre colori RGB. Per questo motivo, la ***luminanza*** di un pixel non è banalmente la somma dei valori $R+G+B$ (o meglio, si può anche fare così, ma questo non tiene conto della percezione umana), piuttosto una media pesata$$Y=0.299×R+0.587×G+0.114×B$$dove i coefficienti sommano a $1$ (i.e. $Y$ è normalizzata nello stesso range di RGB) ed i loro rapporti emulano i rapporti di sensibilità dei recettori visivi umani. Spesso i valori RGB che compaiono in questa formula sono già gamma-compressi, ed è su $Y$ che si calcola la <span style=\"color:#cd0a0a\">threshold del contrasto</span> (spesso è a metà, i.e. $127$). ","x":-15581,"y":-122,"width":880,"height":671,"color":"4"},
		{"id":"18a2bf0c7ac250e6","type":"text","text":"# Chrominance, Hue, Saturation (YUV, HSV)\n\nIl punto è sempre quello: l'occhio umano non percepisce le variazioni in modo lineare. Ma non solo! Essendo RGB uno spazio additivo, quello che percepiamo è il mix di colori, che risulta nella luminance. Quindi vediamo bene le differenze di luminance, ma meno quelle di colore.\n\nPer risolvere questo problema cambiamo base di rappresentazione. Invece RGB usiamo ***YUV***, ovvero luminance ($Y$), blu-differenza ($U=B-Y$) e rosso-differenza ($V=R-Y$). La coppia $UV$ prende il nome di ***Chrominance***, ed essendo l'occhio umano meno sensibile alle sue variazioni è possibile indirizzare la precisione verso la Luminance $Y$ sacrificando la Chrominance $UV$ (i.e. usato per comprimere i file video).\n\nUn'altra possibile rappresentazione per le immagini è ***HSV***, in cui separo\n\n- ***Hue*** - La tinta, i.e. il colore in sé. Cambiare H significa cambiare colore mantenendo invariata la luminosità (i.e. l'intensità della luce emessa dai pixel) e quanto questo è \"diluito\" (i.e. la saturazione). Viene rappresentato come cerchio cromatico ($0°=$ rosso, $120°=$ verde, $240°=$ blu);\n- ***Saturation*** - Quanto un colore è intenso o diluito/sbiadito;\n- ***Value*** - Un altro modo per chiamare la luminosità. A prescindere dal colore, $0=$ nero e $1=$ bianco.\n\nNessuno di questi sistemi viene letto in modo lineare dall'occhio umano (i.e. cambiamenti lineari di Hue, di Saturazione o di Chrominance non corrispondono mai a percezioni di cambiamenti lineari nell'occhio).","x":-16640,"y":-122,"width":880,"height":671,"color":"4"},
		{"id":"c1d6332ea1b81bf0","type":"text","text":"# Face Localization\n\nPrima di runnare un algoritmo di riconoscimento facciale, bisogna anzitutto trovare la faccia. Bene, come?\n\nIndipendentemente da orientazione, scala, illuminazione e via dicendo, l'algoritmo deve essere in grado di identificare la posizione della faccia all'interno dell'immagine. Inizialmente ci si basava su classificatori che davano in output una roba del tipo `sì, questa immagine è una faccia` oppure `no, non lo è`. Approcci più recenti prevedono di riuscire ad agganciare alla faccia una serie di ***tracking points***, i.e. confrontare il contorno e i punti d'interesse della figura che penso che sia una faccia con un set di punti che normalmente identificano una faccia.\n\nSeguono esempi significativi: Hsu. Mottaleb, Jain (2002) e Viola-Jones (2004).","x":-15581,"y":829,"width":880,"height":324,"color":"6"},
		{"id":"f4cc52883462f784","type":"text","text":"# Face Recognition\n\nLa faccia ha una discreta accuracy e un discreto livello di accettabilità/invasività, nonché un'alta universalità. È nel complesso un ***buon compromesso*** a livello di qualità di tratto biometrico.\n\nEssendo le immagini soggette a forti differenze intra-classe, è comune prevedere nella gallery delle varianti ***PIE*** (***Pose***, ***Illumination***, ***Expression***), in cui la stessa persona viene immortalata al variare di questi tre parametri, ai quali viene eventualmente aggiunto l'***Aging***.","x":-14299,"y":829,"width":678,"height":324,"color":"6"},
		{"id":"bd684ec7a9167851","type":"text","text":"# LDA\n\nLa ***PCA*** ha qualche problema, in quanto si limita a massimizzare la varianza globale senza curarsi della varianza delle singole classi (i.e. è un algoritmo ***unsupervised***).\n\nSi pone allora il problema di massimizzare la varianza inter-classe e al contempo minimizzare la distanza intra-classe, realizzando un algoritmo ***supervised***. La ***Linear Discriminant Analysis*** (o ***Discriminante Lineare di Fisher***) fa esattamente questo tramite una ***trasformazione lineare*** $W$.\n\nDistinguiamo due tipi di varianze, qui chiamate ***scatter***. Lo ***scatter intra-classe*** $S_W$ è la varianza dei dati appartenenti alla stessa classe, il cui baricentro $\\mu$ è detto centroide$$S_W=\\sum_{i=1}^k\\sum_{x\\in C_i}(x-\\mu_i)(x-\\mu_i)^T$$in cui $C_i$ è l'$i$-esima classe avente centroide $\\mu_i$. Lo ***scatter inter-classe*** $S_B$ è la varianza dei vari cluster dati dalle classi, pesati secondo il numero di elementi $n_i$ rispetto al baricentro $\\mu$$$S_B=\\sum_{i=1}^kn_i(\\mu_i-\\mu)(\\mu_i-\\mu)^T$$L'idea di LDA è massimizzare il rapporto $S_B/S_W$, operativamente massimizzando la funzione$$J(W)={|W^TS_BW|\\over|W^TS_WW|}$$Questa roba è anche conosciuta come ***Fisher's Solution***, e si riduce a trovare gli autovettori di $S_W^{-1}S_B$ (lei non l'ha fatto, non so se è giusto e non voglio nemmeno saperlo). Anche qui arriviamo ad una riduzione dimensionale, i cui vettori di base vengono simpaticamente chiamati ***Fisherfaces***.","x":-12040,"y":-1957,"width":981,"height":713,"color":"4"},
		{"id":"0c592c293ba482ae","type":"text","text":"# Classic Features Extractors\n\nNessuna lista di metodi per estrarre le features da un'immagine di un volto sarà mai esaustiva.\n\n- ***Wavelet Transform*** - La Fourier Transform fornisce un'analisi media delle frequenze, i.e. dato un segnale stazionario è in grado di scomporlo nelle armoniche. E se il segnale cambia nel tempo? Fourier non se lo aspetta, quindi fa appunto una media. Una Wavelet Transform invece fornisce un'analisi delle frequenze in funzione del tempo. Come? Prendo una ***Mother Wavelet*** (i.e. una sorta di particella d'onda) e la faccio scorrere nel tempo lungo il segnale, ripetendo per diversi valori di frequenza;\n\t- Che c'entra questo con il volto? I passaggi a diverse frequenze permettono di vedere grandi variazioni (e.g. di individuare un occhio) sia le piccole variazioni (e.g. rughe d'espressione).\n- ***Gabor Filter*** - Un filtro (kernel) che si basa sulla Wavelet, cercando specifici componenti all'interno del volto. Di base è una sinusoide modulata da una gaussiana, in cui è possibile cambiare i parametri $\\l$ (lunghezza d'onda della sinusoide, quindi larghezza della banda), $\\th$ (orientazione), $\\g$ (aspect ratio) e $\\s$ (larghezza, i.e. quanto forte è la modulazione gaussiana, di conseguenza anche quante strisce vedo);\n\t- Ovviamente per analizzare un'immagine mi servono tanti siffatti filtri;\n\t- I feature vector sono ottenuti come risultato della convoluzione tra l'immagine e i vari filtri\n\t- Si presentano problemi di dimensionalità e di non-completa-cattura delle informazioni.\n- ***Elastic Bunch Graph Matching*** (***EBGM***) - Una tecnica basata sui grafi che prevede di collegare i vari punti in cui sono stati presi i dati con Wavelet/Gabor Filters. Il ***bunch graph*** così ottenuto (i.e. i nodi sono detti bunch, in quanto gruppi di varianti dello stesso pezzo di faccia, e.g. un occhio rappresentato aperto, poi chiuso, poi che guarda in giro, ...);\n- ***Local Binary Pattern*** (***LBP***) - Un kernel pesato $3\\times3$ che scansiona l'immagine pixel per pixel (in scala di grigi). Quello centrale fa da threshold, gli altri vengono confrontati con esso, generando un valore binario se sono meno ($0$) o più ($1$) luminosi. Tale sequenza di pixel viene convertita in binario (e.g. $10011010=154$) creando il ***pattern LBP*** di quel pixel. L'istogramma dei pattern rappresenta il features vector, spesso calcolato per delle sotto-finestre $k\\times k$ e poi integrato in un unico vettore.\n\t- Un pattern è detto ***uniforme*** se ci sono al più due transizioni $0\\to1$ o $1\\to0$ leggendo la stringa in modo circolare (i.e. se ci sta un solo gruppo di $1$ ed uno solo di zeri, e.g. $10000011$). È utile trovare pattern uniformi, in quanto corrispondono a strutture significative dell'immagine (e.g. bordi);\n\t- LPB è invariante a cambiamenti di illuminazione e robusto rispetto a piccoli cambiamenti locali.","x":-15581,"y":-2007,"width":880,"height":813,"color":"4"},
		{"id":"fb8c3e96830020e4","type":"text","text":"# Ripassino - Covarianza\n\nLa ***Covarianza*** tra due variabili casuali $X$ e $Y$ è una misura di quanto esse sono legate. Formalmente,$$\\text{Cov}(X,Y)=\\E\\bigg[(X-\\mu_X)(Y-\\mu_Y)\\bigg]$$Questo in teoria, quando poi abbiamo delle misure sperimentali sta roba diventa prevedibilmente$$\\text{Cov}(X,Y)={1\\over n}\\sum_{i=1}^n(x_i-\\meanx)(y_i-\\meany)$$\nBene, tutto ciò si interpreta nel seguente modo:\n\n- $\\text{Cov}(X,Y)>0$ - Se $X$ è sopra la sua media, anche $Y$ tenderà ad essere sopra la sua media $\\so$ ***correlazione diretta***;\n- $\\text{Cov}(X,Y)<0$ - Se $X$ è sopra la sua media, $Y$ tenderà ad essere sotto la sua media $\\so$ ***correlazione inversa***;\n- $\\text{Cov}(X,Y)=0$ - Il fatto che $X$ sia sopra la sua media non mi permette di fare alcuna previsione su $Y$ $\\so$ ***assenza correlazione lineare***;\n\nPer come è definita, la Covarianza non ha massimo e minimo. La si può tuttavia normalizzare ottenendo la ***Correlazione***:$$\\text{Cor}(X,Y)={\\text{Cov}(X,Y)\\over\\s_X\\s_Y}$$\n(NB: Al denominatore ho le deviazioni standard, non le varianze). Se ad esempio abbiamo$$X=[1,2,3] \\so \\meanx=2\\qquad Y=[2,4,6]\\so\\meany=4 \\quad\\so\\quad\\text{Cov}(X,Y)={1\\over3}\\bigg[(1-2)(2-4)+(2-2)(4-4)+(3-2)(6-4)\\bigg]={4\\over3}$$\nNormalizziamo per ottenere la ***Correlazione di Pearson*** in $[-1,1]$ $$\\s^2_X = {2\\over3}\\quad\\s^2_X = {8\\over3}\\quad\\so\\quad \\s_x\\s_y={4\\over3}\\quad\\so\\quad\\text{Cor}(X,Y)=1$$Prevedibilmente, la Correlazione mi sta dicendo che la relazione è perfettamente lineare (infatti $y_i=2x_i$).\n\nOra, è intuitivo estendere il concetto di Covarianza al concetto di ***Matrice di Covarianza*** $\\calC$ definita come$$\\calC(X,Y)=\\begin{bmatrix}\n\\text{Cov}(X,X) & \\text{Cov}(X,Y) \\\\\n\\text{Cov}(Y,X) & \\text{Cov}(Y,Y) \\\\\n\\end{bmatrix}=\\begin{bmatrix}\n\\s^2_X & \\text{Cov}(X,Y) \\\\\n\\text{Cov}(Y,X) & \\s^2_Y \\\\\n\\end{bmatrix}$$\nNota che il numero $m$ di osservazioni (i.e. il numero di eventi per i quali ho preso le misure di $X$ ed $Y$, che nell'esempio di prima era $3$) è \"nascosto\" dentro ciascun termine di Covarianza. Sinteticamente, scrivo $\\calC$ come$$\\calC({\\vec X})={1\\over m}\\sum_{i=1}^m \\,\\,({\\vec x_i - \\vec\\meanx})\\,({\\vec x_i - \\vec\\meanx})^T$$dove $\\vec X=\\{X, Y, \\,...\\}$, $\\vec x=\\{x, y, \\,...\\}$ e $\\vec\\meanx=\\{\\meanx,\\meany, \\,...\\}$. Ovviamente moltiplicando per il trasposto faccio tutte le coppie possibili. La somma su $i$ (che indicizza la misura della generica variabile $x\\in\\vec x$) contrae una dimensione, per cui il risultato è una matrice quadrata di dimensione $N\\times N$ dove $N$ è il numero di variabili. Notiamo che\n\n- È sempre simmetrica $\\quad\\so \\quad\\text{Cov}(X,Y)=\\text{Cov}(Y,X)$ \n- Gli elementi diagonali sono sempre le varianze.\n\nData quest'ultima osservazione, è anche chiaro che se scrivo $\\calC$ nella base dei suoi autovettori\n\n- Le dimensioni passano in generale dalle grandezze $\\vec X$ a nuove grandezze $\\vec X'$ date da una qualche combinazione$$X_1'=\\sum_{i=1}^mc^i_1 X_i=\\vec c_1\\cdot\\vec X\\quad\\so\\quad\\vec X'=\\hat c\\,\\,\\cdot\\vec X$$\n- Gli elementi diagonali (i.e. gli unici non nulli, i.e. gli autovalori) sono ancora le varianze delle grandezze $\\vec X'$\n\n***Tutto questo per dire che:*** <span style=\"color:#FFA500\">nota che non sono sicuro di NULLA di ciò che segue</span>\n\n- Il rango massimo della matrice $\\calC$ è $\\max(m,N)$. Questo perché posso imparare al più $N$ covarianze per ciascuna variabile, ma per farlo ho bisogno di almeno $N$ esempi. Se ho $m<N$ esempi apprenderò al più $m$ covarianze;\n- Segue che se $m<N$ la riduzione dimensionale della PCA è implicita: la matrice $\\calC$ ha già autovalori nulli ed è quindi già riducibile ad una sotto-matrice;\n- Se invece $N\\gg m$ (come più spesso accade), allora la PCA ordina gli autovalori e prende quelli più alti. Riduce quindi lo spazio di ricerca a quelle combinazioni di valori che meglio distinguono i dati.","x":-13275,"y":-3661,"width":981,"height":1536,"color":"4"},
		{"id":"ee38d6335014d54b","type":"text","text":"# $\\dD$ Face Recognition\n\nI metodi classici per fare face recognition in $\\dD$ si dividono in $4$ macro-categorie.\n\n- ***Metodi globali*** - Prendono in input l'intera immagine, alto costo computazionale ma non perdono alcuna informazione (e.g. ***PCA***, ***LDA***), per quanto questo può portarli a dare la stessa importanza ai pixel poco significativi (e.g. sfondo) e a quelli veramente utili per discriminare. Inoltre possono avere qualche problema se sono presenti molte variazioni PIE;\n\t- Si possono includere in questa sezione anche le ***CNN***, attualmente stato-dell'-arte. Ne sono esempi ***DeepFace*** (Facebook, 2014) e ***FaceNet*** (Google, 2015).\n- ***Metodi locali*** - Nello spirito dell'esperimento BUBBLES, cercano i dettagli che meglio discriminano tra le classi (e.g. ***Gabor Filter***, ***LBP***). Leggeri a livello di costo computazionale, ma la selezione delle features è spesso \"fatta a mano\";\n- ***Grafi*** - Mappano gli hotspot della faccia, e sarebbero magnifici se non fossero così pesanti in training.","x":-14400,"y":-1796,"width":880,"height":391,"color":"6"},
		{"id":"a0c0b74965846dd1","type":"text","text":"# Datasets?","x":-13040,"y":-600,"width":323,"height":122},
		{"id":"cf74d7ca57f4d4cf","type":"text","text":"# Metodi per il Riconoscimento\n\n-  ***Normal Map*** - Dopo aver generato il modello $\\tD$, lo converto in $\\dD$ similmente ad un planisfero. A questo punto, associo ad ogni pixel come valori RGB i valori dell'orientazione della normale della superficie che in quel punto definiva il volto $\\tD$. Il risultato è una ***heat map*** che risulta più leggera in memoria e più leggera in fase di confronto per il modello;\n- ***Morphable Models*** (***FaceGen***, 2008) - Un software a cui dai 3 foto e lui non solo ti ricostruisce il modello $\\tD$ della tua faccia, ma è anche in grado di simulare emozioni e invecchiamento;\n- ***Iso-Geodesic Stripes*** (2010) - L'idea è questa: visto che i modelli $\\tD$ sono ancora più sensibili di quelli $\\dD$ a variazioni di espressione, proviamo a calcolare le ***geodetiche*** tra i vari punti del volto (tipicamente tutti rispetto ad un unico punto, e.g. la punta del naso). In pratica vengono disegnate sul volto una serie di corone circolari concentriche, ognuna contenente un set di punti e costituente nel modello un nodo di un grafo. Le distanze medie tra queste strisce sono rappresentate come archi del grafo (che prendono il nome di $\\tD$ ***Weighted Walkthroughs*** (***3DWW***). Il matching a questo punto è precisamente il matching di costo minimo tra il grafo della probe e quello del template in galleria.","x":-14379,"y":3620,"width":834,"height":445,"color":"4"},
		{"id":"be6c4e35ea254e0f","type":"text","text":"# HowToBuildFaccia3D\n\n- ***Camera Stereoscopica*** - Una camera dotata di più lenti che catturano la testa da diversi angolazioni. Basso costo, precisione non eccezionale;\n- ***Structured Light Scanner*** - Proietto un pattern sul volto, a seconda di come questo viene riflesso deduco la morfologia della faccia. Questa roba fa fatta da diversi POV, costo e precisione medio alti;\n- ***Laser Scanner*** - Sparo un laser su tutto il volto, molto preciso ma pericoloso per gli occhi.\n- ***Mesh di Immagini*** $2.5\\text{D}$ - Dicesi immagine $2.5\\text{D}$ un'immagine $\\dD$ che oltre alle informazioni RGB contiene la distanza di acquisizione. Fare merge di più immagini $2.5\\text{D}$ consente di ricostruire un modello $\\tD$.\n\nSi può anche ricostruire il volto $\\tD$ a partire dalle ombre del modello $\\dD$ (***Shape from Shading***) assumendo ***superfici lambertiane*** (i.e. la luce incidente sul volto viene diffusa in modo uniforme), o partire da un generico volto affinandone i dettagli per matchare il soggetto (***morphing***).\n\nDopo la fase di scanning vi è in genere un ***preprocessing*** per correggere errori quali buchi o spikes, nonché il problema dell'***alignment*** (i.e. allineare i tracking point dei due modelli). Quest'ultimo avviene in due step:\n\n- ***Coarse Alignment*** - La prima parte è più grossolana, seleziona pochi punti caratteristici e fa un primo aggancio approssimativo. Questo comprende un adjustment di orientazione e scala;\n- ***Fine Alignment*** - La parte precisa ma computazionalmente impegnativa, si basa su un algoritmo ***ICP*** (***Iterative Closest Point***) che calcola le differenze di volume tra poligoni \"vicini\" (sono \"vicini\" grazie al Coarse) per poi minimizzare iterativamente la distanza tra essi.","x":-15581,"y":2852,"width":880,"height":593,"color":"4"},
		{"id":"b94808e4f2c265bd","type":"text","text":"# Face Evaluation (more like \"History of...\")\n\nStoricamente, il primo DB usato come benchmark è stato il programma ***FERET*** (***Facial Recognition Technology***), la cui evoluzione ha seguito diversi step:\n\n- 1994 - Il DB consta di 316 persone, ed effettua test di corretta identificazione, falsi allarmi (i.e. fornendo soggetti non presenti nella gallery) e valutazione dei cambiamenti di posa sui risultati;\n- 1995 - Il DB consta di 817 persone, e introduce i duplicati (i.e. foto della stessa persona, ma scattate in un giorno differente rispetto al resto delle sue foto presenti nel dataset);\n- 1996 - \"More detailed performance analysis\".\n\nSegue ***FRVT*** (***Face Recognition Vendor Test***) che dimostra come le performance calano sensibilmente in $\\dD$ se la foto non è frontale (2000) e se lo stesso soggetto è fotografato in ambienti differenti (e.g. dentro gli studi o in un parco, 2002).\n\nNel 2006 viene introdotto <span style=\"color:#FFA500\">su FRVT?</span> il ***singolo cieco*** come metodologia di test tramite l'ambiente ***BEE*** (***Biometric Experimentation Environment***) fornito insieme al DB dal governo americano: i partecipanti non hanno accesso preventivo ai dati (impossibile fare ottimizzazione ad-hoc del modello sui dati di test).\n\nTra il 2004 e il 2006 la ***Face Recognition Grand Challenge*** (***FRGC***) ha proposto diversi esperimenti per\n\n- Exp1, Exp4 - Classici confronti di immagini $\\dD$ per produrre una matrice di similarità;\n- Exp2 - Variante in cui si introducono più sample per soggetto in gallery, costruendo ancora la matrice;\n- Exp3 - Variante $\\tD$, sempre avente output una similarity matrix.","x":-13219,"y":-122,"width":880,"height":671,"color":"4"},
		{"id":"7eae48c6d6f1da2d","type":"text","text":"# PCA\n\nA livello tecnico, le immagini soffrono della ***curse of dimensionality***, i.e. al crescere delle dimensioni il volume di spazio entro cui si trovano le features cresce a tal punto da rendere i dati sparsi. Un algoritmo di ML che deve muoversi in un enorme spazio sparso ha bisogno di un gran numero di esempi in più in fase di training per generalizzare le regole predittive (in quanto strumento statistico). Per questo, il primo step del riconoscimento facciale è la ***riduzione dimensionale***.\n\nUna possibile soluzione è la ***PCA*** (***Principal Component Analysis***), i.e. individuare un nuovo spazio ortogonale a dimensione $k$ minore di quella di partenza $N$ su cui proiettare i dati in modo tale che la loro varianza sia massima (i.e. siano ancora ben separati).\n\n Ogni immagine del DB è rappresentata da un vettore colonna avente $N$ valori (i.e. i pixel dell'immagine vengono messi tutti in un unico vettore colonna). $X$ è la matrice che raccoglie gli $m$ vettori colonna $N$-dimensionali (i.e. ci sono $m$ immagini nel DB). A questo punto\n \n - $\\calC={1\\over m}XX^T$ è la matrice $N\\times N$ della ***Covarianza delle features***;\n - $C_s={1\\over N}X^TX$ è la matrice $m\\times m$ della ***Covarianza dei sample***;\n\nL'azione di una PCA è quella di prendere i primi $k$ autovettori di $\\calC$ (ordinati secondo gli autovalori, in ordine decrescente) e renderli la base del nuovo spazio proiettando tutte le features su tali direzioni tramite$$\\text{Proj}(x)=\\varphi_k^T(x-\\meanx)$$dove $\\varphi_k$ sono le prime $k$ colonne della matrice di Covarianza scritta nella base di autovettori, comunemente detti ***eigenpictures***. Questo perché in pratica se vai a plottarli sono delle facce strane e deformi, ma che per il modello sono le più utili a discriminare.\n\nTutto ciò ha un problema: la PCA tende a non rendersi conto che le variazioni PIE rappresentano lo stesso soggetto. Pertanto, in DB in cui sono molto presenti tende a funzionare male. Segue LDA per la soluzione.","x":-13275,"y":-1957,"width":981,"height":713,"color":"4"},
		{"id":"a7584d0b54c4fa05","type":"text","text":"# Face (Anti)Spoofing\n\nAnzitutto distinguiamo ***spoofing*** (i.e. voler passare per qualcun altro) e ***camouflage*** (i.e. non voler passare per se stessi). Soffermandoci sul primo problema, ci sono diversi approcci.\n\n- ***Liveness Detection*** - Mostro al sistema la foto di un soggetto enrolled. Anche se io non sono nel DB, lui lo è ed io posso entrare. Non va bene, no? Allora devo assicurarmi che mostrare una foto non sia sufficiente. Ci sono diverse soluzioni per questo, come valutare il ***movimento della testa*** (anche eventualmente a seguito di una ***challenge***, e.g. \"`sposta la testa a sinistra`\"), analizzare la ***qualità*** della presa dati (i.e. una foto ad una foto abbassa in generale la risoluzione), controllare l'***eye-blinking*** o ***far parlare il soggetto*** valutando i movimenti della bocca;\n\t- Una simile challenge prende spesso il nome di ***Fatcha***;\n\t- Nota che se la challenge è troppo ovvia (e.g. chiede sempre di spostare la testa a destra) un video può tranquillamente superare il test di spoofing. Potrei anche ruotare una foto per ottenere ciò, motivo per cui se fai la challenge dovresti anche ***controllare i movimenti del background***;\n\t- ***Tridimensionalità*** - Su un volto esistono triple di punti che non sono mai coplanari. Se individuo tali punti e chiedo all'utente una challenge di movimento, sono in grado di stabilire dalle misure se lo sono o meno. Se risultano coplanari, l'unica spiegazione è che quello mostrato è un video.\n- ***Moiré Patterns*** - Se provo a mostrare un video, si vede l'effetto Moiré (tipicamente tramite LBP);\n- ***Riflessi*** - In caso di spoofing $\\tD$ con una maschera di gomma, quest'ultima riflette più luce rispetto alla pelle, che invece tende ad assorbirla. Questa roba funziona anche se mostro una foto;\n- ***Battito Cardiaco*** - Se hai una risoluzione da figlio di Dio, riesci a vedere le vene che pulsano;\n- ***Eye-Tracking*** - Proietti degli impulsi a schermo e verifichi se gli occhi li seguono di conseguenza;\n\nOra, ovviamente uno vuole anche valutare le performance del sistema di antispoofing. FAR ed FRR non bastano (e quando mai), quindi aggiungiamo tre nuove metriche:\n\n- ***SFAR*** (***Spoofing FAR***) - Percentuale di attacchi spoofing che hanno successo;\n- ***FLR*** (***False Living Rate***) - Percentuale di attacchi spoofing tramite foto/video che passano come legittime persone in carne ed ossa che provano ad entrare;\n- ***FFR*** (***False Fake Rate***) - Percentuale di persone in carne ed ossa identificate come un tentativo di accesso tramite foto/video.","x":-13219,"y":1433,"width":880,"height":890,"color":"4"},
		{"id":"5e4f5bfadd024ecf","type":"text","text":"# B - Viola-Jones (2004)\n\nDi base è un classificatore faccia/non-faccia, ma è molto potente. L'idea è fare una sliding window sull'immagine principale (di size variabile, ovviamente) e poi runnare il classificatore in ognuna di esse. Le features constano di semplici filtri rettangolari divisi simmetricamente in zone bianche e zone nere (***Haar-features***). Il valore restituito da ciascun filtro è la somma dei pixel nella zona bianca meno la somma dei pixel della zona nera. Questo calcolo è un po' semplificato se si usano le ***integral images***, i.e. ogni punto sull'immagine intercetta un rettangolo il cui altro estremo è l'angolo in alto a sinistra. Posso ottenere il valore di qualsiasi rettangolo come somma/differenza di rettangoli (vedi immagine).\n\nIl classificatore è costruito tramite un ***AdaBoost***, i.e. un'unione di weak classifiers lineari con lo scopo di creare una migliore separazione non lineare. AdaBoost seleziona le migliori Haar-features, i.e. i migliori ***Haar-kernel*** da far scorrere entro la finestra.\n\nUn simile algoritmo è molto difficile da trainare (dovendo provare un numero esponenzialmente grande di Haar-kernels), ma una volta trovate le features migliori diventa molto veloce (bisogna muovere sull'immagine un'unica finestra con le features ottimali).\n\nAnche così, però, generalmente questo ***strong classifier*** non è abbastanza efficace. Quello che si fa allora è metterne tre ***in cascata***, partendo dal più semplice (i.e. valuta poche features) fino ad arrivare al più complesso (valuta tante features). Se anche uno solo di essi vota no, il sistema decreta che quella nella finestra non è una faccia.\n\nÈ furbo aggiustare le threshold in modo che il primo classificatore abbia un numero pressoché nullo di FN (i.e. se sono in dubbio, voto sì). Il secondo avrà un po' meno tolleranza, e così via. In questo modo la probabilità cumulativa di ottenere in output un FP diventa molto bassa.\n\nLe ***performance evaluation*** su un simile sistema tengono conto del ***FP Rate*** (i.e. il modello ha detto che è una faccia, ma non lo è), delle ***Non-Localized Faces*** e del ***C-Error*** (i.e. ok, hai individuato una faccia, ma quanto sei allineato bene?).\n\nÈ anche possibile utilizzare questa stessa tecnica ricorsivamente per trovare occhi e bocca (e altre ***ROI***, ***Region Of Interest***) all'interno della faccia (è in generale un ***object detector***).","x":-16640,"y":1433,"width":880,"height":890,"color":"4"},
		{"id":"9f9a55b05c246be5","type":"text","text":"# A - Hsu, Mottaleb, Jain (2002)\n\nQuesto algoritmo consta di due macro-fasi:\n\n- ***Face Candidates Detection*** - Prima trovo tutte le potenziali facce. Come? In tre step:\n\t- ***Illumination Compensation*** - L'illuminazione può falsare le caratteristiche del volto in due modi:\n\t\t- ***Colore*** - Se tutta la scena è illuminata da una luce colorata, i colori sono falsati. Si individuano allora il $5\\%$ dei pixel con maggiore luminosità, si controlla se sono rappresentativi dell'immagine (i.e. se sono più o meno omogenei o se sono tutti concentrati in un punto, e.g. due piccoli punti luce isolati) e in caso sia rilevante se ne calcola il colore medio, che si assume essere il colore medio dell'illuminazione di tutta l'immagine. Se è diverso dal bianco (i.e. l'illuminazione è falsata) o dal colore tipico della pelle (i.e. i pixel che ho preso sono quelli di un volto in primo piano), si normalizza l'intera immagine rispetto a quel colore;\n\t\t- ***Non omogeneità*** - Se per individuare una guancia cerco un triangolo chiaro sopra e scuro sotto, con una forte luce dal basso perdo completamente questa caratteristica. Per risolvere, devo individuare la fonte di luce e compensarla in modo proporzionale. Gli algoritmi per fare ciò, tuttavia, sono un po' complessi (si può addirittura usare una GAN), e non li riporto. In questo algoritmo non vengono fatte correzioni sull'omogeneità, lo riporto per completezza.\n\t- ***Color Space Transformation*** - Può essere utile guardare l'immagine non in RGB ma in YUV o HSV;\n\t- ***Skin-Model Localization*** - A questo punto devo ***segmentare*** l'immagine, i.e. dividerla in zone significative aventi caratteristiche comuni.\n\t\t- Il ***metodo di Otsu*** distingue due sole classi: soggetto e sfondo. Questo si ottiene convertendo l'immagine in BN e provando tutte le possibili soglie di Luminance per dividere le due classi. Quella che minimizza la varianza intra-classe viene considerata la miglior divisione;\n\t\t- In alternativa, posso fare una ***CCA*** (***Connected Component Analysis***) collegando pixel vicini con colore simile. A fine iterazione rimuove i gruppi troppo piccoli.\n- ***Face Candidates Verification*** - Potrei aver individuato cose che in realtà non sono una faccia. Cerco allora le features tipiche del volto per vedere se corrispondono.\n\t- ***Occhi*** - Si individuano con YUV. La zona oculare ha alti valori di U, bassi valori di V e un ampio range di valori di Y. Quest'ultimo tratto meno evidente si evidenzia con le operazioni di ***dilation*** ed ***erosion*** in cui l'immagine viene espansa a rimpicciolita tramite un kernel;\n\t- ***Bocca*** - Al contrario, ha alti valori di V (anche detto $C_r$) e bassi di $U$ (anche detto $C_b$);\n\t- ***Contorno del viso*** - Un po' più facile, esistono algoritmi di edge detection e in genere la faccia ha un contorno comprensibile.","x":-15581,"y":1433,"width":880,"height":890,"color":"4"},
		{"id":"88798430bf23e3d3","type":"text","text":"# Metodi Recenti\n\nOvviamente di recente non si usano né A né B. E allora cosa si usa?\n\n***Deep Learning***. E quando ti sbagli. Roba tipo ***dlib***, ***MediaPipe*** (Google), ***YOLO*** sono tutte robe pre-trainate che trovi online. Fatti un giro, se capita.","x":-16640,"y":907,"width":880,"height":169,"color":"4"},
		{"id":"55b78b2a2edf7dfe","type":"text","text":"# $\\tD$ Face Recognition\n\nI risultati (in linea di principio più accurati) ottenibili in $\\tD$ dipendono fortemente dalla procedura di costruzione del modello del volto tridimensionale.\n\nQuesto fornisce più resistenza allo rispetto a variazioni PIE, ma al contempo rende più semplice lo spoofing tramite l'ingegnerizzazione di modelli ad hoc.\n\nLe features utilizzate includono i cambiamenti di curvatura del volto (***crest lines***, se si tratta di punti molto accentuati come il naso rispetto al resto del volto, o ***local curvatures***, se parliamo di avvallamenti più dolci come la zona degli occhi) e dettagli minori come angolazioni e colori (***local features***).","x":-14301,"y":2987,"width":678,"height":324,"color":"6"},
		{"id":"36f5d5c43cdf3edd","type":"text","text":"# GPT sul Riconoscimento Facciale\n\n- **FEM basati su Deep Learning**:\n    - **FaceNet**: Una delle architetture più popolari per l'estrazione di feature per il riconoscimento facciale. Utilizza una **loss triplet** per imparare una rappresentazione ottimale del volto, in modo che volti simili siano vicini nello spazio delle embedding e volti diversi siano lontani.\n    - **VGG-Face**: Basato sull'architettura VGG, questo modello è stato progettato per estrarre feature facciali rappresentative. Le sue embedding sono molto utilizzate per confronti facciali.\n    - **ArcFace**: Un modello avanzato che utilizza una particolare funzione di loss chiamata **Additive Angular Margin Loss** per migliorare la discriminazione tra diverse classi di volti, generando embedding molto efficaci per il confronto.\n- **FEM tradizionali**:\n    - **Local Binary Patterns (LBP)**: È una tecnica che cattura texture e caratteristiche locali del volto, molto usata nei primi metodi di riconoscimento facciale.\n    - **Histogram of Oriented Gradients (HOG)**: Analizza le direzioni dei gradienti nell'immagine per rappresentare le forme e i contorni del volto.","x":-16640,"y":-920,"width":880,"height":455,"color":"5"},
		{"id":"2395f5208dcbd0b4","type":"text","text":"# Ear Detection & Features Extraction\n\nLa localizzazione in genere fatta con un ***object detector*** come ***Viola-Jones*** (***AdaBoost***, classificatore non-lineare), mentre l'estrazione delle features è un compito lasciato ad una ***NN***.\n\nNel caso di modello tridimensionale, si usano logiche di massima curvatura.\n\nUna volta trovati i punti d'interesse, si racchiude l'orecchio in un rettangolo normalizzabile.","x":-10120,"y":2055,"width":880,"height":268,"color":"4"},
		{"id":"c540fa8a95fc7612","type":"text","text":"# Ear Recognition\n\nL'orecchio è un oggetto complesso, ma meno del volto. Pur permettendo una analisi più leggera (serve meno risoluzione) e semplice (in genere è una parte scoperta del volto), soffre le variazioni di illuminazione e posa.\n\nÈ uno dei tratti con permanenza più alta, restando valido tra gli $8$ e i $70$ anni.","x":-9020,"y":1748,"width":645,"height":261,"color":"6"},
		{"id":"d04d22904e4981bc","type":"text","text":"# Y FACE && !EAR?\n\nHanno fatto uno studio comparativo in cui si aspettavano che l'orecchio performasse meglio (e.g. diversa espressione per la faccia vs diverso lato della testa per l'orecchio, e simili). Risultato: ha vinto la faccia.\n\nMorale della favola: la faccia performa meglio.","x":-10120,"y":1433,"width":880,"height":268,"color":"4"},
		{"id":"c5ac94c35b35d4f6","type":"text","text":"# Voce - Gaussian Mixture Model (GMM)\n\nNon l'ha fatto, serve per il progetto credo","x":-7104,"y":83,"width":645,"height":261,"color":"6"},
		{"id":"6f0324ae75ed4bbc","type":"file","file":"EarFeatures.png","x":-9930,"y":2954,"width":500,"height":391},
		{"id":"4359246fbca402e0","type":"file","file":"Iannarelli.png","x":-8012,"y":2936,"width":539,"height":428},
		{"id":"6884a758da1d3f0f","type":"text","text":"# Metodi per il Riconoscimento\n\n- ***Iannarelli*** - Si basa sulla corretta individuazione del ***Punto Zero***, rispetto al quale vengono valutati tutti gli altri punti (i.e. se si sbaglia questa valutazione, tutto il sistema crolla);\n- ***Diagrammi di Voronoi*** - Tramite i punti e il rettangolo costruisco un diagramma di Voronoi. È tuttavia sensibile ad un non perfetto allineamento;\n- ***Force Fields*** - Se ho capito bene, è un modello in cui i punti sono considerati come particelle massive, o comunque emettono un campo di forza attrattivo. Viene disegnata un'ellisse intorno all'orecchio entro il rettangolo normalizzato, quindi i punti dell'ellisse vengono attratti dagli attrattori, formando un pattern che in teoria costituisce una \"firma\" unica;\n- ***Jets*** - Applico dei ***Gabor Filters*** (Cfr. Classic Features Extractors in $\\dD$ Face Recognition), il Jet è quello che ho chiamato Bunch <span style=\"color:#FFA500\">credo</span>;\n- $\\tD$ ***Models*** - Se ho a disposizione anche la terza dimensione, Cfr. $\\tD$ Face Recognition;\n- ***Thermogram*** - Se ho dati termici è più facile localizzare l'orecchio e non vede l'occlusione dei capelli, ma la risoluzione è molto bassa e il costo è alto;\n- ***Angle Vectors*** - Si scelgono (potenzialmente diverse) triple di landmark e se ne valutano gli angoli interni (invarianti di scala e di rototraslazione).","x":-9136,"y":2910,"width":880,"height":479,"color":"4"},
		{"id":"425aeba4cd4d1e1e","type":"file","file":"Iris.png","x":-8899,"y":556,"width":400,"height":304},
		{"id":"2f94a815f931defe","type":"text","text":"# Iris Recognition\n\nL'iride è una membrana muscolare situata dietro la cornea, una ciambella il cui buco è la pupilla. Il complesso pattern che la contraddistingue è ***randotipico***, il che lo rende un tratto fortemente caratterizzante, nonché permanente.\n\nSarebbe perfetto, non fosse che acquisire dei buoni dati è difficilissimo, sia con luce visibile (più soggetta a rumore) sia con luce infrarossa (meglio, ma ovviamente più costoso e mi perdo i dati sul colore).\n\n","x":-9029,"y":76,"width":660,"height":276,"color":"6"},
		{"id":"ac6926d1d18b85b2","type":"text","text":"# Pro & Cons\n\n- ***Pro*** - Sempre visibile e protetta, permanente, fortemente distintivo, l'immagine può essere acquisita senza contatto diretto (come invece avviene con la retina);\n- ***Contro*** - È un oggetto veramente piccolo ($\\sim 3.5\\,cm^2$), devo allineare l'asse ottico e lo sguardo (gaze), possono esserci riflessioni strane (anche a causa di lenti a contatto) una buona acquisizione richiede una distanza minore di 1 metro.","x":-8065,"y":76,"width":645,"height":276,"color":"4"},
		{"id":"ae8464ae65f71a2f","type":"text","text":"# Localization, Features Extraction & Matching\n\nBisogna anzitutto trovare tutta e sola l'iride. Cerco allora i bordi dell'iride massimizzando la varianza dei pixel in un cerchio (variando posizione e raggio. Si applica un metodo simile per trovare le palpebre, il filtro è ad arco piuttosto che a cerchio). Trovata la zona dell'iride, proseguo con\n\n- ***Segmentazione*** - Creo una maschera a forma di ciambella per isolare tutti e soli i pixel dell'iride;\n- ***Normalizzazione*** - Per rendere le iridi confrontabili, devo normalizzarle. Uso un sistema di coordinate \"pseudo-polari\", noto come ***Rubber Sheet Model***: sono coordinate polari in cui il \"cerchio unitario\" non è un cerchio, né un'ellisse, piuttosto una roba che si adatta precisamente alle imperfezioni della pupilla. Il risultato di tutto ciò è una rappresentazione rettangolare simmetrica;\n\t- Nota che questo non _elimina_ le imperfezioni, ma le _normalizza_.\n\nA questo punto l'estrazione delle features avviene tramite i ***Gabor Filters*** (Cfr. Classic Features Extractors in $\\dD$ Face Recognition). Trovate le features e conservate sottoforma di un compatto codice binario, il confronto tra due template avviene tramite la ***Hamming Distance*** (i.e. confronto bit a bit).\n\nQuesta procedura è detta ***Dougmann System***, dal nome del primo uomo che mise piede bla bla bla","x":-10120,"y":-19,"width":880,"height":466,"color":"4"},
		{"id":"925fd2030a28da5e","type":"file","file":"IrisVisible.png","x":-9029,"y":-220,"width":272,"height":258},
		{"id":"fdf8b11e789fd02f","type":"file","file":"IrisInfrared.png","x":-8641,"y":-202,"width":272,"height":222},
		{"id":"52fe8a837877cad0","type":"file","file":"IrisBorders.png","x":-10500,"y":94,"width":291,"height":240},
		{"id":"fbe58c41b701d4f7","type":"file","file":"IrisSegmentation.png","x":-9880,"y":500,"width":400,"height":257},
		{"id":"0617a660413b14c2","type":"file","file":"IrisNoise.png","x":-8893,"y":-2120,"width":399,"height":200},
		{"id":"5502640d95e0cb4c","type":"text","text":"# NICE (Noisy Iris Challenge Evaluation)\n\nIl dataset ***UBIRIS*** è stato costruito in due sessioni:\n\n- ***UBIRISv.1*** (2005) - Condizioni controllate;\n- ***UBIRISv.2*** (2007) - Condizioni non controllate, campione a predominanza Latino-Caucasica ($90\\%$), presa dati di 4 settimane inframezzate da una di pausa. Il $60\\%$ dei soggetti ha partecipato ad entrambe le sessioni, il restante $40\\%$ ad una sola.\n\nSu ***UBIRISv.2*** è stata lanciata la challenge ***NICE*** in ***singolo cieco*** per trovare il miglior algoritmo in\n\n- ***Segmentazione*** e ***Noise Correction*** (***NICE I***, 2008) - In poche parole, best preprocessing. Su metriche di Classification Error Rate ($E'$) e TYPE-I, TYPE-II Error Rate ($E''$) l'algoritmo vincitore proposto da CASIA (una roba cinese di non so dove) prevedeva i seguenti step:\n\t- ***Preprocessing*** - Serve ad eliminare il rumore dato da ciglia, pori della pelle intorno. Si fa tramite filtri di ***posterization*** (lavora sui colori, evidenziandone le variazioni) e ***canny*** (rilevazione bordi);\n\t- ***Pupil Location*** - Usa un filtro circolare, nonostante la pupilla non sia un cerchio perfetto. Questo causa comunque meno disastri che lavorare con ellissi e correzione delle imperfezioni;\n\t\t- Nota che in quella zona ci sono un sacco di pattern circolari. Ad ognuno viene quindi associato un punteggio di omogeneità (di colore) e separabilità (quanto bene si discosta dalle sue vicinanze, circa) e viene selezionato il match con lo score complessivo più alto.\n\t- ***Linearization*** e ***Limbus Location*** - Il Limbus delimita l'esterno dell'iride (come la pupilla delimita l'interno). Trovare entrambi non porta in generale a due cerchi concentrici. Nello spirito del **rubber sheet model**, li si rende tali.\n- ***Encoding*** e ***Matching*** (***NICE II***, 2010) - Qua in realtà bisogna fare anche la parte precedente, per cui non stupisce che abbia vinto di nuovo CASIA (probabilmente aveva i migliori dati preprocessati). L'idea è semplice: applico LBP, l'output è binario, applico la Hamming Distance.\n ","x":-9133,"y":-1120,"width":880,"height":693,"color":"4"},
		{"id":"a4870765680e2788","type":"file","file":"NICE_II_Winner.png","x":-9133,"y":-1480,"width":880,"height":272},
		{"id":"e5b58914968237be","type":"file","file":"FingerprintMinutiae.png","x":-7560,"y":1679,"width":369,"height":400},
		{"id":"119aed33db32e646","type":"text","text":"# Breve Storia e Acquisizione\n\nCi sono tracce di fingerprint patterns rappresentati in pitture e vasi già in tempi molto antichi, non è chiaro quanto avessero capito e quanto \"boh sti disegni sono belli, mettiamoceli\". Forse la seconda.\n\nPiù recentemente, nell'800 la gente ha iniziato ad interessarsi in modo più scientifico. Galton voleva usarle per dedurre la genetica, ma ha scoperto che erano \"solo\" fortemente discriminative, come confermato da Henry nel 1899 con le minutiae. Cosa? Questo mi permette di identificare le persone? ... al 1924, l'FBI aveva già un DB di quasi un milione di soggetti. Facile capire che serviva una roba che automatizza il check, quindi giù soldi per costruire un AFIS ((Automated Fingerprint Identification System).\n\nInizialmente, la presa dati avveniva con l'inchiostro, per poi essere copiata e stampata (***offline acquisition***). Questa roba si può fare ancora oggi: è la tipologia in cui si inseriscono le ***latent fingerprints***, i.e. commetto un omicidio e ops, ho lasciato le mie impronte (i.e. la mia skin oil) sul coltello, la polizia può facilmente farne una copia. Questo implica una successiva ***digitalizzazione***, il che richiede una certa risoluzione minima perché il dato grezzo sia considerato accettabile (e.g. l'FBI richiede $500$ dpi, i.e. dots-per-inch). \n\nSe la presa dati è consenziente, si possono usare dei ***live scanner*** di diversi tipi:\n\n- ***Ottico*** - Poco costoso e buona risoluzione, ma va pulito dopo ogni utilizzo; \n- ***Capacitivo*** - Ottima risoluzione, ma l'acquisizione dura molto;\n- ***Termico*** - Non viene ingannato da tentativi di spoofing quali dita di silicone perché è in grado di riconoscere la pulsazione delle vene, ma l'immagine scompare rapidamente (... ma stick no?!).\n\nTutti i metodi sopracitati soffrono dei tipici ***problemi di acquisizione*** (e.g. troppo movimento, pressione variabile, quindi distorsione geometrica) e hanno ***parametri di digitalizzazione*** (e.g. risoluzione, profondità, contrasto, area di acquisizione) variabili.","x":-5840,"y":1533,"width":880,"height":693,"color":"4"},
		{"id":"cada1214e8e2b742","type":"file","file":"FingerprintMacro.png","x":-7882,"y":1679,"width":279,"height":400},
		{"id":"fadd10091b275ac7","type":"text","text":"# Fingerprint Recognition\n\nTratto ***randotipico*** fortemente discriminativo definito da due tipi di tratti formati dall'alternarsi di creste e solchi della pelle (***ridges***):\n\n- ***Macro-Singolarità*** - Spirali, archi e loop formano pattern evidenti;\n\t- Diverse etnie tendono a presentare diversi pattern.\n- ***Micro-Singolarità*** (***minutiae***) -  Biforcazioni, punti di fine e delta delle linee sottili costituiscono le vere features discriminative.\n\t- I gemelli presentano pattern simili ma diverse minutiae.","x":-7104,"y":1732,"width":645,"height":295,"color":"6"},
		{"id":"7c2aba2dd87b85cd","type":"text","text":"# Metodi per il Riconoscimento\n\nIl primo step è ***segmentare*** l'immagine, i.e. separare lo sfondo (isotropo) dall'impronta (anisotropa).\n\nFatto questo, tocca evitare calcoli inutili. Si confrontano anzitutto i pattern globali: è inutile procedere con le minutiae se già questi sono differenti. Se c'è un primo match, possiamo procedere in diversi modi:\n\n- ***Correlazione*** - Le due immagini vengono sovrapposte, confrontando pixel per pixel e iterando su tutti i possibili displacement (allineamenti non perfetti). Ovviamente ha un costo computazionale molto alto, ed è sensibile a trasformazioni non lineari (e.g. diversa pressione del dito, quindi deformazione);\n- ***Ridge Features*** - Se non è possibile estrarre le minutiae (e.g. perché la qualità è troppo bassa), questo metodo estrae forme e frequenze dei vari pattern macroscopici.\n- ***Minutiae*** - Vengono estratte e confrontate direttamente le minutiae. È il metodo più discriminativo.","x":-7221,"y":2973,"width":880,"height":355,"color":"4"},
		{"id":"59e7c3a086ae6664","type":"text","text":"# Strumenti per il Riconoscimento\n\nA meno che non lavoriamo con la correlazione (e no, nessuno oggi lavora con la correlazione) devo definire degli strumenti per riconoscere i pattern.\n\nUna ***directional map*** calcola l'orientazione media delle tangenti di tutte le creste della finestra considerata, mentre una ***density map*** considera il numero delle creste nella stessa finestra.\n\n- ***Ridge Features*** - Si interpretano come grandezze medie (finestra di acquisizione \"grande\"), utili ad individuare le singolarità. Un metodo tipico in questo caso è l'***indice di Poincarè***, i.e. la rotazione complessiva di un vettore che segue una curva (i.e. una cresta). In questo modo posso distinguere tra i ***loop*** (rotazione complessiva di $180°$), le ***delta*** ($-180°$) e le spirali (i.e. ***whorl***, $360°$)\n- ***Minutiae*** - Si interpretano come grandezze locali (finestra di acquisizione \"piccola\"), utili ad essere più precisi e a ridurre il rumore. In questo caso uso ulteriori metodi, che possono essere riassunti in:\n\t- Prendo l'immagine binaria e faccio ***thinning***, i.e. riduco i segni delle creste ad $1\\text{px}$ di larghezza. Questo perché sono interessato a vedere solo le minutiae, non la loro intensità;\n\t- Prendo un kernel $3\\times3$ e valuto il contorno di ogni pixel di ogni ridge calcolando il ***crossing number***, i.e. quante volte si passa da un pixel ***bianco*** (***ridge***) ad uno ***nero*** (***valley***, per qualche motivo la convenzione è che le immagini siano salvate \"al contrario\", i.e. bianco su sfondo nero), controllando in modo ciclico. Troviamo quindi\n\t\t- $cn(p) = 0$ - Se nel vicinato non si passa mai da un pixel bianco ad uno nero, allora devono essere per forza tutti pixel neri (non possono essere tutti bianchi, perché prima ho ridotto tutti gli spessori ad $1\\text{px}$). Il pixel $p$ è isolato $\\so$ ***isola***;\n\t\t- $cn(p) = 1$ - Nel vicinato c'è un solo pixel bianco. $p$ è per forza un punto di ***terminazione***;\n\t\t- $cn(p) = 2$\t- I punti bianchi sono due, quindi $p$ è un ***punto interno*** di un qualche ridge;\t\n\t\t- $cn(p) = 3$ - Se ci sono 3 punti bianchi, allora $p$ deve essere un ***punto di biforcazione***;\n\t\t- $cn(p) > 3$ - Minutia complessa. Nota che con un kernel $3\\times3$ $\\max[cn(p)]=4$, quindi wtf Maria.\n\t- Il ***ridge count*** (i.e. il numero di ridge tra due punti fissati) può essere considerato una minutia.\n\nHa senso calcolare una matrice $N\\times N$ per le minutiae e poi \"aggregare\" di dati formando una matrice $n\\times n$ con $n<N$ per le più \"sgranate\" ridge features (i.e. faccio un ***average pooling***).","x":-7221,"y":3600,"width":880,"height":800,"color":"4"},
		{"id":"41349ce518d76a8f","type":"file","file":"FingerprintTools.png","x":-7141,"y":4520,"width":720,"height":340},
		{"id":"080aa8e14697619f","type":"text","text":"# Breve nota terminologica\n\nCome chiameresti questa operazione matematica di \"sgranare\" una matrice $N\\times N$ in una $n\\times n$ con $n<N$?\n\n- ***Pooling*** - Usato in ML e Image Processing;\n- ***Downsampling*** - Signal processing, prendo un dato ogni $k$;\n- ***Binning*** - Tipo gli istogrammi:\n- ***Aggregation*** - Termine generico;\n- ***Coarsening*** - Usato in contesti tipo grafi e simili per intendere il passaggio da una griglia più fine ad una più grossolana.","x":-8040,"y":3786,"width":567,"height":428,"color":"5"},
		{"id":"7935ff9549acc1be","type":"file","file":"ForceFields.png","x":-8922,"y":3786,"width":453,"height":428},
		{"id":"910498450377e092","type":"text","text":"# Un approccio Ibrido\n\nPosso mixare un approccio minutiae-based per come descritto nel riquadro a sinistra con un texture-based (kind of ridge features) che utilizza i ***Gabor Filters*** (e quando mai). Questo avviene in diversi step:\n\n- Estraggo le minutiae dalle due immagini da confrontare:\n- Seleziono delle minutiae di riferimento per fare l'allineamento (nota che tutto questo viene fatto se e solo se il pattern macroscopico è simile, e quindi c'è il ragionevole dubbio che le due immagini possano davvero rappresentare la stessa impronta);\n- Normalizzo l'illuminazione, tolgo lo sfondo, e in generale faccio tutte le *cose* di image preprocessing, compreso probabilmente (anche se non lo dice esplicitamente) sovrapporre una griglia;\n- A questo punto applico 8 Gabor Filters, tutti con la stessa frequenza ma con orientazioni differenti. Ogni cella della griglia produce quindi 8 immagini, che sono la risposta della cella a ciascun filtro;\n- Calcolo la somma delle differenze tra vettori di filtri analoghi e ottengo un punteggio texture-based;\n- Combino col punteggio minutiae-based per aumentare la precisione.","x":-5840,"y":3791,"width":880,"height":419,"color":"4"},
		{"id":"9e8b2a1513d0ae94","type":"file","file":"TraitsOverview.png","x":-8893,"y":1121,"width":394,"height":194},
		{"id":"2edefd620b8def34","type":"text","text":"# Multibiometrics (Multimodal Systems)\n\nÈ possibile fondere più tratti per realizzare ***sistemi multimodali***. La domanda da porsi in questo caso è come combinare diverse features in un'unica predizione globale, e ci sono tre possibili macro-risposte:\n\n - ***Features Level Fusion*** - I dati dei diversi sensori creano un unico feature vector, in tre modi:\n\t - ***Seriale*** -  Concateno i singoli FV in un unico FV;\n\t\t - Non perde informazioni ed è facile da implementare, ma è forte il rischio di overfittare e di creare un modello pesante e lento (cfr. Curse of Dimensionality).\n\t - ***Parallelo*** - Faccio una media pesata (e/o operazioni simili) dei vari FV;\n\t\t - Applicabile solo se ho features \"simili\".\n\t - ***Canonical Correlation Analysis*** (***CCA***) - Cerco di minimizzare le features ridondanti quando unisco i vari FV in uno solo, tramite il calcolo delle correlazioni.\n\t\t - Per fare ciò ho bisogno di un gran numero di dati.\n - ***Score Level Fusion*** - Ogni feature vector estratto da ciascun sensore viene gestito dal rispettivo algoritmo, il quale restituisce uno score. A seconda del significato dei vari score ho più modi di fondere:\n\t - ***Astratto*** - Se lo score è direttamente una classe, diventa Decision Level Fusion;\n\t - ***Rank*** - Se lo score è un punteggio di tipo classifica, basta sommare i vari punteggi creando una classifica cumulativa dalla quale estrarre il punteggio finale (***borda count***);\n\t\t - Un approccio di questo tipo è detto ***classifier-based***.\n\t - ***Misura*** - Se ogni algoritmo ha la sua misura, bisogna prima normalizzare e poi applicare la tua combinazione (non) lineare preferita (magari dai più peso agli algoritmi più accurati).\n\t\t - Un approccio di questo tipo è detto ***transformation-based***.\n - ***Decision Level Fusion*** - La maggioranza è lo stesso identico concetto di una Random Forest, altrimenti posso usare operatori logici come `AND` (i.e. per accettare tutti devono votare `sì`) o `OR` (i.e. per accettare almeno uno deve votare sì).","x":-4344,"y":-341,"width":880,"height":702,"color":"4"},
		{"id":"da1de2625fe0aa22","type":"file","file":"FeatureLevelFusion.png","x":-4344,"y":-1742,"width":879,"height":256},
		{"id":"7b7f07f292dd8a4d","type":"file","file":"ScoreLevelFusion.png","x":-4344,"y":-1360,"width":880,"height":381},
		{"id":"8960655ea5189c3a","type":"file","file":"DecisionLevelFusion.png","x":-4344,"y":-880,"width":879,"height":383},
		{"id":"5875daaf402b4ede","type":"text","text":"## It's All Multimodal\n\nÈ ***multimodale*** se uso sensori diversi.\n\nÈ ***multibiometrico*** se uso due instance diverse dello stesso tratto (anche solo due dita diverse), e ovviamente se uso due tratti diversi. È multimodale anche se per prendere la decisione voglio tre foto anziché una.\n\nÈ ***multiexpert*** se uso rappresentazioni diverse dello stesso tratto per decidere.\n\n ","x":-4344,"y":441,"width":880,"height":245,"color":"4"},
		{"id":"e03067326e47aa83","type":"text","text":"# Lezione 13\n### Riassunta da ChatGPt perché non mi va\n\n1. **PIFS (Partitioned Iterated Function System)**: Tecnica usata per comprimere e indicizzare immagini. Suddivide l'immagine in piccole regioni quadrate non sovrapposte, sfruttando le somiglianze interne per una codifica efficiente.\n    \n2. **FARO (Face Recognition Against Occlusions)**: Sistema di riconoscimento facciale che gestisce occlusioni (ad esempio, quando il volto è parzialmente coperto). Utilizza il metodo Viola-Jones per rilevare il viso e poi analizza gli occhi, il naso e la bocca. Divide il viso in due metà (sinistra e destra), scartando la parte meno illuminata.\n    \n3. **FACE (Face Analysis for Commercial Entities)**: Rilevamento facciale che identifica e corregge l'orientamento del volto. Utilizza il metodo Viola-Jones per rilevare i punti chiave (occhi, naso, bocca) e seleziona la parte del viso meglio illuminata per una miglior analisi.\n    \n4. **FOVEA (Video Frame Organizer via Identity Extraction and Analysis)**: Sistema per tracciare i volti in un video. Riconosce due tipi di identità: temporanee (per volti che appaiono in fotogrammi consecutivi) e permanenti (per volti che compaiono in tutto il video). Mantiene e aggiorna costantemente le identità facciali.\n    \n5. **HERO (Human Ear Recognition against Occlusions)**: Sistema che riconosce le orecchie in immagini, suddividendo l'area in 4 quadranti. Ogni quadrante è trattato indipendentemente per gestire occlusioni locali, utilizzando tecniche di codifica simili a quelle di PIFS.\n    \n\nTutti questi sistemi si concentrano sul miglioramento del riconoscimento facciale e biometrico, affrontando le sfide come occlusioni e variazioni nei volti o nelle immagini (ad esempio, cambiamenti nell'illuminazione).","x":-4404,"y":1601,"width":1000,"height":557,"color":"5"},
		{"id":"bc8bb53d411c6fbc","type":"text","text":"# Lezione 14 - Gallery Entropy","x":-4404,"y":2340,"width":1000,"height":570,"color":"1"},
		{"id":"c2ba3fbffc4679d8","type":"text","text":"# Sì, ma in pratica?\n\nData la gallery, faccio ***supervised training*** (le etichette sono dette ***ground truth***) che consiste nel loop\n\n- FEM\n- Matching\n- Decision\n- Evaluation\n\t- sulla base di quest'ultima modifico qualcosa in una delle fasi precedenti (pesi delle metriche, threshold, fine tuning se il FEM è una NN, ...) e ricomincio finché non ottengo risultati soddisfacenti.","x":-2600,"y":1520,"width":556,"height":372},
		{"id":"f32d6cc54c9299df","type":"text","text":"# Programmare a Oggetti I - Definizioni e Costruttori\n\nNel paradigma a oggetti posso pensare alle ***classi*** come stampini che producono diverse ***istanze*** di quella classe. Tipo, se definisco la classe `Cane` (convenzionalmente chiamate con la maiuscola), posso usarla per generare le istanze `fido = Cane(\"fido\")`, `brio = Cane(\"brio\")`, ecc...\n\nOgni classe definisce delle funzioni al suo interno, dette ***metodi***, che sono standard del tipo di oggetto. Tipo, mi aspetto che sia `fido` sia `brio` siano in grado di abbaiare, in quanto `Cani`, quindi la classe `Cane` conterrà un metodo `abbaia`.\n\nL'argomento con cui ho specificato il nome all'inizio e potenzialmente gli altri argomenti utili a definire la specifica istanza della classe vengono utilizzati in un metodo speciale chiamato `__init__`, detto ***costruttore*** della classe. È intuitivo: quando definisco una nuova istanza sto chiamando `__init__` per assegnargli i parametri che specifico chiamando la classe.\n\nVisto che quando scrivo la classe sono ancora nel mondo degli stampini, ma quando assegno i parametri sono già nel mondo delle istanze, come fa `__init__` a dire \"assegna all'istanza `fido` il nome `fido`\"? È un cane che si morde la coda! (quanto so simpatico oh). La soluzione è definire un ***placeholder*** speciale che farà riferimento all'istanza stessa, quando verrà creata. Convenzionalmente si usa la parola ***self*** (potrei utilizzare altro, ma è fortemente sconsigliato. Sarebbe come chiamare la temperatura \"velocità\": confondi tutti. Forse se vuoi offuscare il codice è utile saperlo).\n\n```python\n>>> class Dog:\n...     species = \"Canis familiaris\"   # Attributi comuni a tutte le istanze\n...     def __init__(self, name, age): # Attributi specifici della singola istanza\n...         self.name = name\n...         self.age = age\n...\n```\n\nSegue che se scrivo\n\n```python\nfido = Cane(\"fido\", 5)\nbrio = Cane(\"brio\", 3)\n```\n\navremo\n\n```python\nfido.age                    # restituisce 5\nbrio.age                    # restituisce 3\nfido.species, brio.species  # restituiscono entrambi 'Canis familiaris'\n```\n\nL'ultimo però è solo un'inizializzazione di default. L'istanza è separata dalla classe, quindi posso dire `fido.species = \"Felis\"` e cambiare l'attributo senza problemi.\n\nOltre ad `__init__` posso creare altre istanze, tipo\n\n```python\n    # Instance method\n    def description(self):\n        return f\"{self.name} is {self.age} years old\"\n\n    # Another instance method\n    def speak(self, sound):\n        return f\"{self.name} says {sound}\"\n```\n\n`brio.description()` restituisce `brio is 3 years old`, mentre `brio.speak(\"wof\")` restituisce `brio says wof`.\n\n","x":-1035,"y":-7300,"width":855,"height":1321,"color":"4"},
		{"id":"32572b8a289896a1","type":"text","text":"# Python Name Conventions\n\nLe librerie (API) hanno funzioni messe lì per essere usate dal pubblico e altre adibite ad uso interno.\n\nMentre linguaggi tipo `C++` e `Java` hanno le keyword specifiche `public` e `private` per definire chi può accedere a cosa, `Python` no. Quindi tocca definire degli standard.\n\n|Convention|Example|Meaning|\n|---|---|---|\n|Single leading underscore|`_variable`|Indicates that the name is meant for internal use only|\n|Single trailing underscore|`class_`|Avoids naming conflicts with Python keywords and built-in names|\n|Double leading underscore|`__attribute`|Triggers name mangling in the context of Python classes|\n|Double leading and trailing underscore|`__name__`|Indicates special attributes and methods that Python provides|\n|Single underscore|`_`|Indicates a temporary or throwaway variable|\n\nChe roba è questo ***name mangling***? Quando Python incontra una roba del genere in una classe, sostituisce `__nome` con `_ClassName__nome`. Questo per evitare override quando faccio nesting di classi (vabbè). Il punto dopo invece (`__name__`) riguarda i ***dunder names*** (**d**ouble **under**score), che sono oggetti particolari che Python usa per essere sempre adeguato. Tipo, la funzione `len` restituisce la lunghezza di un oggetto. Per una stringa è facile, è il numero di lettere. Ma per la tua classe custom? Definire la funzione `__len__(self)` dice a Python che se chiami `len(IstanzaMiaClasse)` deve eseguire il codice dentro `__len__` della classe.\n\nCi sono anche dunder names che però non sono metodi speciali, ma nomi speciali.\n\n- [`__name__`](https://docs.python.org/3/reference/import.html#name__) uniquely identifies the module in the import system.\n- [`__file__`](https://docs.python.org/3/reference/import.html#file__) indicates the path to the file from which a module was loaded.\n\nThe [name-main idiom](https://realpython.com/if-name-main-python/) allows you to execute code when you run the containing file as a script but not when you import it as a module.","x":20,"y":-8340,"width":855,"height":858,"color":"4"},
		{"id":"c4c4a31ed5bbf9e3","type":"text","text":"# Esempi di roba non-pubblica\n\n```python\n_PI = 3.14\n\nclass Circle:\n    def __init__(self, radius):\n        self.radius = _validate(radius)\n\n    def calculate_area(self):\n        return round(_PI * self.radius**2, 2)\n\nclass Square:\n    def __init__(self, side):\n        self.side = _validate(side)\n\n    def calculate_area(self):\n        return round(self.side**2, 2)\n\ndef _validate(value):\n        if not isinstance(value, int | float) or value <= 0:\n            raise ValueError(\"positive number expected\")\n        return value\n```\n\n```python\nclass Point:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    @property\n    def x(self):\n        return self._x\n\n    @x.setter\n    def x(self, value):\n        self._x = _validate(value)\n\n    @property\n    def y(self):\n        return self._y\n\n    @y.setter\n    def y(self, value):\n        self._y = _validate(value)\n\ndef _validate(value):\n    if not isinstance(value, int | float):\n        raise ValueError(\"number expected\")\n    return value\n```","x":20,"y":-9620,"width":855,"height":1112,"color":"4"},
		{"id":"26f5b713e47897e9","type":"text","text":"# Programmare a Oggetti II - Metodi Speciali e Inheritance\n\nScrivere `print(brio)` restituisce un puntatore all'oggetto in memoria. Per cambiare il comportamento di default della funzione `print` sulle istanze della classe posso definire la funzione speciale `__str__`.\n\n```python\n\tdef __str__(self):\n\t\treturn f\"{self.name} is {self.age} years old\"\n```\n\nStesso dicasi (vedi sopra) per il metodo `__len__`. A rigore `__str__` è la rappresentazione rivolta al client, mentre per il programmatore esiste `__repr__` (cambia solo il formato, a seconda di cosa è più utile a chi).\n\nAltri metodi dunder utili includono:\n\n- `__getitem__` - Ti permette di usare l'operatore `[]` sulla classe (e.g. `fido[age]`);\n- `__iter__` - Ti permette di trasformare l'istanza in una lista iterabile, quindi scrivere robe tipo `for x in My_Object`.\n\nIn pratica sono tutti metodi speciali che definiscono i comportamenti del tuo oggetto rispetto a funzioni standard. Definirli tutti significa rendere la tua classe simile ad una classe nativa.\n\nIl concetto di ***Inheritance*** è quello di definire una ***classe figlia*** che ***eredita*** alcuni elementi della classe genitore. Tornando all'esempio dei cani, tutti abbaiano e hanno un'età, ma magari diverse specie abbaiano in modi diversi. Possiamo definire delle sottoclassi per fare override:\n\n```python\nclass JackRussellTerrier(Dog):\n    def speak(self, sound=\"Arf\"):\n        return f\"{self.name} says {sound}\"\n\t\t# Oppure return super().speak(sound), che cerca nella classe\n\t\t# genitore il metodo \"speak\" e lo chiama con il parametro \"sound\" che in\n\t\t# questo caso stiamo definendo \"Arf\" di default.\n\t\t# super() è molto potente, se ci sono N classi annidate cerca in tutte!\n...\n\n>>> miles = JackRussellTerrier(\"Miles\", 4)\n>>> miles.speak()\n> 'Miles says Arf'\n```\n\nMa Miles è ancora un cane, infatti possiamo controllare che sia un'istanza della classe `Cane`:\n\n```python\n>>> isinstance(miles, Dog)\n> True\n>>> miles.species()\n> Canis familiaris\n>>> isinstance(miles, JackRussellTerrier)\n> True\n>>> isinstance(miles, Bulldog)\n> False\n```\n\n","x":20,"y":-7300,"width":855,"height":1321,"color":"4"},
		{"id":"4a35f5d889e637e2","type":"text","text":"# Quali features? (BUBBLES)\n\nIn un simpatico esperimento chiamato ***BUBBLES*** sono state parzialmente oscurate delle facce ben normalizzate, mostrandone solo alcuni pezzetti (bolle) in modo casuale. L'obiettivo dell'esperimento era capire quali porzioni di volto venivano utilizzate per discriminare se questo è espressivo o neutro (EXNEX) oppure se è maschile o femminile (GENDER).\n\nLong story short, per decidere se il volto è emozionato c'è un controllo più fine degli occhi e della bocca, mentre per la seconda task si guardano caratteristiche più generali come dimensioni, proporzioni, contorni.\n\nQuesto per l'AI, perché poi è anche interessante vedere il confronto con quello che guarderebbe un umano, cosa che hanno fatto. Ma sto scrivendo tanto per scrivere, sta roba non mi sembra così utile.\n\nO meglio, il punto è che se scrivo algoritmi per analizzare i dettagli, la prima cosa da chiedersi è quali sono i dettagli da cercare. La scelta è in prima analisi arbitraria, a meno che non faccio \"fine-tuning\" con un'AI.","x":-14400,"y":-2547,"width":880,"height":391,"color":"4"},
		{"id":"811c43decb94ee1d","type":"text","text":"# Progetto","x":-1874,"y":-6770,"width":589,"height":261,"color":"6"},
		{"id":"e671a8e608e0a1dd","type":"text","text":"# Riconoscere Pattern con l'AI\n\nImmagina di voler costruire un programma che riconosca **cani** nelle foto. Se dai in pasto al computer una semplice lista di pixel (una lunga riga di numeri), lui non ha alcuna idea di _dove_ siano le orecchie, il muso o le zampe. Serve **una struttura che tenga conto dello spazio**.\n\nLe ***Convolutional Neural Networks*** (***CNNs***) fanno scorrere un piccolo filtro (***kernel***) sull'immagine in modo da trovare pattern locali. Se cerco un certo schema, scorrendo sull'immagine il kernel produce una mappa di attivazione che descrive dove questo pattern viene trovato. Layer dopo layer, i pattern diventano sempre più dettagliati: prima i bordi, poi identifichi occhi, bocche e simili e infine riesci a mettere tutto insieme per dire \"questo è un cane\".\n\nDi buono c'è che è invariante per rototraslazione (in qualunque posizione si trovi, un occhio verrà sempre individuato come occhio), di meno buono che non mette in relazione zone distanti (ricordiamo, cerca solo pattern locali).\n\nChiaro è che tutto questo vale per immagini statiche. Se abbiamo sequenze dinamiche come frasi o video è necessario ricordare cosa è accaduto in passato. Questo viene fatto con le ***Recursive Neural Networks*** (***RNNs***), che tengono traccia degli stati precedenti... fino ad un certo punto. Già, perché se le sequenze sono troppo lunghe c'è il problema dei ***vanishing gradients*** (i.e. di fatto dimentico le cose più vecchie). Per ovviare a questa cosa esistono modelli con una gestione esplicita della memoria, detti di ***Long Short-Term Memory*** (***LSTM***).\n\nOltre a ciò, esistono situazioni in cui un pattern (e.g. audio) può essere \"stretchato\" (e.g. dico “Buuu…ooon…giorno” non rispettando la metrica standard). Qui il problema è che tutti i metodi visti finora sono ***sequenziali***, i.e. ogni frame guarda solo i suoi vicini. Per rilassare questa condizione entriamo in un'analisi parallela in cui si considerano porzioni più ampie dei dati.\n\nUna ***Time-Delay Neural Network*** (***TDNN***) considera blocchi di frame piuttosto che frame singoli (e.g. ne prendo 10 alla volta e li analizzo con dei Fully Connected Layers). In pratica, gestisce le durate variabili dei fonemi pronunciati in modo strano.\n\nE finalmente entriamo nel mondo dei ***Transformers***. Qui sfruttiamo appieno il concetto di ***parallelismo***, introducendo la ***Self-Attention***: mentre prima i token guardavano solo il precedente o quelli nella stessa finestra, qui ognuno vede tutti gli altri. Come si fa?\n\n- Anzitutto, questa roba era pensata inizialmente (2017) per NLP. Da qui infatti nascono i modelli GPT, che sfruttano la \"cognizione di causa\" di tutto il testo in parallelo per avere rapidamente il contesto utile a rispondere;\n- La prima applicazione su immagini è il ***Vision Transformer*** (***ViT***, 2020): spezzi l'immagine in patch (e.g. blocchi 16x16), appiattisci in un vettore e questo diventa un token simile ad un pezzo di frase in un problema di NLP. Gli aggiungi un positional encoding (i.e. dove si trovava nell'immagine) e dai tutto in pasto al modello, che costruisce un CLS Token riassuntivo delle caratteristiche dell'immagine;\n- Se il ViT riassume le patch in informazioni globali, potrei pensare di star perdendo di vista i gruppi di patch. Per ovviare a questo problema possiamo dividere l'immagine in zone d'interesse, in modo tale che non solo si formi un'idea globale di cosa c'è nell'immagine, ma anche di dove le varie cose sono posizionate localmente. Questa roba si chiama ***Locally Aware Transformer*** (***LA Transformer***);\n- Se passiamo ad un video, non solo mi serve la self-attention spaziale entro il singolo frame, ma anche quella temporale tra frame diversi. Posso applicare un LA Transformer anche in questo caso, <span style=\"color:#FFA500\">ma non sono sicuro che sia quello che ha fatto Michele (perché??)</span>","x":-2979,"y":-7300,"width":855,"height":1321,"color":"4"},
		{"id":"d86bb13b73cf864e","type":"text","text":"# Progetto\n\n\nBuonasera professoressa, le scrivo a nome del gruppo ecc...\n\nPer il progetto dell'esame di Biometric Systems vorremmo combinare l'eye-tracking ed il riconoscimento vocale, aggiungendo una misura anti-spoofing.\n\nL'utente può scegliere (o meno) di dichiarare la propria identità (se si lavora in verifica o in identificazione), dopodiché il sistema propone su uno schermo una frase generata in modo casuale. L'utente dovrà quindi leggere ad alta voce quanto scritto.\n\nLa lettura consente di prendere i dati relativi al movimento oculare tramite una webcam, la voce viene catturata da un registratore.\n\nUn modulo Speech-to-Text controlla se quanto detto dalla voce corrisponde alla frase casuale generata, impedendo di usare voci registrate. In caso il controllo abbia esito positivo, il modello effettua i controlli sui dati biometrici.\n\nL'idea è realizzare un sistema economico a livello di sensori e low effort per l'utente.\n\nAl momento non abbiamo trovato un dataset che includa entrambe queste misure da parte della stessa persona, ma riteniamo plausibile addestrare il modello in modo accettabile anche prendendo i dati da dataset separati. Ne approfittiamo comunque per chiederLe se per caso Lei ne conoscesse qualcuno.\n\nIn alternativa, se utilizzare l'eye-tracking risulti troppo difficile, pensavamo di ripiegare su un face-scan mantenendo invariata la struttura del progetto.\n\nAttendiamo la sua opinione sulla fattibilità di questo progetto per procedere.\n\nCordiali saluti ecc...","x":-1968,"y":-4960,"width":780,"height":842,"color":"5"},
		{"id":"95d5a8fc66a6d083","type":"text","text":"# Idea progetto\n\nUna roba potenzialmente multimodale, in cui usiamo un tratto principale ed uno secondario.\n\nEsempio, per loggare basta parlare (voice scan). Se il punteggio di matching è\n\n- inferiore ad una low threshold: si viene subito rifiutati;\n- superiore ad una high threshold: si viene subito accettati;\n- compreso tra le due threshold: si procede con un secondo tratto biometrico di backup (e.g. hand scan). Combinando i risultati dei due matching dovrei essere in grado di ridurre l'errore ed essere più preciso nel decidere se accettare o meno.\n\t- a questo punto devo ricadere su un sistema classico a singola threshold\n\npossibili problemi:\n\n- esistono DB che hanno sia voce che mano della stessa persona? (suppongo di sì);\n- quanto è difficile combinare i due risultati a livello di matematica (probabilità/statistica)?\n- qualcuno fa già cose simili?","x":-1968,"y":-5680,"width":780,"height":500,"color":"5"},
		{"id":"e36ff17a5440d5b7","type":"text","text":"cooperative private aware kind of user in a controlled setting","x":-951,"y":-5430,"width":531,"height":370},
		{"id":"68312d87ea4ed5b6","type":"text","text":"immagini: riduci dimensionalità, media, in pasto al modello\n\nclaude sonnet su VSCode, devi avere idea ad alto livello chiara","x":20,"y":-5183,"width":855,"height":363},
		{"id":"f5101d255b630972","type":"text","text":"gpaste","x":176,"y":-4590,"width":250,"height":60,"color":"1"},
		{"id":"76aa5e9f996ba8f3","type":"text","text":"","x":-736,"y":-3793,"width":616,"height":513},
		{"id":"297b1973a951d1b5","type":"file","file":"EsempioIdentificazione.png","x":4620,"y":4323,"width":764,"height":261},
		{"id":"f3cca5a3f6df1cd4","type":"file","file":"Gabor.png","x":-16400,"y":-1769,"width":400,"height":337},
		{"id":"c12f8fd7ff115cc8","type":"file","file":"SumOfRectangles.png","x":-17380,"y":1768,"width":570,"height":220}
	],
	"edges":[
		{"id":"fb0a974e652c8a2b","fromNode":"f6da8b1d468646d5","fromSide":"bottom","toNode":"eed580368731a96b","toSide":"top"},
		{"id":"4baf0ebd927b08c9","fromNode":"efcdc18499da9576","fromSide":"bottom","toNode":"f6da8b1d468646d5","toSide":"top"},
		{"id":"b400fc2395efec78","fromNode":"eed580368731a96b","fromSide":"bottom","toNode":"c3c219f1c9689029","toSide":"top"},
		{"id":"ebc0862f7de85daa","fromNode":"066eda0b63762e4f","fromSide":"top","toNode":"4bf5870defbcf5c9","toSide":"bottom","label":"Verifica"},
		{"id":"ed69b891c135d5d4","fromNode":"066eda0b63762e4f","fromSide":"bottom","toNode":"437ae081305ab212","toSide":"top","label":"Identificazione"},
		{"id":"4c9a045aaf3a43ca","fromNode":"4c0ac5a15b1f23ad","fromSide":"bottom","toNode":"bcc60cdc04b5895d","toSide":"top"},
		{"id":"5a39fbc5b7553d67","fromNode":"bcc60cdc04b5895d","fromSide":"bottom","toNode":"f1175d1af9fe2990","toSide":"top"},
		{"id":"51a76407d854014f","fromNode":"c3c219f1c9689029","fromSide":"bottom","toNode":"4c0ac5a15b1f23ad","toSide":"top","label":"Il Modello"},
		{"id":"f8dc485d09a4f976","fromNode":"4c0ac5a15b1f23ad","fromSide":"left","toNode":"cd2095e4c31cfc60","toSide":"right"},
		{"id":"b17ec63688d0500f","fromNode":"cd2095e4c31cfc60","fromSide":"left","toNode":"b81e5ec8e65b5242","toSide":"right"},
		{"id":"f2f94e8cb219c5ae","fromNode":"eed580368731a96b","fromSide":"left","toNode":"8818849b9b5e0e2d","toSide":"right"},
		{"id":"cbdeab744f155415","fromNode":"bcc60cdc04b5895d","fromSide":"left","toNode":"334ff31ec21a80e3","toSide":"right"},
		{"id":"3dd75ff78f2450e2","fromNode":"bcc60cdc04b5895d","fromSide":"left","toNode":"350412f4fee5a3ef","toSide":"right"},
		{"id":"976c9ae939d13916","fromNode":"4bf5870defbcf5c9","fromSide":"top","toNode":"bec8557c6aabbfee","toSide":"bottom"},
		{"id":"e893d87074d91d10","fromNode":"437ae081305ab212","fromSide":"bottom","toNode":"4acb6b0dcd714593","toSide":"top"},
		{"id":"b1ecb18eb75f723f","fromNode":"437ae081305ab212","fromSide":"bottom","toNode":"f9bb82c22ea2981e","toSide":"top"},
		{"id":"0ed9b73d1ff6a25a","fromNode":"4acb6b0dcd714593","fromSide":"bottom","toNode":"0029f2bd8b83473f","toSide":"top"},
		{"id":"7cce9724a77c9257","fromNode":"c3c219f1c9689029","fromSide":"left","toNode":"cd2095e4c31cfc60","toSide":"top"},
		{"id":"ca0c810475b5eedf","fromNode":"066eda0b63762e4f","fromSide":"right","toNode":"25a2bf3a89b19ed9","toSide":"top"},
		{"id":"fb8ab18290c1e1ec","fromNode":"25a2bf3a89b19ed9","fromSide":"bottom","toNode":"21071ea6413c0796","toSide":"top"},
		{"id":"c45e8cead0d4b621","fromNode":"4acb6b0dcd714593","fromSide":"top","toNode":"21071ea6413c0796","toSide":"bottom"},
		{"id":"0e55f198c108c70f","fromNode":"437ae081305ab212","fromSide":"right","toNode":"21071ea6413c0796","toSide":"left"},
		{"id":"919fbe53e63e0385","fromNode":"21071ea6413c0796","fromSide":"right","toNode":"297b1973a951d1b5","toSide":"left"},
		{"id":"6cea3212c2bddb7b","fromNode":"297b1973a951d1b5","fromSide":"left","toNode":"21071ea6413c0796","toSide":"right"},
		{"id":"f67e1ba9d1cf65b2","fromNode":"25a2bf3a89b19ed9","fromSide":"bottom","toNode":"437ae081305ab212","toSide":"top"},
		{"id":"8f86f5502c2d0bf1","fromNode":"f9bb82c22ea2981e","fromSide":"bottom","toNode":"0f97a59a582d24b7","toSide":"top"},
		{"id":"3795f87ae965b8e0","fromNode":"25a2bf3a89b19ed9","fromSide":"right","toNode":"fc3058e5571aad4f","toSide":"left"},
		{"id":"b3d7e4cf4ca16e16","fromNode":"066eda0b63762e4f","fromSide":"right","toNode":"e8a2dd3299cc2c92","toSide":"left","label":"Affidabilità del Modello"},
		{"id":"298b6ea06db7d3db","fromNode":"e8a2dd3299cc2c92","fromSide":"top","toNode":"f9faa215d5fddabb","toSide":"bottom"},
		{"id":"00cb4b1c0b9631a7","fromNode":"c3c219f1c9689029","fromSide":"left","toNode":"8818849b9b5e0e2d","toSide":"bottom"},
		{"id":"d49effe909732c4f","fromNode":"c3c219f1c9689029","fromSide":"right","toNode":"066eda0b63762e4f","toSide":"left","label":"Valutare e Migliorare il Modello"},
		{"id":"d01d943071d0aeab","fromNode":"066eda0b63762e4f","fromSide":"left","toNode":"987aa01415d17eed","toSide":"bottom"},
		{"id":"9e8297080a2edbfc","fromNode":"0f97a59a582d24b7","fromSide":"right","toNode":"f1084d1ab8558173","toSide":"left"},
		{"id":"66028ade3a85c84b","fromNode":"c3c219f1c9689029","fromSide":"left","toNode":"298ea7f436357c7e","toSide":"right"},
		{"id":"eff160d06aa6c526","fromNode":"066eda0b63762e4f","fromSide":"left","toNode":"71532c4f2c736877","toSide":"top"},
		{"id":"09fce5d0787f7422","fromNode":"f4896017c69ba8cb","fromSide":"left","toNode":"f6da8b1d468646d5","toSide":"right"},
		{"id":"8c015246e4917d24","fromNode":"f6da8b1d468646d5","fromSide":"left","toNode":"08d4569a4c15afaf","toSide":"right"},
		{"id":"90933766cb01c0a7","fromNode":"e8a2dd3299cc2c92","fromSide":"bottom","toNode":"59d4e4543428278c","toSide":"top"},
		{"id":"3306ef2e12b3bba0","fromNode":"e8a2dd3299cc2c92","fromSide":"right","toNode":"028aba75caedfa66","toSide":"left"},
		{"id":"d7cb9b0bdbe37a25","fromNode":"9b9d2f3c4858c4e8","fromSide":"top","toNode":"ac00a7e9478802aa","toSide":"bottom"},
		{"id":"0b99adcf1aa69466","fromNode":"9b9d2f3c4858c4e8","fromSide":"top","toNode":"5a7ff4295e8a033f","toSide":"bottom"},
		{"id":"38a5bf2acdb320ce","fromNode":"028aba75caedfa66","fromSide":"right","toNode":"9b9d2f3c4858c4e8","toSide":"left"},
		{"id":"72b43901a48b32c0","fromNode":"f4cc52883462f784","fromSide":"left","toNode":"c1d6332ea1b81bf0","toSide":"right"},
		{"id":"5bbbe8f2268bf786","fromNode":"c1d6332ea1b81bf0","fromSide":"top","toNode":"fd34db194919dca3","toSide":"bottom"},
		{"id":"75425adf3c5314e1","fromNode":"c1d6332ea1b81bf0","fromSide":"bottom","toNode":"9f9a55b05c246be5","toSide":"top"},
		{"id":"8b9790b97da15cf8","fromNode":"fd34db194919dca3","fromSide":"left","toNode":"18a2bf0c7ac250e6","toSide":"right"},
		{"id":"381661943c6eb243","fromNode":"c1d6332ea1b81bf0","fromSide":"bottom","toNode":"5e4f5bfadd024ecf","toSide":"top"},
		{"id":"424c6cbf3780ea1c","fromNode":"c1d6332ea1b81bf0","fromSide":"left","toNode":"88798430bf23e3d3","toSide":"right"},
		{"id":"de42b99cf6beee7b","fromNode":"f4cc52883462f784","fromSide":"bottom","toNode":"55b78b2a2edf7dfe","toSide":"top"},
		{"id":"7fbdadcc9ba03bf2","fromNode":"08d4569a4c15afaf","fromSide":"left","toNode":"f4cc52883462f784","toSide":"right"},
		{"id":"2d593ad12ac51f13","fromNode":"7eae48c6d6f1da2d","fromSide":"top","toNode":"fb8c3e96830020e4","toSide":"bottom"},
		{"id":"dd962317682fad9d","fromNode":"7eae48c6d6f1da2d","fromSide":"right","toNode":"bd684ec7a9167851","toSide":"left"},
		{"id":"07dbac1f40635804","fromNode":"ee38d6335014d54b","fromSide":"right","toNode":"7eae48c6d6f1da2d","toSide":"left","label":"Global"},
		{"id":"01ef99f618b47d44","fromNode":"ee38d6335014d54b","fromSide":"left","toNode":"0c592c293ba482ae","toSide":"right","label":"Local"},
		{"id":"cc3c7efc54160a93","fromNode":"ee38d6335014d54b","fromSide":"top","toNode":"4a35f5d889e637e2","toSide":"bottom"},
		{"id":"a7f4ad481310a0f8","fromNode":"55b78b2a2edf7dfe","fromSide":"left","toNode":"be6c4e35ea254e0f","toSide":"right"},
		{"id":"8d15030c266f7c5b","fromNode":"55b78b2a2edf7dfe","fromSide":"bottom","toNode":"cf74d7ca57f4d4cf","toSide":"top"},
		{"id":"64e475b08a6293d2","fromNode":"f4cc52883462f784","fromSide":"bottom","toNode":"a7584d0b54c4fa05","toSide":"top"},
		{"id":"db1761b13c488f23","fromNode":"f4cc52883462f784","fromSide":"top","toNode":"b94808e4f2c265bd","toSide":"bottom"},
		{"id":"a10a66b981231ab7","fromNode":"f4cc52883462f784","fromSide":"top","toNode":"fd34db194919dca3","toSide":"bottom"},
		{"id":"3d7811de8cd76dd5","fromNode":"f4cc52883462f784","fromSide":"top","toNode":"ee38d6335014d54b","toSide":"bottom"},
		{"id":"0e4e3961e9ba08b6","fromNode":"08d4569a4c15afaf","fromSide":"top","toNode":"2f94a815f931defe","toSide":"bottom"},
		{"id":"eeab236dae364889","fromNode":"08d4569a4c15afaf","fromSide":"bottom","toNode":"fadd10091b275ac7","toSide":"top"},
		{"id":"03b2662ae5f6c1dc","fromNode":"08d4569a4c15afaf","fromSide":"bottom","toNode":"c540fa8a95fc7612","toSide":"top"},
		{"id":"e4c315be1e99cb46","fromNode":"08d4569a4c15afaf","fromSide":"top","toNode":"c5ac94c35b35d4f6","toSide":"bottom"},
		{"id":"411c1cd8d681d92c","fromNode":"c540fa8a95fc7612","fromSide":"left","toNode":"2395f5208dcbd0b4","toSide":"right"},
		{"id":"86cfca57c4bce666","fromNode":"c540fa8a95fc7612","fromSide":"bottom","toNode":"6884a758da1d3f0f","toSide":"top"},
		{"id":"1346e582df8affa5","fromNode":"c540fa8a95fc7612","fromSide":"left","toNode":"d04d22904e4981bc","toSide":"right"},
		{"id":"b8ac21bc45e2f3e2","fromNode":"2395f5208dcbd0b4","fromSide":"bottom","toNode":"6f0324ae75ed4bbc","toSide":"top"},
		{"id":"e46175b36e6289a3","fromNode":"6884a758da1d3f0f","fromSide":"left","toNode":"6f0324ae75ed4bbc","toSide":"right"},
		{"id":"127efc63c679597b","fromNode":"6884a758da1d3f0f","fromSide":"right","toNode":"4359246fbca402e0","toSide":"left"},
		{"id":"4ad2d97475950a3e","fromNode":"6884a758da1d3f0f","fromSide":"bottom","toNode":"7935ff9549acc1be","toSide":"top"},
		{"id":"9139a0dd31b19aec","fromNode":"2f94a815f931defe","fromSide":"bottom","toNode":"425aeba4cd4d1e1e","toSide":"top"},
		{"id":"b0e15525ee262418","fromNode":"2f94a815f931defe","fromSide":"left","toNode":"ae8464ae65f71a2f","toSide":"right"},
		{"id":"4032bab61f32e7fd","fromNode":"2f94a815f931defe","fromSide":"top","toNode":"5502640d95e0cb4c","toSide":"bottom"},
		{"id":"cc720581a07bc5cd","fromNode":"ae8464ae65f71a2f","fromSide":"top","toNode":"5502640d95e0cb4c","toSide":"left"},
		{"id":"30f9eab417a146c5","fromNode":"2f94a815f931defe","fromSide":"right","toNode":"ac6926d1d18b85b2","toSide":"left"},
		{"id":"b14f7b8d72da70f7","fromNode":"5502640d95e0cb4c","fromSide":"top","toNode":"a4870765680e2788","toSide":"bottom"},
		{"id":"515137a7c5b2e781","fromNode":"fadd10091b275ac7","fromSide":"right","toNode":"119aed33db32e646","toSide":"left"},
		{"id":"2d7471009927c171","fromNode":"fadd10091b275ac7","fromSide":"bottom","toNode":"7c2aba2dd87b85cd","toSide":"top"},
		{"id":"72935ab364b2ca11","fromNode":"7c2aba2dd87b85cd","fromSide":"bottom","toNode":"59e7c3a086ae6664","toSide":"top"},
		{"id":"5ff5838fd66fe5da","fromNode":"59e7c3a086ae6664","fromSide":"left","toNode":"080aa8e14697619f","toSide":"right"},
		{"id":"567f2b3e7347b86e","fromNode":"59e7c3a086ae6664","fromSide":"right","toNode":"910498450377e092","toSide":"left"},
		{"id":"9327a6b2b45cbc30","fromNode":"7c2aba2dd87b85cd","fromSide":"right","toNode":"910498450377e092","toSide":"top"},
		{"id":"8faf7205bae6eddd","fromNode":"08d4569a4c15afaf","fromSide":"right","toNode":"2edefd620b8def34","toSide":"left"},
		{"id":"bc5707ea50ff0a98","fromNode":"f6da8b1d468646d5","fromSide":"left","toNode":"2edefd620b8def34","toSide":"right"},
		{"id":"8b64abcb3764e6a2","fromNode":"2edefd620b8def34","fromSide":"top","toNode":"da1de2625fe0aa22","toSide":"top"},
		{"id":"770c6559cf2c8df7","fromNode":"2edefd620b8def34","fromSide":"bottom","toNode":"5875daaf402b4ede","toSide":"top"},
		{"id":"9bbe0ee96a2049a1","fromNode":"298ea7f436357c7e","fromSide":"left","toNode":"bc8bb53d411c6fbc","toSide":"right"},
		{"id":"349b36ce33e042f7","fromNode":"f32d6cc54c9299df","fromSide":"right","toNode":"26f5b713e47897e9","toSide":"left"},
		{"id":"553c5076d3a1cf6d","fromNode":"26f5b713e47897e9","fromSide":"top","toNode":"32572b8a289896a1","toSide":"bottom"},
		{"id":"59a63c9c683488a4","fromNode":"32572b8a289896a1","fromSide":"top","toNode":"c4c4a31ed5bbf9e3","toSide":"bottom"},
		{"id":"6597b29aab04b628","fromNode":"811c43decb94ee1d","fromSide":"left","toNode":"e671a8e608e0a1dd","toSide":"right"},
		{"id":"cc01868d2846f9e9","fromNode":"811c43decb94ee1d","fromSide":"right","toNode":"f32d6cc54c9299df","toSide":"left"},
		{"id":"fa2b4649840beced","fromNode":"811c43decb94ee1d","fromSide":"bottom","toNode":"95d5a8fc66a6d083","toSide":"top"},
		{"id":"1ff7e669e921414c","fromNode":"0c592c293ba482ae","fromSide":"left","toNode":"f3cca5a3f6df1cd4","toSide":"right"},
		{"id":"bc1b2f3fb9d04cee","fromNode":"5e4f5bfadd024ecf","fromSide":"left","toNode":"c12f8fd7ff115cc8","toSide":"right"}
	]
}