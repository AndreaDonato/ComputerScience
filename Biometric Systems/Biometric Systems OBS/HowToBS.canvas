{
	"nodes":[
		{"id":"eed580368731a96b","type":"text","text":"# Schema Concettuale\n\nL'idea generale di un ***Biometric System*** (***BS***) è semplice, e consta di due semplici step.\n\n- ***Enrollment*** - ***Costruisce un DB*** catturando i dati biometrici grezzi (detti ***sample***, e.g. la foto del volto), estraendone le ***features*** (e.g. la distanza tra gli zigomi, dalla punta del naso al mento, ...) ed associando loro un'identità (e.g. \"Questo è Andrea\"). I ***template*** così ottenuti vengono raccolti nella ***gallery***.\n\t- Ovviamente è una fase preventiva all'azione, ma potrei continuare a raccogliere nuovi dati per il mio DB anche mentre sono in fase di riconoscimento.\n- ***Riconoscimento*** - Prendo il modello con il suo DB, gli fornisco gli opportuni dati grezzi in modo che lui possa estrarne le features (***probe***) e confrontarle con ciò che ha nel DB. Due scenari:\n\t- ***Verifica*** - L'utente dichiara di essere registrato. Il sistema confronta i suoi TB con quelli dell'utente che dice di essere (Controllo 1:1);\n\t\t- FaceID di iPhone è un ***identity claim*** implicito.\n\t- ***Identificazione*** - L'utente non dichiara niente, sta al sistema capire chi è. Per fare ciò deve confrontare i suoi TB con tutti quelli presenti nel DB (Controllo 1:N). A questo punto, anche qui, due scenari:\n\t\t- \"Questo tizio è sicuramente uno di quelli che hai nel DB\". Anche se la similarità è bassa, restituisco comunque il best match tra quelli che ho (***closed-set***), anche se così rischio l'errore;\n\t\t- \"Non è detto che tu sappia chi è\". Se ottengo una similarità bassa potrei scegliere di restituire un messaggio del tipo \"`Reject - Secondo me questo non sta nel DB`\" (***open-set***).\n\nTutto questo avviene sotto l'***assunzione che ogni persona sia unica***. Questo dipende dalle features. Se si usa il DNA è ovvio, ma in generale la fase di recognition non otterrà mai un match al $100\\%$ con la gallery, per motivi\n\n- ambientali, e.g. illuminazione e/o angolazione;\n- fisiologici, e.g. posso avere la \"faccia tirata\" perché ho l'influenza.\n\nPer questo motivo, il modello deve prevedere una ***tolleranza*** entro cui gestire match multipli.","x":-555,"y":1327,"width":918,"height":718,"color":"4"},
		{"id":"c3c219f1c9689029","type":"text","text":"# Schema Logico-Implementativo\n\nUna volta scelto il tipo dei dati grezzi, le fasi concettuali vengono generalmente implementate tramite 4 ***moduli***.\n\n- ***Sensori*** - Raccolgono i dati biometrici grezzi;\n- ***Feature Extraction Module*** - Dati i dati dei sensori, ne estrae le features per come previsto dal modello e le usa per costruire il feature vector;\n\t- serve compatibilità con i dati dei sensori? penso dipenda da come è scritto il codice... Di certo se il modello si aspetta la foto di un volto non ha senso dargli una foto di una macchina (proverà comunque a estrarne delle features per come è addestrato a fare, con risultati imprevedibili). Magari se lo si associa ad un sistema che riconosce i volti in una foto in cui questi sono nascosti...\n- ***Matching Module*** - Fa i calcoli e restituisce i match con i vari template. Questo può essere fatto con\n\t- ***Distanza*** - Se è \"piccola\" il match è \"buono\", e.g. distanza euclidea tra due FV;\n\t- ***Similarità*** - Più legata all'orientazione dei FV, in genere si calcola il coseno dell'angolo che formano. Contrariamente alla distanza, il match è \"buono\" se la similarità $\\to1$.\n- ***Decision Module*** - Dati i risultati del processo di matching, sceglie cosa farci sulla base delle politiche e delle threshold (che va scelta simulando, in base alla convenienza che in genere è minimizzare false acceptance rate\n\n\nquesti moduli funzionano in riconoscimento, in enrollment mi fermo al feature extraction?","x":-555,"y":2160,"width":918,"height":620,"color":"3"},
		{"id":"0944ccc58190b193","type":"text","text":"features\nmeasures\n\nenglish makes no sense","x":238,"y":-680,"width":250,"height":125},
		{"id":"747086ac91be7c18","type":"text","text":"project + oral same session\n\nproject = build biometric system + performance evaluation, ma prima parla con lei. always mention the sources\n\nvene, retina, iride, orecchio sono tratti abbastanza univoci per fare pattern recognition\n\nfaccia importante, molte sue tecniche di pr si prendono per fare altro\n\nSVM utili. embeddings (set distanze tra punti) usati per fare training.\n\nbehavioural traits more difficult to reproduce for an attacker.\n\nverification - assess identity (i have claim of identity). dichiaro chi sono e l'algoritmo è comparativo con il DB di gente autorizzata ad entrare (e.g. quando sblocchi il telefono è un claim di identità implicito)\n- Questa roba deve essere sia strict che flessibile, perché se sono stanco e ho la faccia tirata non è che il telefono non si sblocca...\nidentification - (i dont). ho un DB pseudocompleto e riconduco l'input al best guess tra tutti quelli che ho.\n\ni tratti devono essere discriminativi. quelli che non lo sono (e.g. il colore dei capelli) sono considerabili \"soft biometrics\". assunzione che ogni persona sia unica (siccome non posso avere 10% accuracy questo in realtà non è vero, ma è una buona approx).\n\ndetto ciò, se individuo un buon tratto, quali features devo estrarne? mediapipe libreria per estrarre features. minuties? need triplets (x y orientazione).\n\nmanca un'intera lezione di 3h qui","x":1308,"y":-455,"width":716,"height":766},
		{"id":"19750c0603c5e6dd","type":"text","text":"The Biometric Consortium define biometrics as “automatic recognition of a person\naccording to discriminative characteristics”.","x":720,"y":-230,"width":408,"height":215},
		{"id":"efcdc18499da9576","x":-555,"y":-200,"width":918,"height":839,"color":"4","type":"text","text":"# Premesse alla Biometrica: Classi e Pattern Recognition\n\nGli oggetti rappresentati nelle immagini possono essere raggruppati in diverse ***classi*** (e.g. fiori, volti, paesaggi), i cui elementi presentano caratteristiche uniche. Guardando questo insieme di caratteristiche, un osservatore è (idealmente) in grado di dire che \"questa immagine rappresenta un cane\".\n\nEntrando più nel dettaglio (i.e. osservando caratteristiche più complesse) potremmo addirittura scendere nel dettaglio delle ***sottoclassi***, e distinguere un pastore tedesco da un labrador.\n\nSe l'osservatore è un sistema informatico concettualmente cambia poco. La singola caratteristica che considero si chiama ***feature***, l'insieme delle caratteristiche utili a discriminare tra diverse classi si chiama ***feature vector*** (o ***pattern***), e il compito di riconoscere a quale (sotto)classe appartiene l'immagine in input viene eseguito da un modello di ***machine learning supervisionato*** (in ***classificazione***) detto ***pattern recognition***.\n\nDetto questo, ***la biometrica è un modello di pattern recognition in cui ogni individuo è una classe a sé stante***.\n\n\"Scusa, ma allora fa tutto l'algoritmo?\"\nSì, ma lo devi progettare. Ci sono tre questioni principali alle quali devi rispondere.\n\n- Pur vero che ***è il modello ad estrarre i pattern***, ***tu devi riconoscere quali dati grezzi è meglio fornirgli***.\n\t- Se provi ad identificare le persone dalla foto della spalla, il modello ce la metterà tutta ad estrarre i pattern migliori per discriminare tra le varie persone, ma se i dati grezzi non offrono nulla di significativo lui non troverà niente di significativo per discriminare, producendo pessime performance;\n\t- Questo include la ***diversificazione*** dei dati grezzi. Se ho scelto la faccia come dato grezzo ma ogni persona è rappresentata da una singola foto frontale con perfetta illuminazione e senza espressioni, il modello potrebbe avere qualche difficoltà se poi gli chiedo di riconoscere qualcuno che ride di profilo.\n- Devi anche ***metterlo nelle migliori condizioni di estrarre i pattern dai dati grezzi***.\n\t- Potrei fare una semplice Random Forest e lasciare tutto all'algoritmo. Oppure ha più senso dividere il programma in moduli e dedicarne uno al processo di ***features extraction***, il cui output viene poi dato in input del modulo che fa i calcoli sulle features. Così, per dire...\n- Dati i pattern, devi dire al modello come discriminare tra di essi, cioè ***definire la metrica di valutazione***.\n- Dati i risultati del modello, sta a te ***definire la tolleranza*** di accettazione o rifiuto tramite delle threshold.\n\nTutto questo si applica a qualsiasi tipo di input, sia esso un'immagine, un audio, i dati di un accelerometro, ..."},
		{"id":"f6da8b1d468646d5","type":"text","text":"# Biometrics\n\nPer **biometrics** si intende la **capacità di riconoscere una persona dai suoi tratti somatici e/o comportamentali**, detti ***tratti biometrici*** (***TB***).\n\nÈ un'**alternativa all'autenticazione tramite oggetto** (e.g. \"Il tuo contatto è l'uomo con la valigetta nera\") **o  conoscenza** (e.g. \"Il tuo contatto conosce la password\"), ed è un buon **punto d'incontro tra facilità di utilizzo** (non richiede oggetti o memoria) **e precisione**.","x":-390,"y":860,"width":589,"height":261,"color":"6"},
		{"id":"f4896017c69ba8cb","x":-1660,"y":791,"width":600,"height":399,"color":"4","type":"text","text":"# Breve nota storica\n\nNel 1882 il grande capo della polizia di Parigi Bertillon decise di iniziare a prendere varie misure biometriche del corpo dei detenuti (e.g. forma della mano, della testa, dettagli facciali).\n\nQuesta roba ebbe un sacco successo, tanto che nel 1896 anche l'FBI iniziò a schedare le persone e nel 1900 queste carte divennero regolamentate dalla legge francese.\n\nQualcuno però storse il naso. Per la privacy? No, perché questa roba era inefficiente. Tale Galton introdusse il concetto di ***minutiae***, ovvero piccolissimi tratti distintivi in grado di identificare in modo univoco una persona. Un esempio? Data una ***fingerprint***, le minutiae sono i punti in cui le linee terminano o si dividono.\n\n"},
		{"id":"ffa28aa2dfac9d98","x":-1660,"y":1278,"width":552,"height":122,"type":"text","text":"Ancora oggi si fa riferimento alle misurazioni biometriche \"grossolane\" come misure alla bertillon? mi sa che l'ho letto ma non ritrovo dove"},
		{"id":"c6df0c468aac74b6","x":640,"y":639,"width":780,"height":620,"type":"text","text":"C'è tutta una pippa mentale sul tipo di utente che se hai voglia poi ti vedi\n\n- Cooperative: the user is interested in recognition (an impostor might try to be recognized as a legal user). \n- Non-cooperative: the user is indifferent or even adverse to recognition (an impostor might try to avoid being recognized)\n- Public/Private: users of the system are customers or employees of the entity installing the system\n- Used/Non used: frequency of use of the biometric system (more times a day, daily, weekly, monthly, occasionally …).\n- Aware/Not aware: the user is aware or not of the recognition process\n\ne poi sui setting\n\n- Controlled: capture settings can be controlled, distortions mostly avoided (e.g., for face, pose, illumination, and expression), defective templates can be rejected, and capture repeated\n- Uncontrolled/undercontrolled: capture settings cannot be controlled, template can present various levels of distortion, defective templates can be rejected, but capture cannot be repeated"},
		{"id":"237a1e0bfd57425c","type":"text","text":"in genere si sacrifica genuine acceptance per minimizzare false acceptance.\n\nse il TB cambia (e.g. invecchio) dovrei comunque essere in grado di riconoscere quel TB. fingerprints sono considerate universal and reliable ma possono esserci danni. voce non universale obv, cataratta per retina (o iride?), ...\n\nzero-error attack dichiaro di essere qualcun altro\nspoofing fare qualcosa to mislead recognition system (posso creare falso fingerprint con colle e simili, e seguono tecniche anti-spoofing). posso presentare la foto dell'iride di un'altra persona, o usare lenti a contatto.\n\nDNA univoco, ma se ricevo donazione di midollo potrebbe essere ambiguo.\nretina funziona bene. behavioral traits difficili da riprodurre. walking richiede strategia raffinata in modo molto specifico per ogni persona, difficile da riprodurre a lungo. signature, soprattutto la dinamica del polso che la fa.\n\nhand-crafted features = features... con cui creare il FV (si possono anche estrarre come embeddings, cioè il livello quasi-finale di una NN, anche detto latent space in quanto non interpretabile da un umano ma utilizzabile da una macchina)\n\nminutiae = bifurcation of lines, end of lines, ... (on fingerprint FP)\n\ncityblock (L1 distance) ce ne sono tipo mille, ma distinguo solo distanza e similarità. la covarianza è una misura di similarità, usata per FP. per confrontare gli istogrammi c'è la distanza di Bhattacharyya. in generale, ogni tipologia di gallery (i.e. tipo di template al suo interno) ha la sua misura ottimale. dyamic time warping per dati con accelerometri tipo, nel senso che confronta due funzioni considerando gli stretch e le fasi. in pratica prova a farle corrispondere. più è difficile più sono distanti. insomma, è la distanza da usare tra segnali(t).\n\nFP. in genere problema di orientazione e che spesso si trovano solo frammenti di FP. ci sono modi speciali di fare la distanza (NN).\n\nerrori di tipo I e II non sono ottimizzabili contemporaneamente, in genere serve trade-off. l'analisi degli errori si porta dietro tutti i problemi del training in ML.\n\nricorda che false rejection ha al denominatore il totale dei probes che should be accepted ($\\text{true but rejected}\\over\\text{TbR + True and Accepted}$) . FR e GA sum up to one (i.e. genuine probes). nei casi limite si lascia la scelta all'umano.\n\nil training è sempre supervised.\n\nmore template for same person to address intra-class variations. meglio pochi ma buoni che prendere 200 frame di un video in cui close frames sono simili e la risoluzione è bassa.","x":2200,"y":2700,"width":708,"height":1201},
		{"id":"cd2095e4c31cfc60","type":"text","text":"# How To Features\n\nAnzitutto, cosa devo salvare nella gallery? Cioè, con quali criteri scelgo le features (i.e. i dati grezzi da cui poi il modello estrarrà delle buone features)?\n\nDevo scegliere ***TB in grado di identificare univocamente il generico essere umano***, pertanto devono essere\n\n- ***universali*** (i.e. posseduti da tutti, salvo rare eccezioni);\n- ***permanenti*** (i.e. che non possono cambiare nel tempo);\n- ***misurabili*** dai sensori (i.e. quantificabili in numeri);\n- la cui misura è ***accettabile*** dall'utente (i.e. che rispettino la privacy).\n\nSimili TB sono detti ***strong features*** (***SF***), come ***impronte digitali***, ***retina*** e ***orecchie***.\n\nViceversa, un TB come il colore dei capelli è una ***weak feature*** (***WF***), in quanto non universale (impossibile distinguere tra le persone calve), non permanente (posso fare una tinta, o invecchiare, o usare una parrucca) e soprattutto non univoca.\n\nCi sono delle vie di mezzo? Ovviamente sì. I ***tratti comportamentali*** come ***gait*** (andamento della camminata), ***scrittura*** (in particolare la ***dinamica di movimento*** del polso e del braccio) e la ***voce*** sembrano da un lato una SF, ma dall'altro possono cambiare con l'età o con le condizioni fisiche e/o ambientali. Sicuramente sono ***più difficili da riprodurre*** da parte di un attaccante.\n\n","x":1820,"y":1327,"width":720,"height":718,"color":"4"},
		{"id":"f109114cd2f3fef4","x":760,"y":1536,"width":540,"height":300,"color":"3","type":"text","text":"# Enrollment\n\nOvvero, come costruisco la gallery? Devo capire\n\n- Quali sono i migliori dati grezzi da dare in pasto al modello (i.e. scegliere se usare foto di volti, fingerprint, ...);\n- Come prendere le misure (i.e. sensori);\n- Quale modello è in grado di estrarne le features migliori"},
		{"id":"f7c635f63f3a96b7","x":1122,"y":5440,"width":250,"height":60,"type":"text","text":"# Retina"},
		{"id":"f4cc52883462f784","x":1666,"y":5475,"width":250,"height":60,"type":"text","text":"# Face"},
		{"id":"c5ac94c35b35d4f6","x":-888,"y":5480,"width":599,"height":140,"type":"text","text":"# Voce - Gaussian Mixture Model (GMM)"},
		{"id":"1ec09233624528e6","x":-208,"y":5605,"width":353,"height":170,"type":"text","text":"# Signature "},
		{"id":"fadd10091b275ac7","x":292,"y":5605,"width":250,"height":91,"type":"text","text":"# Fingerprint"},
		{"id":"2f94a815f931defe","x":581,"y":5952,"width":250,"height":60,"type":"text","text":"# Iris"}
	],
	"edges":[
		{"id":"fb0a974e652c8a2b","fromNode":"f6da8b1d468646d5","fromSide":"bottom","toNode":"eed580368731a96b","toSide":"top"},
		{"id":"5f9e327042098636","fromNode":"eed580368731a96b","fromSide":"bottom","toNode":"c3c219f1c9689029","toSide":"top"},
		{"id":"4baf0ebd927b08c9","fromNode":"efcdc18499da9576","fromSide":"bottom","toNode":"f6da8b1d468646d5","toSide":"top"},
		{"id":"0e7dabb490cb55e3","fromNode":"f6da8b1d468646d5","fromSide":"left","toNode":"f4896017c69ba8cb","toSide":"right"},
		{"id":"0160816f258ecb33","fromNode":"eed580368731a96b","fromSide":"right","toNode":"f109114cd2f3fef4","toSide":"left"},
		{"id":"2530e5244bb21b0f","fromNode":"f109114cd2f3fef4","fromSide":"right","toNode":"cd2095e4c31cfc60","toSide":"left"}
	]
}