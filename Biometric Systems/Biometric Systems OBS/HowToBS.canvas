{
	"nodes":[
		{"id":"0944ccc58190b193","type":"text","text":"features\nmeasures\n\nenglish makes no sense","x":238,"y":-680,"width":250,"height":125},
		{"id":"747086ac91be7c18","type":"text","text":"project + oral same session\n\nproject = build biometric system + performance evaluation, ma prima parla con lei. always mention the sources\n\nvene, retina, iride, orecchio sono tratti abbastanza univoci per fare pattern recognition\n\nfaccia importante, molte sue tecniche di pr si prendono per fare altro\n\nSVM utili. embeddings (set distanze tra punti) usati per fare training.\n\nbehavioural traits more difficult to reproduce for an attacker.\n\nverification - assess identity (i have claim of identity). dichiaro chi sono e l'algoritmo è comparativo con il DB di gente autorizzata ad entrare (e.g. quando sblocchi il telefono è un claim di identità implicito)\n- Questa roba deve essere sia strict che flessibile, perché se sono stanco e ho la faccia tirata non è che il telefono non si sblocca...\nidentification - (i dont). ho un DB pseudocompleto e riconduco l'input al best guess tra tutti quelli che ho.\n\ni tratti devono essere discriminativi. quelli che non lo sono (e.g. il colore dei capelli) sono considerabili \"soft biometrics\". assunzione che ogni persona sia unica (siccome non posso avere 10% accuracy questo in realtà non è vero, ma è una buona approx).\n\ndetto ciò, se individuo un buon tratto, quali features devo estrarne? mediapipe libreria per estrarre features. minuties? need triplets (x y orientazione).\n\nmanca un'intera lezione di 3h qui","x":1308,"y":-455,"width":716,"height":766},
		{"id":"19750c0603c5e6dd","type":"text","text":"The Biometric Consortium define biometrics as “automatic recognition of a person\naccording to discriminative characteristics”.","x":720,"y":-230,"width":408,"height":215},
		{"id":"f6da8b1d468646d5","type":"text","text":"# Biometrics\n\nPer **biometrics** si intende la **capacità di riconoscere una persona dai suoi tratti somatici e/o comportamentali**, detti ***tratti biometrici*** (***TB***).\n\nÈ un'**alternativa all'autenticazione tramite oggetto** (e.g. \"Il tuo contatto è l'uomo con la valigetta nera\") **o  conoscenza** (e.g. \"Il tuo contatto conosce la password\"), ed è un buon **punto d'incontro tra facilità di utilizzo** (non richiede oggetti o memoria) **e precisione**.","x":-390,"y":860,"width":589,"height":261,"color":"6"},
		{"id":"c6df0c468aac74b6","type":"text","text":"C'è tutta una pippa mentale sul tipo di utente che se hai voglia poi ti vedi\n\n- Cooperative: the user is interested in recognition (an impostor might try to be recognized as a legal user). \n- Non-cooperative: the user is indifferent or even adverse to recognition (an impostor might try to avoid being recognized)\n- Public/Private: users of the system are customers or employees of the entity installing the system\n- Used/Non used: frequency of use of the biometric system (more times a day, daily, weekly, monthly, occasionally …).\n- Aware/Not aware: the user is aware or not of the recognition process\n\ne poi sui setting\n\n- Controlled: capture settings can be controlled, distortions mostly avoided (e.g., for face, pose, illumination, and expression), defective templates can be rejected, and capture repeated\n- Uncontrolled/undercontrolled: capture settings cannot be controlled, template can present various levels of distortion, defective templates can be rejected, but capture cannot be repeated\n\nserve a qualcosa? ai posteri l'ardua sentenza","x":640,"y":639,"width":780,"height":620},
		{"id":"efcdc18499da9576","type":"text","text":"# Premesse alla Biometrica: Classi e Pattern Recognition\n\nGli oggetti rappresentati nelle immagini possono essere raggruppati in diverse ***classi*** (e.g. fiori, volti, paesaggi), i cui elementi presentano caratteristiche uniche. Guardando questo insieme di caratteristiche, un osservatore è (idealmente) in grado di dire che \"questa immagine rappresenta un cane\".\n\nEntrando più nel dettaglio (i.e. osservando caratteristiche più complesse) potremmo addirittura scendere al livello delle ***sottoclassi***, e distinguere un pastore tedesco da un labrador.\n\nSe l'osservatore è un sistema informatico concettualmente cambia poco. La singola caratteristica che considero si chiama ***feature***, l'insieme delle caratteristiche utili a discriminare tra diverse classi si chiama ***feature vector*** (o ***pattern***), e il compito di riconoscere a quale (sotto)classe appartiene l'immagine in input viene eseguito da un modello di ***machine learning supervisionato*** (in ***classificazione***) detto ***pattern recognition***.\n\nDetto questo, ***la biometrica è un modello di pattern recognition in cui ogni individuo è una classe a sé stante***.\n\n\"Scusa, ma allora fa tutto l'algoritmo?\"\nSì, ma lo devi progettare. Ci sono tre questioni principali alle quali devi rispondere.\n\n- Pur vero che ***è il modello ad estrarre i pattern***, ***tu devi riconoscere quali dati grezzi è meglio fornirgli***.\n\t- Se provi ad identificare le persone dalla foto della spalla, il modello ce la metterà tutta ad estrarre i pattern migliori per discriminare tra le varie persone, ma se i dati grezzi non offrono nulla di significativo lui non troverà niente di significativo per discriminare, producendo pessime performance;\n\t- Questo include la ***diversificazione*** dei dati grezzi. Se ho scelto la faccia come dato grezzo ma ogni persona è rappresentata da una singola foto frontale con perfetta illuminazione e senza espressioni, il modello potrebbe avere qualche difficoltà se poi gli chiedo di riconoscere qualcuno che ride di profilo.\n- Devi anche ***metterlo nelle migliori condizioni di estrarre i pattern dai dati grezzi***.\n\t- Potrei fare una semplice Random Forest e lasciare tutto all'algoritmo. Oppure ha più senso dividere il programma in moduli e dedicarne uno al processo di ***features extraction***, il cui output viene poi dato in input del modulo che fa i calcoli sulle features.\n- Dati i pattern, devi dire al modello come discriminare tra di essi, cioè ***definire la metrica di valutazione***.\n- Dati i risultati del modello, sta a te ***definire la tolleranza*** di accettazione o rifiuto tramite delle threshold.\n\nTutto questo si applica a qualsiasi tipo di input, sia esso un'immagine, un audio, i dati di un accelerometro, ...","x":-554,"y":-200,"width":918,"height":839,"color":"4"},
		{"id":"4c0ac5a15b1f23ad","type":"text","text":"# Feature Extraction Module\n\nSe il FEM è al lavoro significa che ho già deciso quali sono i migliori dati grezzi da dare in pasto al modello (i.e. ho scelto di usare foto di volti, o fingerprint). A questo punto, a seconda di quello che ho scelto, esistono diversi metodi per l'estrazione delle features da quel particolare tipo di dati grezzi.\n\n- ***Algoritmi Classici*** - Utilizzano modelli geometrici e analisi matematica per estrarre le informazioni rilevanti;\n\t- Algoritmi come ***SIFT*** (**Scale-Invariant Feature Transform**) e ***SURF*** (**Speeded-Up Robust Features**) partono con un set di punti chiave di un oggetto (e.g. set di punti chiave di un occhio con relative distanze), cercano localmente nell'immagine delle zone interessanti a cui agganciare questi punti chiave (e.g. forti variazioni di intensità di colore dei pixel) e provano a fare una misura di match, restituendo tutte le ***porzioni locali*** di immagine in cui hanno fatto best match;\n\t- ***HOG*** (**Histogram of Oriented Gradients**) calcola il gradiente e restituisce un template che in pratica è l'***intera immagine*** con i contorni evidenziati;\n\t- Alcuni scelgono le features che meglio separano le classi (***riduzione dimensionale***). Esempi sono ***LDA*** (**Linear Discriminant Analysis**) e ***PCA*** (**Principal Component Analysis**).\n\t- Sempre utile la ***trasformata di Fourier*** per i segnali audio.\n- ***Algoritmi Statistici*** - Autoesplicativo, esempi sono ***LBP*** (**Local Binary Patterns**) per il riconoscimento facciale e la ***Wavelet Transform*** per la riduzione del rumore e la scomposizione dei segnali;\n- ***Machine Learning*** - Possono tornare utili algoritmi di ML classico come ***Random Forest*** e ***SVM*** (**Support Vector Machines**), soprattutto in caso di dati strutturati;\n- ***Neural Networks*** - Una NN che spara in output un FV. In questo caso ***non ho idea di cosa rappresentino i dati che estraggo***. E che mi importa? Che quando vado a fare il matching in teoria dovrei scegliere la metrica più adatta al tipo di dato in input, ma se non so qual è devo andare a tentativi.","x":-555,"y":3148,"width":918,"height":651,"color":"4"},
		{"id":"bcc60cdc04b5895d","type":"text","text":"# Matching Module \n\nDati due template da comparare, ci sono sostanzialmente due alternative:\n\n- I template sono vettori (i.e. FV) contenenti misure di features significative (e.g. distanze tra i punti chiave di un volto, coordinate e orientazione delle minutiae sulle fingerprints). In questo caso posso cercare la ***minima distanza***.\n\t- Un esempio banale è la ***distanza euclidea*** (***L2***), o la ***distanza di Manhattan*** (***L1***);\n\t- Uno meno banale è la ***distanza di Bhattacharyya***, che misura la ***sovrapposizione tra due pdf o istogrammi***.\n\t- Per dati dinamici (e.g. due tracce audio) si usa il ***Dynamic Time Warping*** (***DTW***), che con un processo non lineare confronta meglio due forme d'onda sfasate.\n- I template sono funzioni, istogrammi, oggetti per i quali non è ovvio definire una distanza o per i quali non è rilevante nel confronto. In questo caso si cerca la ***massima similarity***.\n\t- L'esempio più comune è la ***cosine similarity*** $S_C$, che guarda l'angolo individuato dai due vettori. $S_C$ è massima quando $\\cos\\vartheta$ vale $1$;\n\t- La ***Pearson Correlation*** valuta la ***correlazione lineare tra pdf e istogrammi***, quindi è sensibile al disallineamento (contrariamente al DTW).\n\nTutto questo assume che io sappia cosa rappresenta il template, ma questo non è vero se esso è generato da una NN. E quindi? Le provo tutte e vedo che succede!","x":-555,"y":4012,"width":918,"height":559,"color":"4"},
		{"id":"f1175d1af9fe2990","type":"text","text":"# Decision Module\n\nsulla base della threshold decido","x":-555,"y":4760,"width":918,"height":480},
		{"id":"c3c219f1c9689029","type":"text","text":"# Schema Logico-Implementativo\n\nUna volta scelto il tipo dei dati grezzi, le fasi concettuali vengono generalmente implementate tramite 4 ***moduli***.\n\n- ***Sensori*** - Raccolgono i dati biometrici grezzi;\n- ***Feature Extraction Module*** - Dati i dati dei sensori, ne estrae le features per come previsto dal modello e le usa per costruire il feature vector (vero? o non necessariamente il template è un FV? o non sempre il FV è un vettore nel senso stretto del termine?);\n\t- serve compatibilità con i dati dei sensori? penso dipenda da come è scritto il codice... Di certo se il modello si aspetta la foto di un volto non ha senso dargli una foto di una macchina (proverà comunque a estrarne delle features per come è addestrato a fare, con risultati imprevedibili). Magari se lo si associa ad un sistema che riconosce i volti in una foto in cui questi sono nascosti...\n- ***Matching Module*** - Fa i calcoli e restituisce i match con i vari template. Questo può essere fatto con\n\t- ***Distanza*** - Se è \"piccola\" il match è \"buono\", e.g. distanza euclidea tra due FV;\n\t- ***Similarità*** - Più legata all'orientazione dei FV, in genere si calcola il coseno dell'angolo che formano. Contrariamente alla distanza, il match è \"buono\" se la similarità $\\to1$.\n- ***Decision Module*** - Dati i risultati del processo di matching, sceglie cosa farci sulla base delle politiche e delle threshold (che va scelta simulando, in base alla convenienza che in genere è minimizzare false acceptance rate\n\n\nquesti moduli funzionano in riconoscimento, in enrollment mi fermo al feature extraction?","x":-555,"y":2340,"width":918,"height":570,"color":"3"},
		{"id":"350412f4fee5a3ef","type":"file","file":"CosineSimilarity.png","x":-1060,"y":4485,"width":400,"height":86},
		{"id":"334ff31ec21a80e3","type":"file","file":"DTW.png","x":-1001,"y":4012,"width":283,"height":400},
		{"id":"066eda0b63762e4f","type":"text","text":"# Performance Evaluation\n\nQuanto funziona bene questo giocattolo?\n\nSulla base di questo modifico la threshold, il peso delle metriche, il FEM, ...","x":1024,"y":4012,"width":767,"height":559,"color":"6"},
		{"id":"eed580368731a96b","type":"text","text":"# Schema Concettuale\n\nL'idea generale di un ***Biometric System*** (***BS***) è semplice, e consta di due semplici step.\n\n- ***Enrollment*** - ***Costruisce un DB*** catturando i dati biometrici grezzi (detti ***sample***, e.g. la foto del volto), estraendone le ***features*** (e.g. la distanza tra gli zigomi, dalla punta del naso al mento, ...) ed associando loro un'identità (e.g. \"Questo è Andrea\"). I ***template*** così ottenuti vengono raccolti nella ***gallery***.\n\t- Ovviamente è una fase preventiva all'azione, ma potrei continuare a raccogliere nuovi dati per il mio DB anche mentre sono in fase di riconoscimento.\n- ***Riconoscimento*** - Prendo il modello con il suo DB, gli fornisco gli opportuni dati grezzi in modo che lui possa estrarne le features (***probe***) e confrontarle con ciò che ha nel DB. Due scenari:\n\t- ***Verifica*** - L'utente dichiara di essere registrato. Il sistema confronta i suoi TB con quelli dell'utente che dice di essere (Controllo 1:1);\n\t\t- FaceID di iPhone è un ***identity claim*** implicito.\n\t- ***Identificazione*** - L'utente non dichiara niente, sta al sistema capire chi è. Per fare ciò deve confrontare i suoi TB con tutti quelli presenti nel DB (Controllo 1:N). A questo punto, anche qui, due scenari:\n\t\t- \"Questo tizio è sicuramente uno di quelli che hai nel DB\". Anche se la similarità è bassa, restituisco comunque il best match tra quelli che ho (***closed-set***), anche se così rischio l'errore;\n\t\t- \"Non è detto che tu sappia chi è\". Se ottengo una similarità bassa potrei scegliere di restituire un messaggio del tipo \"`Reject - Secondo me questo non sta nel DB`\" (***open-set***).\n\nTutto questo avviene sotto l'***assunzione che ogni persona sia unica***. Questo dipende dalle features. Se si usa il DNA è ovvio, ma in generale la fase di recognition non otterrà mai un match al $100\\%$ con la gallery, per motivi\n\n- ambientali, e.g. illuminazione e/o angolazione;\n- fisiologici, e.g. tengo la bocca aperta, rido.\n\nPer questo motivo, il modello deve prevedere una ***tolleranza*** entro cui gestire match multipli.","x":-555,"y":1360,"width":918,"height":718,"color":"4"},
		{"id":"cd2095e4c31cfc60","type":"text","text":"# How To Features\n\nAnzitutto, cosa devo salvare nella gallery? Cioè, con quali criteri scelgo le features (i.e. i dati grezzi da cui poi il modello estrarrà delle buone features)?\n\nDevo scegliere ***TB in grado di identificare univocamente il generico essere umano***, pertanto devono essere\n\n- ***universali*** (i.e. posseduti da tutti, salvo rare eccezioni);\n- ***permanenti*** (i.e. che non possono cambiare nel tempo);\n- ***misurabili*** dai sensori (i.e. quantificabili in numeri);\n- la cui misura è ***accettabile*** dall'utente (i.e. che rispettino la privacy);\n- ***non-eludibili*** (i.e. non facilmente imitabili da artefatti).\n\nSimili TB sono detti ***strong features*** (***SF***), come ***impronte digitali***, ***retina*** e ***orecchie***.\n\nViceversa, un TB come il colore dei capelli è una ***weak feature*** (***WF***), in quanto non universale (impossibile distinguere tra le persone calve), non permanente (posso fare una tinta, o invecchiare, o usare una parrucca) e soprattutto non univoca.\n\nCi sono delle vie di mezzo? Ovviamente sì. I ***tratti comportamentali*** come ***gait*** (andamento della camminata), ***scrittura*** (in particolare la ***dinamica di movimento*** del polso e del braccio) e la ***voce*** sembrano da un lato una SF, ma dall'altro possono cambiare con l'età o con le condizioni fisiche e/o ambientali. Sicuramente sono ***più difficili da riprodurre*** da parte di un attaccante, e danno il meglio di sé quando usate in combinazione con altri TB (***Multimodal Biometrics***).\n\n","x":-1761,"y":3148,"width":760,"height":651,"color":"4"},
		{"id":"8818849b9b5e0e2d","type":"text","text":"# Perché la Modularità? Perché non una NN?\n\nCosa mi impedisce di usare direttamente una NN in classificazione? Niente, se vuoi fallo.\n\nPerò devi rinunciare ad un sacco di cose:\n\n- Non sai quali features sta guardando la NN, quindi non ne hai il controllo;\n- Non è semplice ottimizzare la metrica che preferisci (e.g. FAR, FRR);\n- Non è generalizzabile (in quanto non modulare), cioè ti serve una NN per la verifica, una per l'identificazione, una per l'open-set, ...\n\nIn pratica, lasciare ad un unico monolitico modello di NN l'intero compito del riconoscimento biometrico è in generale una cattiva idea.","x":-1761,"y":1508,"width":760,"height":570,"color":"4"},
		{"id":"f4896017c69ba8cb","type":"text","text":"# Breve nota storica\n\nNel 1882 il grande capo della polizia di Parigi Bertillon decise di iniziare a prendere varie misure biometriche del corpo dei detenuti (e.g. forma della mano, della testa, dettagli facciali).\n\nQuesta roba ebbe un sacco successo, tanto che nel 1896 anche l'FBI iniziò a schedare le persone e nel 1900 queste carte divennero regolamentate dalla legge francese.\n\nQualcuno però storse il naso. Per la privacy? No, perché questa roba era inefficiente. Tale Galton introdusse il concetto di ***minutiae***, ovvero piccolissimi tratti distintivi in grado di identificare in modo univoco una persona. Un esempio? Data una ***fingerprint***, le minutiae sono i punti in cui le linee terminano o si dividono.\n\n","x":-1681,"y":791,"width":600,"height":399,"color":"4"},
		{"id":"ffa28aa2dfac9d98","type":"text","text":"Ancora oggi si fa riferimento alle misurazioni biometriche \"grossolane\" come misure alla bertillon? mi sa che l'ho letto ma non ritrovo dove","x":-2313,"y":930,"width":552,"height":122},
		{"id":"c2ba3fbffc4679d8","type":"text","text":"# Sì, ma in pratica?\n\nData la gallery, faccio ***supervised training*** (le etichette sono dette ***ground truth***) che consiste nel loop\n\n- FEM\n- Matching\n- Decision\n- Evaluation\n\t- sulla base di quest'ultima modifico qualcosa in una delle fasi precedenti (pesi delle metriche, threshold, fine tuning se il FEM è una NN, ...) e ricomincio finché non ottengo risultati soddisfacenti.","x":1142,"y":1707,"width":556,"height":371},
		{"id":"bec8557c6aabbfee","type":"file","file":"SingleAndOverall.png","x":1024,"y":2840,"width":767,"height":187},
		{"id":"4bf5870defbcf5c9","type":"text","text":"# Errori in Verifica\n\nIn verifica, un soggetto viene accettato se la distanza tra la probe e i template associati all'***identità reclamata*** è minore di una certa ***threshold*** (e viceversa con la similarity). 4 casi:\n\n- ***Genuine Acceptance*** (***GA***) - il claim è vero, il sistema accetta;\n- ***False Rejection*** (***FR***) - il claim è vero, ma il sistema rifiuta (***Type-I Error***);\n\t- Gli si associa la FR Rate, ovvero la probabilità che un utente registrato sia rifiutato$$\\text{FRR}(t) = {\\text{\\#Claim di utenti registrati che vengono rifiutati}\\over\\text{\\#Claim di utenti registrati}}$$\n\t- Segue con analoghe definizioni che $\\text{GAR} = 1-\\text{FRR}$.\n- ***False Acceptance*** (***FA***) - il claim è falso, ma il sistema accetta (***Type-II Error***);\n\t- Qui si usa FA Rate, intuitivamente l'opposto della precedente$$\\text{FAR}(t) = {\\text{\\#Claim di impostori che vengono accettati}\\over\\text{\\#Claim di impostori}}$$\n- ***Genuine Rejection*** (***GR***) - il claim è falso, il sistema rifiuta.\n\t- Segue che $\\text{GRR} = 1-\\text{FAR}$.\n\nOvviamente il rischio maggiore è un Type-II, e in generale la valutazione delle performance si basa sul minimizzare gli errori, piuttosto che massimizzare le identificazioni corrette. In questo senso ci sono criteri come $\\text{ZeroFAR}$ (i.e. metto la threshold in modo che non ci sia alcun errore Type-II) e $\\text{ZeroFRR}$, ma anche valutazioni in cui accetto un bilanciamento in cui essi sono uguali ($\\text{EER}$, ***Equal Error Rate***) o trade-off come le curve ***ROC*** e ***DET***.","x":1024,"y":3148,"width":767,"height":651,"color":"4"},
		{"id":"b81e5ec8e65b5242","type":"text","text":"# Ma è una buona feature?\n\nAnche se lo sembra, ci sono alcuni dettagli da tenere in considerazione.\n\n- ***Wide Intra-Class Variations*** - La stessa persona può produrre sample molto diversi. Devo essere bravo a costruire una gallery rappresentativa di tutte le possibili varianti in cui la persona può presentarsi (e.g. bocca aperta, profilo, occhiali);\n- ***Small Inter-Class Variations*** - Persone diverse possono produrre sample simili (e.g. gemelli, padre-figlio, ma anche sosia);\n- ***Noise*** - Questo riguarda più il modello, che dovrebbe essere robusto rispetto al rumore nei dati grezzi (e.g. cicatrici sulle dita, illuminazione non omogenea);\n- ***Non-Universality*** - Anche se scelgo un tratto abbastanza universale, devo sempre tenere in considerazione il fatto che qualcuno può non averlo;\n- ***Spoofing*** - Quanto è robusta rispetto ad chi prova ad impersonare un altro soggetto?\n\t- I tratti comportamentali sono in genere meno imitabili e/o falsificabili;\n\t- Se so come funziona il modello (e.g. prende la distanza tra i punti più luminosi e tra i punti più scuri) posso produrre un'immagine fittizia (***hand-crafted feature***) dalla quale il modello estrae esattamente quello che mi serve per superare il controllo (***white-box attack***, cioè conosco il funzionamento interno).\n\t\t- Questo è un grosso \"se\", perché in genere non so come funziona il modello (i.e. è una ***black-box***). ","x":-2760,"y":3148,"width":760,"height":651,"color":"4"},
		{"id":"16f11eab75b1f055","type":"text","text":"# Errori in Verifica (Matematica)\n\nSia `id(template)` la funzione che restituisce la ***ground truth*** (vera identità) del `template` che gli si passa. Sia $p_j$ la $j$-esima probe e $g_k$ il $k$-esimo template della gallery.$$\\text{FRR}(t) = \\frac{\\left| \\{ p_j : s_{xj} \\leq t, \\ \\text{id}(g_x) = \\text{id}(p_j) \\} \\right|}{\\left| \\{ p_j : \\text{id}(g_x) = \\text{id}(p_j) \\} \\right|}$$È banalmente la traduzione di quello che c'è scritto nel riquadro precedente, dove $s$ è la similarità e $t$ la threshold (identico, con la diseguaglianza al contrario, per la distanza). $g_x$ si intende ottenuto come output di una qualche funzione `TopMatch(p, identity)` che restituisce i template della gallery che *best-matchano* con `p`. Ciò detto il significato è chiaro:\n\n- Numeratore - Numero di probe $p_j$ la cui identità è la stessa del suo `TopMatch` che però avendo ottenuto un punteggio di similarità $s_{xj}$ minore di $t$ sono state rifiutate.\n- Denominatore - Numero di probe $p_j$ la cui identità è la stessa del suo `TopMatch`.\n\nDiscorso analogo per il gemello malvagio Type-II$$\\text{FRR}(t) = \\frac{\\left| \\{ p_j : s_{xj} \\geq t, \\ \\text{id}(g_x) \\neq \\text{id}(p_j) \\} \\right|}{\\left| \\{ p_j : \\text{id}(g_x) \\neq \\text{id}(p_j) \\} \\right|}$$A questo punto, come detto, uno in genere usa la ***ROC***.\n\nIl grafico ROC passa per $(0,0)$ e $(1,1)$: se metto la soglia troppo bassa rifiuto tutto (quindi sia $\\text{FAR}$ che $\\text{GAR}$ sono $0$), e viceversa. La ROC mi dice che ***potenzialmente posso scegliere una threshold lungo la curva***, quindi in generale più l'AUC è grande più sono contento, perché così posso fare scelte migliori. La scelta, tuttavia, è sempre un compromesso.","x":2020,"y":3144,"width":767,"height":660,"color":"4"},
		{"id":"237a1e0bfd57425c","type":"text","text":"in genere si sacrifica genuine acceptance per minimizzare false acceptance.\n\nse il TB cambia (e.g. invecchio) dovrei comunque essere in grado di riconoscere quel TB. fingerprints sono considerate universal and reliable ma possono esserci danni. voce non universale obv, cataratta per retina (o iride?), ...\n\nzero-error attack dichiaro di essere qualcun altro\nspoofing fare qualcosa to mislead recognition system (posso creare falso fingerprint con colle e simili, e seguono tecniche anti-spoofing). posso presentare la foto dell'iride di un'altra persona, o usare lenti a contatto.\n\nDNA univoco, ma se ricevo donazione di midollo potrebbe essere ambiguo.\nretina funziona bene. behavioral traits difficili da riprodurre. walking richiede strategia raffinata in modo molto specifico per ogni persona, difficile da riprodurre a lungo. signature, soprattutto la dinamica del polso che la fa.\n\nhand-crafted features = features... con cui creare il FV (si possono anche estrarre come embeddings, cioè il livello quasi-finale di una NN, anche detto latent space in quanto non interpretabile da un umano ma utilizzabile da una macchina)\n\nminutiae = bifurcation of lines, end of lines, ... (on fingerprint FP)\n\ncityblock (L1 distance) ce ne sono tipo mille, ma distinguo solo distanza e similarità. la covarianza è una misura di similarità, usata per FP. per confrontare gli istogrammi c'è la distanza di Bhattacharyya. in generale, ogni tipologia di gallery (i.e. tipo di template al suo interno) ha la sua misura ottimale. dyamic time warping per dati con accelerometri tipo, nel senso che confronta due funzioni considerando gli stretch e le fasi. in pratica prova a farle corrispondere. più è difficile più sono distanti. insomma, è la distanza da usare tra segnali(t).\n\nFP. in genere problema di orientazione e che spesso si trovano solo frammenti di FP. ci sono modi speciali di fare la distanza (NN).\n\nerrori di tipo I e II non sono ottimizzabili contemporaneamente, in genere serve trade-off. l'analisi degli errori si porta dietro tutti i problemi del training in ML.\n\nricorda che false rejection ha al denominatore il totale dei probes che should be accepted ($\\text{true but rejected}\\over\\text{TbR + True and Accepted}$) . FR e GA sum up to one (i.e. genuine probes). nei casi limite si lascia la scelta all'umano.\n\nil training è sempre supervised.\n\nmore template for same person to address intra-class variations. meglio pochi ma buoni che prendere 200 frame di un video in cui close frames sono simili e la risoluzione è bassa.","x":4880,"y":4292,"width":708,"height":1201},
		{"id":"4f7aebf1deb97d5e","type":"text","text":"# Tecniche di Training","x":3100,"y":4012,"width":739,"height":559,"color":"6"},
		{"id":"08d4569a4c15afaf","type":"text","text":"# Implementazioni\n\n","x":4383,"y":2540,"width":314,"height":170,"color":"6"},
		{"id":"f7c635f63f3a96b7","type":"text","text":"# Retina","x":4209,"y":1898,"width":250,"height":60},
		{"id":"f4cc52883462f784","type":"text","text":"# Face","x":4753,"y":1933,"width":250,"height":60},
		{"id":"c5ac94c35b35d4f6","type":"text","text":"# Voce - Gaussian Mixture Model (GMM)","x":4160,"y":3074,"width":599,"height":140},
		{"id":"fadd10091b275ac7","type":"text","text":"# Fingerprint","x":5234,"y":2320,"width":250,"height":91},
		{"id":"2f94a815f931defe","type":"text","text":"# Iris","x":5160,"y":2904,"width":250,"height":60},
		{"id":"1ec09233624528e6","type":"text","text":"# Signature ","x":3663,"y":2196,"width":257,"height":124},
		{"id":"a3d2e59f3d6b2d43","type":"text","text":"### **Riconoscimento Facciale (Face Recognition)**\n\n- **FEM basati su Deep Learning**:\n    - **FaceNet**: Una delle architetture più popolari per l'estrazione di feature per il riconoscimento facciale. Utilizza una **loss triplet** per imparare una rappresentazione ottimale del volto, in modo che volti simili siano vicini nello spazio delle embedding e volti diversi siano lontani.\n    - **VGG-Face**: Basato sull'architettura VGG, questo modello è stato progettato per estrarre feature facciali rappresentative. Le sue embedding sono molto utilizzate per confronti facciali.\n    - **ArcFace**: Un modello avanzato che utilizza una particolare funzione di loss chiamata **Additive Angular Margin Loss** per migliorare la discriminazione tra diverse classi di volti, generando embedding molto efficaci per il confronto.\n- **FEM tradizionali**:\n    - **Local Binary Patterns (LBP)**: È una tecnica che cattura texture e caratteristiche locali del volto, molto usata nei primi metodi di riconoscimento facciale.\n    - **Histogram of Oriented Gradients (HOG)**: Analizza le direzioni dei gradienti nell'immagine per rappresentare le forme e i contorni del volto.\n\n### 2. **Impronte Digitali (Fingerprint Recognition)**\n\n- **FEM basati su Deep Learning**:\n    - **DeepPrint**: Un sistema che usa reti neurali profonde per estrarre feature dalle impronte digitali. Il modello estrae le caratteristiche importanti come i minuzie (biforcazioni, terminazioni) e genera embedding per il matching.\n- **FEM tradizionali**:\n    - **Minutiae-based Methods**: Sono i metodi più comuni per l'estrazione di feature dalle impronte digitali. Identificano le **minuzie**, come biforcazioni e terminazioni dei solchi, per rappresentare un'impronta in modo univoco.\n    - **Orientation Field**: Questo metodo estrae le direzioni principali dei solchi in diverse regioni dell'impronta, che possono poi essere confrontate tra diverse impronte.\n\n### 3. **Riconoscimento della Voce (Speaker Recognition)**\n\n- **FEM basati su Deep Learning**:\n    \n    - **x-vector**: È un modello di deep learning che estrae vettori (x-vectors) dalle caratteristiche vocali. Questi vettori sono utilizzati per il riconoscimento del parlante, basandosi sul timbro e altre caratteristiche della voce.\n    - **Deep Speaker**: Un altro modello basato su deep learning che si occupa di estrarre feature dalla voce. Il modello apprende a generare embedding vocali che possono essere usate per identificare un parlante.\n- **FEM tradizionali**:\n    \n    - **Mel Frequency Cepstral Coefficients (MFCC)**: Una delle tecniche più comuni per l'estrazione delle caratteristiche vocali. Estrae le **componenti spettrali** della voce che possono essere usate per confrontare i diversi speaker.\n    - **Linear Predictive Coding (LPC)**: Un altro metodo per rappresentare il segnale vocale, modellando come il tratto vocale filtra il segnale sonoro.\n\n### 4. **Riconoscimento dell'Iride (Iris Recognition)**\n\n- **FEM tradizionali**:\n    - **Daugman’s Algorithm**: Un algoritmo molto utilizzato per l'estrazione di feature dall'iride. Cattura l'informazione del pattern di texture dell'iride e la rappresenta con un **codice binario** (iris code).\n    - **Wavelet-based Methods**: Tecniche che utilizzano wavelet per analizzare il pattern di texture dell'iride a varie scale e livelli di risoluzione.\n\n### 5. **Riconoscimento della Camminata (Gait Recognition)**\n\n- **FEM tradizionali**:\n    - **Gait Energy Image (GEI)**: Un metodo che utilizza una serie di immagini silhouette della camminata di una persona per estrarre un'immagine energetica media che rappresenta il modello di camminata.\n    - **Motion History Image (MHI)**: Cattura il movimento attraverso una sequenza temporale di frame e rappresenta la variazione del movimento in una singola immagine.\n- **FEM basati su Deep Learning**:\n    - **GaitNet**: Un'architettura basata su reti neurali che apprende direttamente i modelli di camminata da sequenze video e genera embedding per distinguere tra le persone.\n\n### Conclusione:\n\nOltre alla rete neurale che puoi usare per estrarre feature, ci sono FEM specifici per ogni tipo di input (facce, impronte, voce, ecc.). Questi moduli possono essere sia **deep learning-based** che **metodi più tradizionali**, e la scelta dipende dal tipo di dati e dalle performance richieste per l'applicazione.","x":4920,"y":-15,"width":1356,"height":1567},
		{"id":"4acb6b0dcd714593","type":"text","text":"# Errori open-set\n\nIn questo caso ci sono due domande da porsi: Il soggetto è nel DB? Se sì, chi è?\n\nCom'è ovvio, due domande ci danno $4$ possibili scenari, di cui uno non si può mai verificare (se valuto che il soggetto non è nel DB non posso sapere chi è). Dal punto di vista del modello ci sono solo due casi, a cui seguono i sottocasi legati alle performance:\n\n- ***Alarm*** (`Il soggetto è nel DB`) - Il modello pensa di aver riconosciuto il soggetto.\n\t- Se il soggetto è davvero nel DB `Correct Detect`, dopodiché\n\t\t- se pensava fosse un altro (i.e. ha fatto `Correct Detect` per il motivo sbagliato) abbiamo `Incorrect Identification`;\n\t\t- se ha indovinato anche l'identità `Correct Identification` (nel complesso abbiamo un ***Correct Result***).\n\t- Se il soggetto non è davvero nel DB abbiamo un ***falso allarme***, quindi sia una `Incorrect Detect` che una `Incorrect Identify`.\n- ***No Alarm*** (`Il soggetto non è nel DB`) - Il modello non riconosce il soggetto.\n\t- Se davvero il soggetto non è nel DB, `Correct Result`","x":460,"y":5460,"width":740,"height":580},
		{"id":"f9bb82c22ea2981e","type":"text","text":"# Errori closed-set\n\nDiamo per scontato che il soggetto sia nel DB. Ma chi è?","x":1662,"y":5460,"width":638,"height":580},
		{"id":"437ae081305ab212","type":"text","text":"# Errori in Identificazione\n\n\nTrattiamo in modo diverso i casi di ***open-set*** e ***closed-set***.","x":1153,"y":4900,"width":509,"height":226,"color":"6"}
	],
	"edges":[
		{"id":"fb0a974e652c8a2b","fromNode":"f6da8b1d468646d5","fromSide":"bottom","toNode":"eed580368731a96b","toSide":"top"},
		{"id":"4baf0ebd927b08c9","fromNode":"efcdc18499da9576","fromSide":"bottom","toNode":"f6da8b1d468646d5","toSide":"top"},
		{"id":"0e7dabb490cb55e3","fromNode":"f6da8b1d468646d5","fromSide":"left","toNode":"f4896017c69ba8cb","toSide":"right"},
		{"id":"b400fc2395efec78","fromNode":"eed580368731a96b","fromSide":"bottom","toNode":"c3c219f1c9689029","toSide":"top"},
		{"id":"ebc0862f7de85daa","fromNode":"066eda0b63762e4f","fromSide":"top","toNode":"4bf5870defbcf5c9","toSide":"bottom"},
		{"id":"ed69b891c135d5d4","fromNode":"066eda0b63762e4f","fromSide":"bottom","toNode":"437ae081305ab212","toSide":"top"},
		{"id":"4c9a045aaf3a43ca","fromNode":"4c0ac5a15b1f23ad","fromSide":"bottom","toNode":"bcc60cdc04b5895d","toSide":"top"},
		{"id":"5a39fbc5b7553d67","fromNode":"bcc60cdc04b5895d","fromSide":"bottom","toNode":"f1175d1af9fe2990","toSide":"top"},
		{"id":"91c5a703e4e4f09d","fromNode":"f1175d1af9fe2990","fromSide":"right","toNode":"066eda0b63762e4f","toSide":"left","fromEnd":"arrow"},
		{"id":"fd6d873061044eca","fromNode":"bcc60cdc04b5895d","fromSide":"right","toNode":"066eda0b63762e4f","toSide":"left","fromEnd":"arrow"},
		{"id":"51a76407d854014f","fromNode":"c3c219f1c9689029","fromSide":"bottom","toNode":"4c0ac5a15b1f23ad","toSide":"top"},
		{"id":"f8dc485d09a4f976","fromNode":"4c0ac5a15b1f23ad","fromSide":"left","toNode":"cd2095e4c31cfc60","toSide":"right"},
		{"id":"5fba9db0c0f76a6a","fromNode":"4c0ac5a15b1f23ad","fromSide":"right","toNode":"066eda0b63762e4f","toSide":"left","fromEnd":"arrow"},
		{"id":"0c2f1bd56dd21a9d","fromNode":"c3c219f1c9689029","fromSide":"right","toNode":"08d4569a4c15afaf","toSide":"left"},
		{"id":"b17ec63688d0500f","fromNode":"cd2095e4c31cfc60","fromSide":"left","toNode":"b81e5ec8e65b5242","toSide":"right"},
		{"id":"00cb4b1c0b9631a7","fromNode":"c3c219f1c9689029","fromSide":"left","toNode":"8818849b9b5e0e2d","toSide":"right"},
		{"id":"f2f94e8cb219c5ae","fromNode":"eed580368731a96b","fromSide":"left","toNode":"8818849b9b5e0e2d","toSide":"right"},
		{"id":"cbdeab744f155415","fromNode":"bcc60cdc04b5895d","fromSide":"left","toNode":"334ff31ec21a80e3","toSide":"right"},
		{"id":"3dd75ff78f2450e2","fromNode":"bcc60cdc04b5895d","fromSide":"left","toNode":"350412f4fee5a3ef","toSide":"right"},
		{"id":"0e20e45b5c51fcea","fromNode":"c3c219f1c9689029","fromSide":"right","toNode":"c2ba3fbffc4679d8","toSide":"left"},
		{"id":"9317642108ae0651","fromNode":"eed580368731a96b","fromSide":"right","toNode":"c2ba3fbffc4679d8","toSide":"left"},
		{"id":"8e4f0e365747437d","fromNode":"4bf5870defbcf5c9","fromSide":"right","toNode":"16f11eab75b1f055","toSide":"left"},
		{"id":"976c9ae939d13916","fromNode":"4bf5870defbcf5c9","fromSide":"top","toNode":"bec8557c6aabbfee","toSide":"bottom"},
		{"id":"b9c42001295177b3","fromNode":"066eda0b63762e4f","fromSide":"right","toNode":"4f7aebf1deb97d5e","toSide":"left"},
		{"id":"5e7d73c793f6dc68","fromNode":"16f11eab75b1f055","fromSide":"right","toNode":"4f7aebf1deb97d5e","toSide":"top"},
		{"id":"e893d87074d91d10","fromNode":"437ae081305ab212","fromSide":"bottom","toNode":"4acb6b0dcd714593","toSide":"top"},
		{"id":"b1ecb18eb75f723f","fromNode":"437ae081305ab212","fromSide":"bottom","toNode":"f9bb82c22ea2981e","toSide":"top"}
	]
}