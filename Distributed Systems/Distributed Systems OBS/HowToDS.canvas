{
	"nodes":[
		{"id":"f82b77ddb75926d4","type":"text","text":"# Distributed Systems\n\nUn DS è un sistema in cui $n$ processi cooperano per completare una task.\n\nUn processo è definito da una ***timeline*** e dagli ***eventi*** (e.g. creazione di un file, modifica) che avvengono su di essa. Quando si verifica un evento sul processo $i$, questo può informare tramite un ***messaggio*** il processo $j$.\n\nLo scambio di messaggi avviene tramite canali FIFO, assumendo che questi siano ***asincroni*** (i.e non esiste un tempo limite entro il quale sono certo che il messaggio arriverà) e ***unreliable*** (i.e. i messaggi si possono perdere).","x":295,"y":-303,"width":640,"height":303,"color":"6"},
		{"id":"eead1953cd38ffd8","type":"text","text":"# L1 (Lamport Clock)\n\nMettiamo processi da 0 a 3. Ogni volta che su un processo da 1 a 3 avviene un e, questo lo notifica a $p_0$, il quale può ricostruire la run (è un osservatore). visto che non so quanto ci mettono i singoli messaggi ad arrivare a p_o, potrei avere delle inconsistenze! Non solo potrebbe non essere consistente, ma potrebbe non essere nemmeno una run!! come risolvo? Con un canale FIFO! weak assumption, very easy to implement. ma questo canale è per ogni coppia $(i;0)$ non mi dice niente sull'ordine dei diversi processi. Quindi così è una run, ma non necessariamente consistente. what if i give you a global clock (aka Real Clock RC), assume every process can use it. very strong assumption, cause no god gives us a clock, but i can use it to build a consistent run.\n\ndiciamo che $\\delta$ è un upper bound per il tempo impiegato da ogni canale per deliverare il messaggio a p_0. Ogni messaggio ha un timestamp, e p_0 ha una finestra di osservazione larga $\\delta$. entro questa finestra ha un buffer in cui aspetta di essere sicuro di ricevere tutti i messaggi di quella finestra.\n\n\n$e\\to e' \\Rightarrow RC(e)<RC(e')$. **Clock condition**. Non serve RC, basta qualsiasi clock con questa proprietà. LA freccia non è al contrario! $e\\to e'$ implica una necessità di ordine, che non è necessaria per **eventi concorrenti** (segnati con una freccia sul grafico, i singoli punti sono eventi indipendenti)\n\nDefiniamo un sapienza clock SC local to every p. se $e_1^1\\to e_2^1$ e il primo ha timestamp 1, il secondo deve avere timestamp maggiore, quindi sarà 2. dopodiché procedo con gli eventi in ordine, finché 7 sulla linea 2 non manda un messaggio alla linea 1. a qualsiasi numero sono arrivato sulla linea 1, ricevere un messaggio da 2 deve avere timestamp maggiore sia della linea 2 che di quella 1.\n\n- se 1 era arrivata a 5, il massimo +1 è 8;\n- se era a 12, è 13.\n\nQuesta roba è il **Lamport (or logical) Clock**\n\nora, sul singolo canale FIFO sono sicuro di avere i timestamp in ordine. ma prima di aggiungere $e_2^3$ devo aspettare tutti gli eventi precedenti? Non proprio, solo quelli che precedono logicamente. vedi grafico. e come entra $\\delta$ in tutto questo? se ricevo $e_2^3$ con timestamp 4, allora devo aspettare tutti gli eventi degli altri processi con timestamp fino a 4. this might make me wait. e se p_3 non ha nessun evento per un bel po'? ...boh si è dimenticato?...\n\n","x":-840,"y":-7209,"width":640,"height":1180},
		{"id":"941a38f0264b5e2e","type":"text","text":"# L2 (Lamport vettoriale - vector clock)\n\nse ci sono eventi concorrenti (quindi non c'è un ordine tra loro). i loro timestamp non possono essere numeri. allora posso modificare un po' la definizione dei timestamp, e renderli non numeri ma con la loro history. in questo modo$$e\\to e' \\Leftrightarrow TS(e)\\subseteq TS(e')$$\nin questo modo due eventi senza freccia sono semplicemente due eventi le cui storie non sono una un sottoinsieme dell'altra. ma la history di tutta la run! è un vettore. ad esempio, la timestamp di $e_2^3$ è $[1,4,2]$, ovvero il vettore dei lamport clock di ogni processo, così non pesa 5 terabyte dopo un'ora.\n\nvedi secondo grafico.\n\np_0 riceve $[141]$, quindi sa che deve ricevere prima una notifica da 1, 3 da 2 (ma di questo sono sicuro, perché il singolo canale è FIFO) e 1 da 3. nota che così non serve $\\delta$.\n\nquindi la condizione di correlazione $e_i\\to e_j$ è $$\\forall\\,k\\, VC(e_i)[k] \\leq VC(e_j)[k]\\quad\\wedge\\quad\\exists\\,k': VC(e_i)[k'] < VC(e_j)[k']$$se non metto la seconda condizione potrebbero essere lo stesso evento. ovviamente se so che i due eventi appartengono a processi differenti non serve la seconda condizione, anzi, basta confrontare il clock associato ad un singolo processo $k$ (e.g. \\[310\\] sul processo 1 avviene prima di \\[240\\] sul processo 2. controllando NON SONO CONVINTO DI QUESTA ROBA)\n\nvector clock usato nei DB distribuiti. così vediamo anche i deadlock (how?)\n\nreachable = run at some point is equal to the cut","x":-100,"y":-7079,"width":620,"height":923},
		{"id":"dd46627802841779","type":"text","text":"# L3\n\nesempio foto\n\ndefiniamo $|\\Theta(e_i)|=\\sum_k VC(e_i)[k]$, ovvero la \"misura\" del vector clock.\n\nvoglio sapere se esiste $e_k$ t.c. $e_k$ NON è avvenuto prima di $e_i$, ma è avvenuto prima di $e_j$. In pratica voglio sapere se esiste un evento $e_k$ tra $e_i$ e $e_j$ (non letteralmente nel diagramma spaziotempo, basta che sia vero nella run). A livello di VC lo traduco come? $VC(e_i)[k]$ è il numero di eventi del processo $k$ che sono avvenuti prima dell'evento $e_i$, quindi$$VC(e_i)[k] < VC(e_j)[k]$$\n\nora, inizio a registrare. Se mi arriva $[001]$ posso registrarlo? Sì, perché sono sicuro che non può arrivare un messaggio che sia avvenuto prima di questo. e se poi arriva $[232]$? Ovviamente non posso registrarlo. Cosa discrimina tra i due casi, intuizione a parte? posso deliverare solo se una delle componenti dell'incoming message è esattamente +1 rispetto a quello che ho salvato (nel qual caso la aggiorno). riguardati come funziona il VC, funziona. Nota che è facilitato dal fatto che il singolo canale è FIFO, ma non è strettamente necessario che lo sia.\n\n\n\n- Se è $p_0$ che requesta i local states, non è detto che riesca a ricostruire un global state coerente (i messaggi arrivano ai processi in momenti diversi)\n- potremmo fare che quando il processo $p_i$ riceve la request fa broadcast su tutti gli altri processi. se un processo riceve lo snapshot del local state da un altro processo, fa partire il suo broadcast senza aspettare la notifica di $p_0$.\n- il Cut sui punti in cui i processi fanno broadcast è consistente. questo perché i canali sono FIFO, quindi una qualsiasi freccia che implica una relazione $e_i\\to\\ e_j$ avviene prima della comunicazione broadcast. vabbè lo dimostra per contraddizione con la definizione.\n- sta roba si chiama protocollo chandy-lamport\n- se fai così non ci stanno deadlock (continua ad accennarlo senza entrare nel dettaglio)\n\nTutta questa era la fase 1, poi passeremo agli atomic commit (blockchain e cose varie)","x":1272,"y":-7080,"width":780,"height":923},
		{"id":"120f425563a323e0","type":"text","text":"run = ogni possibile esecuzione degli eventi in modo che l'ordine del singolo processo sia rispettato\n\ntopological order = consistent run?\n\ncioè tipo che su un grafo in generale non c'è topological order perché ci possono essere cicli (qui proviamo che non possono esserci cicli perché sono diagrammi space-time)","x":648,"y":-6814,"width":328,"height":393},
		{"id":"b2214078d2cc0c48","type":"text","text":"# L4 (Two Phase Commits)\n\nCINECA?\nnello schemino in cui (nome, voto) viene splittato in (nome) e (voto) su 3 computer diversi serve ovviamente che ci siano protocolli locali tipo login e rollback localmente su ognuno di essi, ma è un sistema distribuito in cui le operazioni {splitta nomevoto, salva nome, salva voto, cancella nomevoto} deve essere atomico, cioè o tutto o niente. Se non implemento nessun protocollo per assicurarmi di ciò, rischio un'inconsistenza (e.g. il nome viene perso, posso votare di nuovo).\n\nquesta roba si realizza con two phase commit: quando parte la transazione, o tutti i siti la fanno o non la fa nessuno.\n\nassumiamo che i processi siano either crash fail, crash stop or crash recovery. sistema asincrono, no byzantine nodes (?).\n\n- A1 - if processes reach a decision (either commit or abort) it must be the same for all of them;\n\t- versione alternativa scritta sul libro: all processes that reach a decision, reach the same one\n- A2 - A process cannot reverse its decision after reaching one;\n- A3 - The commit decision can be reached only if all processes voted `yes`.\n\t- non dico iff perché voglio lasciarmi aperta la pista di abortire anche se tutti hanno votato `yes`.\n\nnota che anche un protocollo che non fa letteralmente nulla soddisfa queste proprietà, quindi mi servono condizioni aggiuntive.\n\n- safety propriety - nothing bad should happen\n- liveness property - \"ok, but do something\"\n\nse hai solo safety è ok se non fai nulla (e.g. A1 2 3), se sei pazzo sei liveness. tipicamente richiediamo di essere safe ma essere as live as you can.\n\n- A4 - ***If there are no failures*** and all processes voted `yes` then decision is `commit`.\n","x":-11040,"y":8160,"width":880,"height":761},
		{"id":"c60727cb76e10ee4","type":"text","text":"\nin un processo del genere solitamente c'è un coordinatore e dei partecipanti. questi ultimi votano, il primo no (oppure sì, not a problem if coordinator is also participant). quindi: sistema distribuito è così\n\n- coordinatore C manda `voteRequest` ai partecipanti P\n- i quali decidono `yes` or `no`\n\t- se votano `no` fanno `abort`, e ciò è safe.\n- e mandano il voto a C\n\t- e se il messaggio si perde o è molto lento? C deve essere sicuro di avere tutti i voti `yes` per, in caso, fare commit. Quindi è safe.\n- se C riceve TUTTI `yes`, allora può decidere di fare commit. sta di fatto che qualcosa decide\n- allora manda la decisione a tutti, i quali eseguono.\n\nse il protocollo non ha failures e sincrono (i.e. i messaggi arrivano in tempi utili) è perfetto. In una situazione reale non è così.\n\nma posso mai aspettare per sempre? ovviamente no, metto un timeout al termine del quale mando a tutti una decisione di abort.\n# Failures\n\n- se si perde `voteRequest`?\n\t- la prima volta al partecipante scade il timeout e chiede a C di mandare `voteRequest`\n\t- la seconda fa un `abort`.\n- se si perde il voto?\n\t- C manda di nuovo la request\n\t- la seconda volta che scade il timeout decido di abortire.\n\t\t- eh ma magari arriva un attimo dopo ed erano tutti `yes`. \"eh, such is life\"\n- se si perde la decisione che C manda a P?\n\t- se ho votato no, easy, tanto ho già abortito\n\t- se ho votato yes non è per niente safe abortire, quindi mi tocca aspettare. magari gli rimando un \"`aoo sta decisione??`\" sperando che arrivi. e magari lo chiedo tante volte e nessuno mi risponde. eh. magari C è morto. allora magari chiedo agli altri P se hanno ricevuto la decisione (assumo che non mentano). gli altri P possono rispondere in due modi\n\t\t- ho ricevuto la decisione, fine\n\t\t- non l'ho ricevuta, e\n\t\t\t- ho votato no - P fa un `abort`\n\t\t\t- ho votato `yes`. cazzo. chiedo a un altro. se chiedo a tutti e TUTTI hanno votato `yes` e NESSUNO ha ricevuto la decisione, tutti fanno `abort`.\n\nin sostanza, two-phase commit è safe ma non live. scopriremo che nessun protocollo è sia safe che live. c'è un teorema di impossibilità di avere entrambe. per questo chiediamo protocolli safe e as live as can be. è anche un cooperative protocol\n","x":-11040,"y":8990,"width":880,"height":1180},
		{"id":"be570a1920f04449","type":"text","text":"\n# Log\n\ntutto questo ha senso se i processi sono fail-stop. e se sono fail-recover? quando recoverano perdono memoria! Tranne? Il ***LOG*** (sulla singola macchina. ovviamente puoi farlo sul DS ma servirebbe un TPC per il log condiviso, il che significa provare a risolvere il problema con il problema).\n\n\"eh ma il log sta su disco, può fallire\". Ho capito, ma mica puoi risolvere tutto. magari fai più dischi ma muoiono tutti. non solo. quando chiamo una syscall per scrivere su disco, magari questa scrive in RAM. allora devo forzare a scrivere su disco (tipo con una `flush`, su linux si chiama `fsync`?). ma magari il disco ha un buffer e salta la corrente, perdo il buffer. si usa un disco solo per il log e lo configuro in modo che non usi il buffer. (ha fatto un commento su journaled filesystem? non ho sentito!).\n\nil punto di tutto sto pippone è che mi serve assumere che il log sia safe in modo che quando recovero leggo il log.\n\ndetto questo. è meglio scrivere sul log e poi mandare il messaggio o viceversa? in entrambi i casi potrei morire nel mezzo! ma è meglio la seconda scelta. se rivivo e leggo sul log che ho mandato il messaggio non lo manderò mai. meglio mandare e loggare, alla peggio mando messaggi duplicati. però puoi ricevere voti di una request che non sai di aver mandato. tu nel dubbio vedi sta roba e dici `nono che è sta roba, abortite tutti`. mi sa che posso scegliere entrambi a seconda delle necessità\n\nstesso problema per P. mando il voto e loggo o viceversa?\n- voto e loggo, morendo in mezzo - quando rivivo non so che ho votato. né posso rimandare il voto, perché potrebbe essere diverso. \n- viceversa - quando rivivo penso di aver mandato il messaggio, ma non posso essere sicuro, quindi nel dubbio lo rimando.\nquindi se ho votato no abort, altrimenti aspetto.\n\ncapiamo che in pratica se voto no non cambia niente, se voto `yes` è meglio log e poi send, alla peggio mando lo stesso messaggio due volte perché so qual è.\n\nStesso problema per C quando manda la decisione. prima safety, quindi\n- log then send is ok, if you die in the middle you see the decision, not sure if has been sent, send it again\n- send then log - safe only if it's an abort.\n\nse quando vedi la decisione presa la rimandi, se non vedi nulla mandi un abort.\n\nyou should be very careful what you log and when.","x":-10983,"y":10296,"width":753,"height":1804},
		{"id":"23a2f6367ee648bd","type":"text","text":"è anche possibile che C faili mentre manda i messaggi, tipo che lo dice a qualcuno e poi crasha. legge il log e rimanda a tutti.","x":-10158,"y":11139,"width":345,"height":118},
		{"id":"604f69980144ca26","type":"text","text":"# Paxos\n\nÈ generale come Two-Phase Commit, ma ha più fault tolerance e più liveness.\n\nProtocollo maggioritario in cui dividiamo i nodi in tre categorie:\n\n- Proposers (`P`) - non una buona idea averne uno solo, se muore il protocollo è morto. non serve una maggioranza però, quindi ne basta uno vivo.\n- Acceptors (`A`) - Sono i nodi più importanti, perché decidono a maggioranza su quale valore deve fare `commit` l'intero sistema;\n\t- Se ci sono $n$ `A`, la tolleranza del protocollo è il fail di $f=\\text{floor}({n-1\\over2})$;\n\t\t- se ci sono più di $f$ failures, il protocollo si ferma (so it's safe)\n\t- In genere $n$ è dispari, perché la tolleranza è la stessa per $n-1$ e a sto punto ne metto uno in più.\n\nin molte applicazioni pratiche, ogni nodo è sia `A` che `P` che `L`.\n\na inizio protocollo vengono distribuiti staticamente tra i `P` un infinito numero di round in modo che nessuno abbia lo stesso.\n\nprimo messaggio: `prepare(1)`. scelgo il round in modo che sia maggiore dei precedenti, se sono anche `A` e so a cosa siamo arrivati sono facilitato. In generale provo a massimizzare la liveness, perché il protocollo in sé già mi garantisce safeness.\n\ngli `A` raccolgono le prepare, votano e mandano le `promise`.\n\ni `P` raccolgono le promise e decidono quale valore deve essere accettato.\n- se si raggiunge il ***quorum*** $Q\\geq n-f$ (i.e. un numero $Q$ di di `A` ha votato lo stesso valore `x`) allora mandano una `accept(i, x)` agli `A`;\n\nQuando gli `A` ricevono una `accept` mandano una `learn` ai `L`, che in pratica è il voto.\n\n- se i `L` vedono un numero $\\geq Q$ di `learn` con lo stesso valore e round, allora fanno `commit` e il valore diventa chosen\n\nl'obiettivo di paxos è che sia impossibile scegliere due valori diversi.\n\nnon è live manco senza failures e con messaggi sincroni. in pratica lo è a meno di non essere unlucky.","x":-9280,"y":10296,"width":753,"height":1030},
		{"id":"0351c84f97899850","type":"text","text":"# L5 (Paxos)\n\nconsesus problema dell'agreement (in questo caso commit o abort). stessa cosa risolta nella blockchain (devi sapere se è accettata o no). Two-phase commit è il primo step verso ***paxos*** (perché basta un solo nodo offline e tutto muore). paxos è il nome di una possibile isola greca (in realtà esiste, ma lamport non lo sapeva) perché racconta sta storia di sti tizi greci con un piccolo parlamento di non professionisti. lo manda così e glielo rifiutano. alla fine dopo lunghe lotte lo accettano ma aggiungono delle note esplicative (le fa Keith Marzullo). nessuno lo capisce, allora pubblica un articolo chiamato \"paxos made easy\". nessuno lo capisce. poi ci sta \"paxos for dummies\".\n\nogni nodo ha un DB. in google filesystem every file is replicated in 5 different places. replication system, paxos is a replication system. se tutto viene fatto in parallelo tra 5 server, ognuno di essi deve avere una copia coerente. sui google doc possono lavorare più persone! quindi serve consenso sull'ordine delle singole operazioni.\n\n- serve consenso non sulla serie di valori, ma sul singolo valore. dobbiamo tollerare i crash failures. sistema asincrono con n processi.\n\nci sono n processi più importanti, detti acceptors. nella storia di lamport sono i membri del parlamento\n\npoi ci sono i proposers. \n\nha senso chiedere che tutti gli acceptor debbano votare su ogni singolo valore perché questo sia accettato, ma così torno al two-phase con i relativi problemi. allora facciamo che serve la maggioranza (perché se così non fosse due proposte opposte possono essere approvate contemporaneamente). se serve maggioranza e ci sono due proposte parallele almeno un computer sarà in entrambe le commissioni, e permetterà coerenza. tolerates $floor({n-1\\over2})$ errors?\n\n- primo messaggio - un proposer manda la proposta con un messaggio di `prepare`. perché non è una `voteRequest`? perché che ne so se mi sono appena svegliato e gli altri proposer hanno proposto altro?\n\t- visto che non abbiamo un orologio usiamo il round. ad ogni processo è preassociato un set di numeri che sono i round in cui può iniziare la conversazione. sono statici e mai condivisi (e.g. se il processo 1 ha il round 1, nessun altro ha il round 1).\n\t- a questo punto capiamo che la `prepare` è una `prepare(3)`, cioè `preparati al round 3`\n- l'acceptor risponde con una `promise(3, lastround in cui ho votato, valore per cui ho votato)` in cui comunicano al processo che ha mandato la `prepare`\n\t- cosa hanno votato l'ultima volta\n\t- quello in mezzo è il numero dell'ultimo round per cui ho votato. posso rispondere `null null` se non ho ancora votato\n\t- perché si chiama promise? perché significa `from now on i will never partecipate to a round smaller than 3`\n- Se il proposer riceve n messaggi in risposta, ha un quadro chiaro della situazione. se la maggioranza risponde di aver votato $5$, è inutile che io proponga $7$, tanto non vincerà mai! (Paxos è per il singolo valore!!).\n\t- anche se volevo proporre 7, propongo 5, o in generale x. (?)\n- mando una `accept(3, x)`, cioè al round 3 dico (agli acceptors? o al proposer? A che mi serve sto passaggio??) che ho accettato x\n- gli acceptors mandano un `learn(3, x)` a tutti gli altri (che di base è il loro ***voto***). Se un learner riceve una maggioranza di voti per quel round (altrimenti no?)\n\t- se sono all'inizio e nessuno ha votato parto con una `accept`.\n\nse dopo il round 3 parte il round 1, gli acceptors non rispondono. allora magari il processo prova con il prossimo numero nel suo array statico, che magari è 4 (se sono 3 processi)\n\nse al round 2 c'era la maggioranza (3/5) ma uno degli acceptors muore, arrivano solo 4 promises, magari 2 voti e due nulli. che faccio? non blocco il protocollo, ma non accetto il nuovo valore.\n\nnon è importante che il processo faccia approvare il proprio valore, ma che tutti concordino.\n\nsta dicendo una cosa circa il fatto che non è possibile cambiare il voto una volta che una maggioranza ha preso una decisione\n\nè possibile che nel round 2 qualcuno voti x ma poi nel round 3 la maggioranza vota y. se la minoranza ha votato x ma al round dopo le loro promise si perdono, il nuovo proposer riceve solo dei \"non ho votato\". è totalmente safe fare accept y.\n\nse le accept ci mettono troppo, il prossimo round comincia. gli acceptor promettono di non partecipare a round inferiori. quando le accept arrivano, vengono rifiutate.\n\nnon è live, ma tanto è impossibile esserlo essendo safe. in pratica lo è, nel senso che quando lo implementi i segnali di norma non rallentano tutti insieme, o simili.\n\nnon tollera malicious activities, quindi non usato nelle blockchain.\n\nfino a qualche centinaio di acceptor paxos va bene, ma blockchain ne ha migliaia.","x":-8516,"y":9911,"width":850,"height":1800},
		{"id":"0084479ff459b8fb","type":"text","text":"# L5 (Paxos II)\n\n`THM` - If acceptors vote for `x` in round $i$, then no value $x'\\neq x$ can be chosen in previous rounds.\n\n`Proof` - Se ho votato (i.e. mandato una `learn`) significa che ho ricevuto un `accept`. Se esiste una `accept` significa che è stato raggiunto $Q$. Procediamo per induzione sul round $i$.\n- Base induttiva (`round 0`) - Il teorema è ovvio;\n- Assumiamo vero per $i-1$ e dimostriamo per $i$ - $Q\\subseteq A$. Diciamo che i facenti parte del $Q$ abbiano votato l'ultima volta al più al round $j$. Segue che nessuno dei $Q$ ha votato tra $j+1$ e $i-1$. Se $j$ valesse $-1$ avremmo finito la dimostrazione. Diciamo che al round $j$ all'interno di $Q$ almeno 1 deve aver votato per $x$ (per definizione è il valore associato al maxround). potrebbe essere l'unico? Sì, perché tutti gli altri accept potrebbero essere stati persi. Ma se almeno uno ha votato x significa che un proposer gli ha detto così. quindi nessun altro `A` può aver scelto $y\\neq x$. Ma posso usare l'ipotesi induttiva, quindi nei round $<j$ nessun valore $x'\\neq x$ può essere stato scelto. La chiede all'esame :)\n\nSe tutte le learn si perdono al round i, dipende\n- se c'era Q, quel valore prima o poi verrà approvato (segue da `THM`)\n- altrimenti \n\nin ogni caso mi serve un timeout sul learner, in modo che dicano ai `P` \"inizia un nuovo round\".\n\nse modifico il protocollo in modo che $A_i$ non cambi mai il suo voto, si rompe tutto perché potrebbe non esiste mai più una maggioranza...\n\nha senso dividere i proposer dal resto nel paradigma client-server.\n\ntutta sta roba non funziona se un qualsiasi nodo è byzantine\n\nOra, in genere vogliamo approvare una sequenza di valori (non uno solo). Allora runno diverse instance. di fila? Ma no, viva le pipeline.\n\n- la `prepare(i)` diventa una `prepare(p, i)`, in cui `p` è il numero dell'istanza.\n\t- posso ottimizzare mandando una singola prepare che dice `prepare(1-1000, 1)` ovvero preparati al primo round di 1000 istanze;\n\t- stessa cosa per la prima `promise`, che dice di essere pronto per le istanze `1-1000`, round `1`;\n\t\t- possiamo chiamarle `super-prepare` e `super-promise`\n\t- e le `accept`? Il sistema è solo pronto alle istanze, non le ha ancora ricevute. Quando `P` riceve $T_1$, cioè il primo valore da approvare (tipo dal client) è sufficiente mandare una `accept(1, 1, T1)` ovvero `accept(istanza, round, value)`. Se ne ricevo un'altra posso mandare direttamente `accept(2, 1, T2)`? Sì, perché l'importante è l'univocità della coppia `istanza, round`.\n\t- finora abbiamo assunto che i `P` non fossero in competizione. se c'è un altro `P` a ricevere delle T che faccio?\n\t\t- il secondo proposer si prenota le istanze `1001-2000`. sta roba non è ottimale, perché magari approvo le istanze `1, 2, 1001` ma da qui in poi non posso fare commit perché devo aspettare tutte le istanze tra `2` e `1001`.\n\t\t- altra opzione è che tutti mandino le T al primo che ha fatto le `1000` richieste, che Lamport chiamava ***coordinator***.\n\t\t\t- e se muore? se ne fa un altro , \"come il papa\"\n\t\t\t- questo è il cosiddetto leader problem, che è difficile come il consenso. la soluzione è che qui non deve funzionare per davvero, perché alla peggio Paxos funziona ma non ottimizzato\n\t\t\t- se per magia esistesse un perfetto protocollo di scelta del leader avremmo un solo proposer. in quel caso paxos diventa live (l'unico problema in questo senso era infatti che ci fossero tanti P)","x":-7608,"y":10071,"width":800,"height":1552},
		{"id":"4d644fa5db15f42d","type":"text","text":"# Fast Paxos Safety Theorem\n\nIn FP, if acceptor a votes\\* for x in round j, then no $x'\\neq x$ can be chosen from previous rounds.\n\nEsattamente come Paxos, per induzione.\n\nin round i qualcuno ha votato xm e c'è un quorum $Q_i\\geq$. j è il maxround.\n\nif $j>0$:\n- if $\\exists\\, x:$ ha abbastanza voti in Q da poter raggiungere una maggioranza contando i voti fuori da Q (che non conosco)\n\t- cioè x compare come uno dei voti ed ha questa proprietà, in questo caso è ovvio\n- ***or*** $x$ è l'unico valore che compare nelle promise:\n\t- se c'è solo x, è ovvio che nessun altro può avere la maggioranza, uso l'ipotesi\n- then choose $x$\n- else:\n\t- choose any value (che comunque può essere l'$x$ del teorema. se sono in questo caso significa che questo x non ha nessuna delle proprietà precedenti)\n\ne se ci sono due valori y e z? il mi sta dicendo che nessun valore diverso da y e da z è stato votato in precedenza. Si escludono a vicenda, quindi nessun valore può essere stato votato in precedenza. mando una accept con y, z oppure il mio valore. qualunque cosa io scelga è comunque safe, e diventa la x del teorema (è ovvio quindi che se scelgo x in questo modo è perché nessuno aveva una maggioranza, i.e. nessun x' era stato votato prima di questo round j)","x":-5632,"y":10790,"width":580,"height":817},
		{"id":"1955f0b3fa813eab","type":"text","text":"# Fast-Paxos\n\ne se una volta che gli acceptors  sono pronti alle prime 1000 istanze ogni volta che arriva un valore ad un qualsiasi proposer questo manda direttamente una accept senza passare per il coordinator?\n\npotenzialmente più P fanno questa cosa in parallelo, ed entrambi mandano `accept(1, 1, X)` dove `X` è diverso a seconda del P. Questo succede perché tutte le prepare sono state mandate per il round 1! Potrebbe succedere che su 5 acceptors\n- due voti si perdono, non so il voto\n- uno ha votato x1\n- uno x2\n- uno non ha votato\nora, io proposer che vedo queste promise quale valore scelgo? non posso mai essere sicuro di un valore safe, perché se scelgo x1 magari tutti gli altri hanno votato x2 e rompo tutto (e viceversa)\n\nuna soluzione è aumentare il quorum a 2/3 anziché 50%+1. Mettiamo che gli A sono così divisi\n\n- promise che non arrivano sono 1/3\n- tra le promise che arrivano\n\t- 1/3 vota x1 = A\n\t- 1/3 vota x2 = B\n\ntolleranza di fast-paxos = $f'=floor({n-1\\over3})$ e il quorum è $n-f'=Q$. Grazie alla funzione floor è safe la scelta di qualsiasi $X_i$ tale che nessun altro $x_j$ di cui mi arriva una promise con il voto non possa mai avere la maggioranza. Il caso peggiore è appunto 1/3 e 1/3, e chiaramente scelgo un numero di acceptors (divisibile per 3) + 1 (e.g. 7). in questo modo almeno uno tra A e B è strettamente minore di 1/3, quindi posso con certezza scegliere un valore safe.\n\ndevo dire al sistema che sto facendo un fast round. il protocollo è infatti\n\n- prepare(1-n, 1)\n- promise(1-n, 1, x, y) (cosa sono x e y??)\n- accept_any(1-n, 1)\n\t- questo prepara per l'accept proveniente da più proposer. bisogna farlo solo quando è safe accettare davvero qualsiasi valore, tipo all'inizio quando nessuno ha votato.\n\nse nessuno ha una maggioranza nel quorum (i.e. se A=7 e Q=5, se nessun valore ha almeno 3 voti) è safe scegliere un qualsiasi valore, quindi il proposer potrebbe anche far partire una `accept_any` per far partire un altro fast-round in cui è possibile proporre qualsiasi valore? Ha senso? non molto! ha senso proporre un valore che ha una maggioranza relativa. Può avere senso che lo si faccia se vedi 5 voti diversi, altrimenti si procede con paxos normale.\n\nin generale fast paxos è più lento di paxos. wow. quand è che invece performa bene?\n\n- non ci sono molti proposer\n- poche transazioni\n- in generale, pochi conflitti all'interno del quourm.\n\t- server molto lontani tra loro non sanno lo stato l'uno degli altri, quindi è più probabile avere conflitti. ha senso in small cluster è quando c'è poco load.\n\nin pratica non è molto usato: riduce la tolleranza per un boost in velocità che spesso con i conflitti neanche esiste (i sistemi reali sono spesso in overload)\n\nspesso inoltre non c'è davvero bisogno di aspettare l'ordine giusto per il commit","x":-6808,"y":10170,"width":797,"height":1461},
		{"id":"0f2b4bbb443afbb1","type":"text","text":"# Esonero 4 novembre","x":-797,"y":1678,"width":357,"height":245,"color":"1"},
		{"id":"48ebe9989920f9da","type":"text","text":"# Consensus Problem\n\nL'Atomic Commit è un caso particolare del più generale problema del consenso, in cui $n$ processi devono arrivare a concordare su un valore proposto come input del protocollo da uno dei processi.\n\nMolti problemi possono essere ricondotti a questo), e chiedo\n\n- agreement (for the safeness) - if 2 non-faulty processes decide a value, it must be the same\n- validity (or non-triviality)- il valore scelto è uno degli input, ovvero uno scelto da uno dei processi (non una roba a caso che arriva da chissà dove)\n\t- avoids trivial protocols like \"whatever the input, choose 17\"\n- termination (for the liveness) - prima o poi il protocollo deve finire\n\nnon mi interesso della complessità. pure se è exp, almeno c'è un algoritmo.\n\n","x":295,"y":1351,"width":640,"height":499,"color":"6"},
		{"id":"0ebec2401a36c3ef","type":"text","text":"# Sincronizzazione degli Orologi\n\n- Causal Delivery - Se \n- Snapshot Consistency","x":295,"y":-2080,"width":640,"height":220,"color":"3"},
		{"id":"d582dc2a1cfd5670","type":"text","text":"# DTLog (Distributed Transaction Log)\n\nSe i processi sono ***fail-recover***, quando resuscitano perdono tutta la memoria tranne il ***log***.\n\nAssumendo che il log non possa avere `fail` e dividendo i nodi in coordinatori e partecipanti, la domanda è: ***meglio scrivere sul log e poi mandare il messaggio o viceversa?*** Se riesco ad eseguire entrambe le operazioni va tutto bene, ma in entrambi i casi potrei morire nel mezzo. L'idea è che, se succede, quando torno operativo leggo il log e riprendo da dove mi ero fermato. Ma è safe fare così?\n\n- `C - voteRequest` - Quando inizia il protocollo scrive sul log `Start2PC`, che contiene la lista dei partecipanti. ***L'ordine è indifferente***:\n\t- se prima scrivo sul log e poi muoio, la lista dei partecipanti è ancora là e nel dubbio rimando la `voteRequest`.\n\t- Viceversa, se risvegliandomi non vedo nulla nel log la scelta migliore è mandare `abort` in `broadcast`;\n- `P - vote` - Qui è cruciale distinguere due casi:\n\t- ***Se decido di votare `yes` devo prima scriverlo sul log***. Viceversa, se mando e muoio poi non so cosa ho votato. Dovrei fare `abort`, ma non posso. Insomma, no.\n\t- **Se decido di votare `no` l'ordine è indifferente**, ma a sto punto faccio come `yes`.\n- `C - Decision` - Anche qui devo distinguere\n\t- ***Se decido `commit` devo prima scriverlo sul log***, per gli stessi motivi;\n\t- **Se decido `abort` l'ordine è indifferente**.\n\nIn pratica per una decisione negativa l'ordine è indifferente, mentre per una positiva è meglio `log then send`, alla peggio mando lo stesso messaggio due volte.\n\n###### Obiezione! Il log è su disco, può fallire!\n\nHo capito, ma mica puoi risolvere tutto. Magari fai più dischi e muoiono tutti. Non solo: quando chiamo una `syscall` per scrivere su disco, magari questa scrive in RAM. Allora dovrei forzare a scrivere su disco (tipo con una `flush`, su linux si chiama `fsync`?). Ma magari il disco ha un buffer e salta la corrente, perdo il buffer. Potresti usare un disco solo per il log e configurarlo in modo che non usi il buffer.\n\nIl punto è che è ragionevole assumere che il log sia safe per sviluppare il protocollo","x":-4043,"y":2356,"width":764,"height":1054,"color":"4"},
		{"id":"d63911fe834dbe22","type":"text","text":"# \\* Chiarimento - Il voto in Paxos\n\nQuando negli statement dico \"`A0` votes for `x`\" intendo dire che manda una `learn(j, x)`.\n\nSe ha mandato una `learn`, ha ricevuto una `accept`. Se è partita una `accept`, significa che è un `P` ha ricevuto un Quorum di `promise`, quindi sa cosa è safe far votare.\n\nQuindi il senso del teorema è \"in questo `round` di cui parliamo, che chiamiamo `j`, se un `A` vota `x` sono sicuro che nei round precedenti nessun `x'` aveva già raggiunto un Quorum\".\n\nIn altre parole, so che ad ogni `round` in cui anche solo un singolo `A0` vota `x` sono sicuro che il valore su cui tutti eventualmente concorderanno (i.e. il valore scelto) può essere alternativamente `x` oppure un valore che verrà scelto nei prossimi `round`.\n\nBene, e a che mi serve? Se così non fosse ed esistesse un valore scelto `x'` nei round precedenti, il fatto che `A0` voti `x` renderebbe il protocollo inconsistente, perché significherebbe che in round diversi vengono scelti diversi valori su cui vengono chiamate le `learn`, e di conseguenza diversi `L` potrebbero apprendere diversi `X`.","x":-3936,"y":3533,"width":551,"height":636,"color":"4"},
		{"id":"93212aed82c21df9","type":"text","text":"# Two-Phase Commit\n\nL'idea è di per sé semplice, poi si complica per farla funzionare in caso di ritardi e/o failures: dividiamo i nodi (processi) in ***coordinatori*** e ***partecipanti***, quindi abbiamo i seguenti step:\n\n- Il coordinatore `C` inizia il protocollo mandando una `voteRequest` ai partecipanti `P`;\n- Ogni partecipante decide cosa votare tra `yes` e `no`;\n\t- Se vota `no` fa direttamente `abort`.\n- La decisione di voto viene inviata a `C`;\n- Se `C` riceve le risposte di tutti i `P` e queste sono tutte `yes`, allora ***può*** prendere la decisione di fare `commit`.\n\t- Notare che non è obbligato a fare `commit`, sta di fatto una decisione viene presa.\n- `C` invia la decisione a tutti, i quali eseguono.\n\nTutto troppo bello. In un sistema reale ***i messaggi possono essere persi***, o fare talmente tanto ritardo che si danno per persi (tramite un timer). Cosa succede in questo caso?\n\n- Se si perde la `voteRequest`,\n\t- la prima volta che scade il suo timer, `P` chiede a `C` di rimandare la `voteRequest`;\n\t- la seconda assume che `C` sia morto e nel dubbio fa un `abort`.\n- Se si perde il voto, a `C` scade il timer per la risposta.\n\t- la prima volta `C` manda di nuovo la `voteRequest`;\n\t- la seconda assume che `P` sia morto e nel dubbio decide per un `abort`.\n- Se si perde la decisione che `C` manda a `P`\n\t- se io `P` ho votato `no` non è un problema, tanto ho già fatto `abort`;\n\t- se ho votato `yes` non è per niente safe scegliere di fare `abort`, quindi mi tocca aspettare. Magari mando a `C` un gentile sollecito come \"`aoo sta decisione??`\" sperando che arrivi. Ma magari lo chiedo tante volte e nessuno mi risponde. Magari `C` è morto. Allora chiedo agli altri `P` se hanno ricevuto la decisione. Questi possono rispondere in tre modi:\n\t\t- `ho ricevuto la decisione` - applico la decisione;\n\t\t- `non l'ho ricevuta, e...`\n\t\t\t- `... ho votato no` - faccio un `abort`;\n\t\t\t- `ho votato yes` - brutta situazione, non posso concludere niente e mi tocca chiedere ad un altro. Se chiedo a tutti, ***tutti*** hanno votato `yes` e ***nessuno*** ha ricevuto la decisione, tutti fanno `abort`.\n\nNotare che è un protocollo safe, ma è molto poco live (ci sono molti più modi per fare `abort` che per fare `commit`). Questa fratellanza tra `P` lo rende un ***cooperative protocol*** nel quale anche un singolo fail (tutti i processi sono fail-stop) fa fare `abort` a tutti.\n","x":-3025,"y":2356,"width":753,"height":1054,"color":"4"},
		{"id":"e69724a397d8eb9b","type":"text","text":"# Paxos Safety Theorem\n\n`THM` - If some acceptor `A0` votes for `x` in round `j`, then no value `x'`$\\neq$`x` can be chosen in previous rounds (i.e. se c'è stata una `learn(j, x)` da parte di `A0`, vuol dire che ha ricevuto una `accept(j, x)` da parte di `P0`. Se `P0` ha mandato una `accept(j, x)` significa che ha ricevuto un numero $\\geq Q$ di `promise` e sa che nessun altro `x'`$\\neq$`x` ha avuto una maggioranza nei round precedenti a `j`, i.e. è certo che `x` sia safe).\n\n`Proof` - Procediamo per induzione sul round $i$.\n\n- Base induttiva (`round 0`) - Il teorema è ovvio, non c'è nessun `round` precedente quindi non può essere stato già scelto nessun valore $\\neq$`x`;\n- Assumiamo vero il teorema per il `round i-1`;\n- Passo induttivo (`round i`) - A `P0` arrivano $|Q|$ messaggi di tipo `promise`. Tra questi, chiamiamo `j` il `maxround`, ovvero il $\\max_k$(`last round I voted`$_k$), dove $k\\in Q\\subseteq A$.\n\t- Segue per definizione che nessuno dei $k\\in Q$ ha votato tra i `round j+1` e`i-1`.\n\t\t- Se `j` valesse `-1` (i.e. `non ho mai votato`) avremmo finito la dimostrazione.\n\t- Segue anche che al `round j` all'interno di $Q$ almeno uno deve aver votato per $x$;\n\t\t- Potrebbe essere l'unico? Sì, perché tutti gli altri `accept` rivolti agli altri `A` potrebbero essere stati persi.\n\t- Ma se almeno uno ha votato `x` significa che un `P` gli ha detto di farlo con una `accept(j, x)`, quindi al `round j` nessun `A` può aver votato `y`$\\neq$`x`.\n\t- E per i `round` precedenti? A questo punto posso usare l'ipotesi induttiva, quindi nei `round`$<$`j` nessun valore `x'`$\\neq$`x` può essere stato scelto.","x":-3025,"y":3533,"width":753,"height":636,"color":"4"},
		{"id":"7bd4339e9dc5832c","type":"file","file":"2 - Atomic Commit/Paxos.png","x":-2021,"y":4728,"width":753,"height":474},
		{"id":"4043ace7e5011791","type":"text","text":"# Fast-Paxos\n\nFast-Paxos richiede che sia già stato scelto il coordinatore `P1`, i.e. tutti ne sono a conoscenza. Gli `A` sanno che possono accettare solo le `accept` provenienti da `P1`, e in qualche modo tutti i `P` conoscono le decisioni di `P1`.\n\nDetto questo, `P1` ha a disposizione due modi per far partire un `round`:\n\n- Può mandare una `prepare(i)` esattamente come in Paxos standard. In questo caso si parla di ***slow round***, ed è sostanzialmente un ***Paxos con leader*** (i.e. un Paxos che è anche live);\n- Può mandare direttamente una `accept(1, ⊥)`, ovvero una speciale `accept` senza un valore associato che comunica agli `A` di accettare le `accept` di qualsiasi `P`. Questa ***accept-any*** fa partire un ***fast round***: ogni volta che ad un qualsiasi `P` arriva un valore `x`, questo manda direttamente una `accept(1, x)`, senza passare per `P1`.\n\nFocalizziamoci sul `fast round`. Diversi `P` possono mandare una `accept(1, X)` dove `X` è diverso a seconda del `P`. Potenzialmente è un delirio! Possono succedere due cose:\n\n- Un singolo `x` raggiunge il Quorum, comunque esso sia definito. Tutti i learner apprendono quel valore, il protocollo termina al primo `round` e noi tiriamo un sospiro di sollievo;\n\t- In generale, la speranza del `fast round` è proprio quella di ***raggiungere il consenso già al primo `round`***, in modo che non vadano risolti conflitti.\n- Il protocollo non termina al primo `round` (i.e. nessun voto raggiunge il Quorum, quantomeno non per tutti i learner), tocca fare il secondo. Qui però nascono i problemi.\n\t- Il secondo `round` non può essere un `fast round`. Questo è ovvio se si pensa al fatto che i vari `P` hanno già espresso la propria opinione, ed ora bisogna solo risolvere le controversie in modo safe. Questo `slow round` viene infatti anche detto ***recovery round***. E cosa bisogna fare in questo `recovery round`? Esattamente quello che si fa in Paxos standard. `P1` sonda gli `A` con una `prepare`, riceve le `promise` con i voti e sulla base di questo sceglie il valore safe da proporre. Ma qual è il valore safe?\n\nPer capire qual è il valore safe da proporre, poniamo $|A|=5$. Siamo nel `recovery round`, dunque al coordinatore `P1` dovrebbero essere arrivate le `promise`. Diciamo che due voti vanno persi, un voto dice `x`, uno dice `y` e un altro non ha mai votato. Se fossimo in Paxos standard sarebbe facile scegliere cosa proporre: basta vedere il `maxround` tra `x` e `y`. Problema: sono entrambi `round 1`, ***la scelta non è univoca***. Cosa deve fare `P1`?\n\nIn questa situazione non posso mai essere sicuro di un valore safe, perché se scelgo `x` e i due voti andati perduti sono per `y` ho rotto il protocollo (e viceversa). La soluzione a questo dilemma è stare zitti, cioè non mandare alcuna `accept`. Se abbiamo capito come funziona Paxos, se il `P` non manda la `accept` significa che ***non ha ricevuto un Quorum di `promise`***.\n\n\"Ma come, $Q=\\ceil*{5\\over2}=3$!\" ... sì, ma evidentemente qui non è più sufficiente. ***Bisogna aumentare il quorum a*** $\\ceil*{2|A|\\over3}$. Scegliamo ***necessariamente*** $|A|=3n+1$ (e.g. 7) e riconsideriamo il dilemma del Quorum, che con questa modifica diventa $Q=\\ceil*{14\\over3}=5$.\n\n- $1\\over3$ delle `promise` non arriva, i.e. due voti vanno persi;\n- Il caso peggiore è che dentro il Quorum ci sia un perfetto equilibrio, e.g. $1\\over3$ dei voti sono per `x` e $1\\over3$ sono per `y`. Ma per la definizione di $|A|$ il Quorum è un numero dispari, quindi è impossibile che ci sia equilibrio perfetto: $Q=\\ceil*{{2\\over3}(3n+1)}=\\ceil*{2n+{2\\over3}} = 2n+1$.\n\t- Diciamo quindi che `x` ha ricevuto tre voti e `y` due. $Q=5$ quindi\n\t\t- Può `y` aver ottenuto un $Q$ di voti? No. Anche se i due voti persi fossero stati per `y`, arriverebbe al massimo a $4$;\n\t\t- `x` invece potrebbe arrivare a $5$, quindi `x` è il valore da proporre.\n\t- Poniamo invece che ci siano due voti a testa per `x` e `y`, e il quinto è un non-voto. Nessuno dei due può aver già ottenuto una maggioranza, quindi è safe scegliere qualsiasi valore (anche `z`$\\neq$`x`,`y`, se lo sceglie `P1`, proprio come in Paxos standard);\n\t\t- Qui `P1` ***potrebbe anche scegliere di far partire un altro `fast round`***. Ha poco senso se vede una maggioranza relativa in $Q$, ne ha di più se vede tutti voti diversi.\n\t- Notare che (come in Paxos standard) non è detto che basti un solo `recovery round`.\n\nLa morale della favola è la seguente: se c'è un valore che potrebbe già aver ottenuto un Quorum di voti, allora quello è il valore da proporre nel `recovery round`. Ma è la stessa regola di Paxos standard! Sì, quello che cambia è solo l'aumento di $Q$, dovuta al fatto che sto provando a \"forzare la mano\" con i `fast round`. Questo ***tentativo di velocizzare*** viene pagato a caro prezzo, perché\n- ***la tolleranza scende*** a $f'=\\floor*{n-1\\over3}$;\n- il nuovo $Q$ ***vale anche per i `fast round`***, il che significa che scende la probabilità di far approvare subito un valore (ogni `L` deve sempre vedere $Q$ `learn`!).\n\nDetto questo, è anche possibile unire Multi-Paxos e Fast-Paxos mandando la `super-prepare` e ricevendo una `super-promise`. Chiaramente, `P1` utilizzerà una `accept any` solo quando è safe accettare davvero qualsiasi valore, e.g. al primo round, quando nessuno ha votato.\n","x":-1018,"y":4728,"width":797,"height":1756,"color":"4"},
		{"id":"47a9499f82f58b66","type":"text","text":"# Esercizi III - Atomic Commit Protocols, Paxos\n\n**Exercise 13:** Give an ACP that also satisfies the converse of condition AC3. That is, if all processes vote Yes, then the decision must be Commit. Why is it not a good idea to enforce this condition?\n\n- <span style=\"color:#FFA500\">Cerca AC3 su atomic_commit.pdf (also for ex 14)</span>\n\n**Exercise 14:** Consider 2PC with the cooperative termination protocol. Describe a scenario (a particular execution) involving site failures only, which causes operational sites to become blocked.\n  \n**Exercise 15:** Show that Paxos is not live.\n\n- `accept` messages are so slow the timer expires at every round. `Acceptors` receive a new `prepare`, follows a `promise`, then the `accept` from a previous round arrive. Since the `Acceptors` made a `promise` referred to an higher round, any previous `accept` will be ignored. No `learn` will ever be sent, thus no vote and no chosen value at any round.\n  \n**Exercise 16:** Assume that acceptors do not change their vote. In other words, if they vote for value v in round i, they will not send learn messages with value different from v in larger rounds. Show that Paxos, with this modification, is safe. Unfortunately, the modification introduces a severe liveness problem (the protocol can reach a livelock).\n\n- Suppose each round a single `acceptor` decides a value to vote (i.e. a Quorum of `promises` is reached, then all `accept` messages get lost but one). It is possible to reach a \"stable\" situation in which all acceptors have decided a value but no value has a Quorum, thus no learner will ever learn anything (still the protocol goes on!).\n  \n**Exercise 17:** How many messages are used in Paxos if no message is lost and in the best case? Is it possible to reduce the number of messages without losing tolerance to failures and without changing the number of proposers, acceptors, and learners?\n\n- Theoretically, the minimum number of messages is given by\n\t- $Q$ `prepare` to obtain $Q$ `promise` (so $2Q$);\n\t- $Q$ `accept` to obtain $Q$ `learn` for every learner (so $Q+LQ$);\n\t- So it's $Q(3+L)$.\n- So yes it is, since in a real situation both `prepare` and `accept` are broadcasted to all the acceptors, so even in the best case we have $A(3+L)$ messages (of course, $A>L$).\n  \n**Exercise 18:** Assume that you remove the property that every round is associate to a unique proposer. After collecting a quorum of n-f promises (where n is the number of acceptors and f is such that n=2f+1), the proposer chooses one of the values voted in max round in the promises (of course it is not unique, the proposer chooses just one in an arbitrary way). Show that Paxos is not safe any more.\n\n- This is basically Fast Paxos with the wrong quorum. So another way of posing the question is: why $Q=\\text{floor}({n+1\\over2})$ is not enough anymore? In standard Paxos, $Q$ is enough for the following reason: since each and every round is associate to a unique proposer, the vote associated to the `maxround` is unique too. Thus, there's no ambiguity in choosing `x`by the rule of `maxround`. Why is this ambiguity a problem?\n\t- Suppose $n = 7$. At first round, two proposers start the protocol. All the acceptors reply with an \"`I never voted`\" statement, so `P1` feels free to send an `accept(1, x)` and same `P2` with `accept(1, y)`. For some reason, 4 acceptors receive the `accept(1, x)` and 3 the `accept(1, y)`. Let's suppose some learner (but not all of them!) learns `x`, since 4 is actually a quorum. Not all the learners have chosen the value, so the protocol goes on with, say, `round 2`. Let's assume a single proposer sends a `prepare(2)` and receives back 4 `promises` (which is actually the Quorum). Only, three of them say `I voted y` and only one says `I voted x`. Also, they have the same `maxround`. Since the only rule is to choose the value associated with `maxround`, the proposer may choose `y` and deliver to all the acceptors a `accept(2, y)`. A Quorum of acceptors may eventually tell the remaining learners to `learn(2, y)`. We're done with the protocol! Every learner has learnt a value, only for some of them it's `x` while for someone else it's `y`. This version of Paxos is not safe, but like we said  it is sufficient to rise the Quorum to ${2\\over3} n$ to achieve safeness (Fast-Paxos).\n\n**Exercise 19:** Assume that all proposers are learners as well. Let even rounds be assigned to proposers with the rules that we know. Moreover, If round 2i is assigned to proposer p, then also round 2i+1 is assigned to proposer p. Odd rounds are “recovery” rounds. If round 2i is a fast round and if the proposer of round 2i sees a conflict (it is also a learner), then the proposer immediately sends an accept for round 2i+1 with the value that has been most voted in round 2i, without any prepare and any promise. Is safety violated? If yes, show an example. If not, demonstrate safety. \n\n- I assume that the `learn` messages to the `Proposer/Learner` may be lost (and by that I also mean that they take so long they're eventually considered lost by some timeout event). Depending on the interpretation of the question (more specifically, of the word \"conflict\") I have two different answers. \n\t- `Counterexample` - Since no Quorum is ever mentioned in the question, even with, say, $|A| = 22$ the `Proposer/Learner` may receive only $3$ messages, where $2$ votes are a majority. In this case, it is absolutely ***not safe*** to force an `accept(2i+1, x)`, and this is quite obvious: in the remaining $19$ `learn` messages it is fairly possible that some other `x'` has reached a Quorum, which the `P/L` can't see. So, forcing `x` may result in some `L` learning `x` and some others learning `x'`;\n\t- `Safety Proof` - If it's implied that the conflict is within a Quorum of received `learn` messages, it is trivially safe:\n\t\t- If such majority is $>{1\\over3}|A|$, then not only it's safe, it's the only right value to be chosen;\n\t\t- If it isn't, by using the mutual exclusion in the spirit of the Fast Paxos Safety Theorem proof we can conclude that any value is safe, so not only it's safe, it's optimal.\n\n**Exercise 20:** You are an optimization freak. You realize that in Fast Paxos, in some cases, it is not necessary that the proposer collects n-f’ (the Fast Paxos quorum) promises to take a decision. Which is the minimum quorum and under what hypothesis this minimum quorum is enough to take a decision?\n\n- If, as it happens in real cases, the `proposer` is also a `learner`and he has learnt some value `x`, for the Fast Paxos Safety Theorem he doesn't need no `promise` at all, he just knows `x` is the value to`accept`;\n- Also if it's the first round, instead of `accept any` he could easily `accept(1, x)`, where `x` is its own value. Not a fulgid example of democracy, though;\n- <span style=\"color:#FFA500\">Boh, altro??? Così gli sto dicendo che il minimo è 0...</span>\n\n\n\n\n","x":-3025,"y":5328,"width":1757,"height":1603,"color":"5"},
		{"id":"1a4ae7dd4ea788ae","type":"text","text":"# Ma vale la pena?\n\nIn una parola: ***no***.\n\nIn generale Fast-Paxos è più lento di Multi-Paxos, ma può avere buone performance se\n\n- Ci sono pochi `P` e poche transazioni (i.e. poche $T$ che arrivano a pochi `P`);\n- Ci sono pochi conflitti all'interno del Quorum, ad esempio in piccoli cluster di server con poco carico (viceversa, i server non conoscono lo stato l'uno degli altri, e sale la probabilità di avere conflitti).\n\nIn pratica non è molto usato: riduce la tolleranza per un boost in velocità che spesso con i conflitti neanche esiste (i sistemi reali sono spesso in overload).\n\n","x":-1018,"y":6588,"width":797,"height":343,"color":"4"},
		{"id":"5aaddbdcdf2d4e68","type":"text","text":"spesso inoltre non c'è davvero bisogno di aspettare l'ordine giusto per il commit\n\n# ?","x":-959,"y":7028,"width":680,"height":117,"color":"1"},
		{"id":"8d15b0299f305f56","type":"text","text":"# Fast Paxos Safety Theorem\n\n`THM` - If some acceptor `A0` votes for `x` in round `j`, then no value `x'`$\\neq$`x` can be chosen in previous rounds (i.e. se c'è stata una `learn(j, x)` da parte di `A0`, vuol dire che ha ricevuto una `accept(j, x)` da parte di `P0`. Se `P0` ha mandato una `accept(j, x)` significa che ha ricevuto un numero $\\geq Q$ di `promise` e sa che nessun altro `x'`$\\neq$`x` ha avuto una maggioranza nei round precedenti a `j`, i.e. è certo che `x` sia safe).\n\n`Proof` - Esattamente come Paxos, per induzione su $i$.\n\n- Base induttiva (`round 0`) - Il teorema è ovvio, non c'è nessun `round` precedente quindi non può essere stato già scelto nessun valore $\\neq$`x`;\n- Assumiamo vero il teorema per il `round i-1`;\n- Passo induttivo (`round i`) - Se `maxround = j`$=0$, il teorema è ovvio. Se `j`$>0$ \n\t- per scegliere il valore da proporre ho due vie:\n\t\t- ***se*** $\\exists$`x` t.c. ha abbastanza voti in $Q$ da poter raggiungere una maggioranza contando i voti fuori da _$Q$_ (che non conosco) ***oppure*** `x` è l'unico valore che compare nelle `promise`, ***allora*** scegli `x`;\n\t\t- ***altrimenti***, scegli un qualsiasi valore.\n\t- Nota che, mentre il `se` è ovvio per definizione, le scelte date da `oppure` e da `altrimenti` sono giustificate dal teorema stesso, che uso per ipotesi induttiva.\n\t\t- Se compare solo `x`, secondo il teorema non può essere stato già scelto alcun valore diverso da `x`, quindi `x` è safe;\n\t\t- Se non siamo in nessuno dei due casi del `se`, e.g. ci sono due o più valori, posso comunque applicare il teorema, il quale mi dice che nessun valore diverso da `y` è stato votato in precedenza, né da `z`, e via dicendo. In pratica si escludono tutti a vicenda, quindi nessun valore può essere stato votato in precedenza, quindi qualsiasi valore è safe.","x":238,"y":5242,"width":753,"height":729,"color":"4"},
		{"id":"ee7f51e5be213a53","type":"text","text":"se hai tempo fai delle gif","x":-2773,"y":4935,"width":250,"height":60,"color":"5"},
		{"id":"00258a5944b2d60e","type":"text","text":"# Paxos I - Definizione e Primo Round\n\nProtocollo maggioritario a turni in cui dividiamo i nodi in tre categorie:\n\n- ***Proposers*** (***`P`***) - Ad inizio protocollo, ad ognuno di questi nodi viene assegnato un array statico di valori interi tali che nessuna coppia di `P` abbia un numero in comune. Tali interi (detti ***round***) definiscono in che ordine i `P` potranno proporre un singolo valore `v` da fare accettare al sistema (i.e. un valore sul quale potenzialmente tutti alla fine concorderanno e sul quale tutti potranno fare `commit`);\n\t- Non è una buona idea averne uno solo, se muore l'intero protocollo è morto. Al contempo, perché il protocollo non muoia ne basta uno vivo.\n- ***Acceptors*** (***`A`***) - Sono i nodi più importanti, perché decidono a maggioranza su quale valore deve fare `commit` l'intero sistema (questo compito sta ai nodi di tipo `L`). Se ce ne sono $n$, il protocollo tollera il fail (***resilience***) di al più $f=\\floor*{n-1\\over2}$ nodi di tipo `A`;\n\t- Se ci sono più di $f$ fail sugli `A`, il protocollo si ferma (\"*safe but not live*\");\n\t- In genere $n$ è dispari, perché in questo modo la tolleranza resta la stessa anche se muore un nodo. Tanto vale metterne uno in più.\n- ***Learners*** (***`L`***) - Sono quelli che alla fine del protocollo danno il via libera al `commit` se tutto è andato bene (i.e. se vedono una maggioranza di voti da parte degli `A`).\n\nDetto questo, il ***primo round*** del protocollo funziona nel seguente modo:\n\n- Un qualsiasi `P` (e.g. `P0`) può iniziare il protocollo tramite un messaggio `prepare`, associando alla sua richiesta il più piccolo valore di `round` presente nel suo vettore statico. Per costruzione, non è detto che tale valore sia proprio `1`. Scegliamo quindi `3` per guardare ad un generico primo `round`. Con una `prepare(3)`, ***un singolo `P` dice a tutti gli `A`*** di prepararsi per il round `3`;\n- Gli `A` che ricevono la `prepare` (il sistema è asincrono, i messaggi si possono perdere o metterci un tempo talmente lungo da essere dati per persi!) rispondono a `P0` con una `promise(3, -, -)`, che significa `prometto di partecipare al round 3 e che in futuro non parteciperò ad alcun round < 3. Inoltre non ho mai votato`;\n- Se a `P0` arriva un numero $\\geq Q = n-f$ (detto ***quorum***) di `promise`, è in grado di dire con certezza che ***non c'è alcun valore già votato in un precedente round*** (e a maggior ragione nessuno con una maggioranza), ***quindi propone il proprio valore*** `v` tramite una `accept(3, v)`;\n\t- Se ne arrivano meno di $Q$ semplicemente scatta il timeout e si ricomincia.\n- Gli `A` che ricevono `accept(3, v)` mandano una `learn(3, v)` (i.e. il ***voto***) ai `L`;\n- Ogni `L` riceve in generale un diverso numero di `learn`. Se tale numero di `learn(3, v)` (i.e. stesso round e stesso valore) è $\\geq Q$, allora il generico ***`L0` accetta il valore `v`***.\n\t- Se tutti i `L` accettano, il protocollo è finito;\n\t- In caso contrario, il protocollo ricomincia con un altro round.","x":-2021,"y":2356,"width":753,"height":1054,"color":"4"},
		{"id":"98ff8ea31a5611b2","type":"text","text":"# Paxos II - Secondo (e generico) Round\n\n- Un qualsiasi `P` (e.g. `P1`) ricomincia il protocollo tramite una `prepare`, associando alla sua richiesta il più piccolo `round` nel suo vettore statico, diciamo `5`.\n\t- In generale è ottimale scegliere il round in modo che sia $>$ del massimo round già giocato dall'intero sistema. Il singolo `P` per come è stato descritto finora non può avere questa informazione, ma in alcuni casi è possibile che i `P` siano anche `A`. Questa ottimizzazione mira a massimizzare la *liveness*, perché il protocollo in sé già mi garantisce *safeness*.\n- Gli `A` che ricevono le `prepare` rispondono a `P1` con una `promise(5, 3, v)`, che significa `prometto di partecipare al round 5 e che in futuro non parteciperò ad alcun round < 5. L'ultima volta che ho votato era il round 3, e ho votato v`;\n\t- Se `P1` avesse estratto `2` come `round`, la `prepare(2)` sarebbe stata ***ignorata*** da tutti gli `A` che hanno già inviato una `promise(n, *, *)` con `n > 2`. In tal caso il protocollo attende il timeout e riprova con un altro `round`.\n- Se a `P1` arriva un numero $N\\geq Q=n-f$ di `promise`, ci sono diversi possibili scenari:\n\t- Al secondo `round` ci sono due scenari verosimili:\n\t\t- Nessuno degli $N$ `A` ha mai votato (i.e. il primo `round` è terminato per via dello scadere del timer, quindi è possibile che sia andata perduta la maggioranza di una qualsiasi tipologia di messaggio del protocollo). In questo caso torniamo all'algoritmo del primo `round`;\n\t\t- Qualcuno degli $N$ `A` ha votato `v`. ***`P0` non può proporre il proprio valore*** `x`, ma è costretto a riproporre `v`.\n\t- Al generico `round n`, ***`P1` propone il valore associato alla `promise` avente il più grande valore di `ultimo round a cui ho votato`***.\n- A questo punto abbiamo capito che se è partita una `accept` significa che `P1` ha ricevuto un numero $N \\geq Q$ di `promise`, quindi in pratica se siamo qui è perché\n\t- Viene proposto un nuovo valore `x` perché nelle `promise` non c'era alcun voto;\n\t- Viene riproposto il valore `v` più recentemente votato.\n- Gli `A` mandano una `learn` con il voto corrente.\n\t- Notare che se partono $M\\geq Q$ `learn` (i.e. $M\\geq Q$ `accept` dello stesso `round` arrivano a destinazione senza perdersi ed entro il timer) allora sicuramente esiste una ***maggioranza di `A` su un singolo valore `v`***. Questo necessariamente implica che nessun valore che non sia `v` potrà mai essere approvato per il `commit`, perché dallo step successivo qualsiasi maggioranza $N\\geq Q$ vista dal `P1` di turno conterrà sicuramente il valore `v`, che sarà anche quello con `round` maggiore. Questa osservazione si formalizza in un ***safety theorem***.\n- Ogni`L` riceve in generale un diverso numero di `learn`. Se il numero di `learn` con stesso round e stesso valore è $\\geq Q$, allora il generico ***`L0` accetta il valore `v`***.\n\t- Come conseguenza del teorema precedente, se un singolo `L0` accetta un valore `v` è impossibile che qualsiasi altro `L1` accetti un valore `x`$\\neq$`v`.","x":-2021,"y":3533,"width":753,"height":1077,"color":"4"},
		{"id":"c604e7d9b7779409","type":"text","text":"# Multi-Paxos\n\nOgni run di Paxos raggiunge il consenso su un singolo valore. Tuttavia, in una situazione reale è verosimilmente necessario approvare una sequenza di valori, non uno solo. La soluzione ovvia è eseguire diverse instance del protocollo una dopo l'altra.\n\nMa si può fare meglio? Spoiler: sì. Consideriamo la seguente modifica al protocollo: invece di mandare una `prepare(i)` in cui diciamo \"preparati al round $i$...\" sottintendendo \"... per una singola istanza che farà approvare un singolo valore\" proviamo a dire \"***preparati al round $i$ di $N$ istanze, ognuna delle quali farà approvare un singolo valore***\".\n\n- La `prepare(i)` diventa una `prepare(N, i)`, in cui `N` è il numero dell'istanza. Posso quindi ottimizzare inizializzando il protocollo con una singola `prepare(1-1000, 1)` ovvero \"`preparati al primo round di 1000 istanze`\", di fatto evitando $999$ messaggi `prepare(i)`.\n\t- Possiamo chiamare questo tipo di messaggio \"`super-prepare`\".\n- Stessa cosa per la prima `promise(1-1000, 1, -, -)`, con cui ogni `A` comunica di essere pronto per il primo `round` delle istanze `1-1000`;\n\t- Possiamo chiamare questo tipo di messaggio \"`super-promise`\".\n- E le `accept`? Il sistema è solo *pronto* alle istanze, non le ha ancora ricevute. Quando `P1` riceve $T_1$, cioè il primo valore da approvare (e.g. dal client) è sufficiente mandare una `accept(1, 1, T1)` ovvero `accept(istanza, round, value)`.\n\t- Se ne riceve un'altra può mandare direttamente `accept(2, 1, T2)`? Sì, perché l'importante è l'univocità della coppia `istanza, round`;\n\t- E se si perdono tutte le `accept`? Banalmente, `P1` la rimanda.\n\nBene, se si perde l'`accept` di `P1` è `P1` stesso a rimandarla. Perché non qualcun altro? Prenotando le istanze `1-1000`, `P1` sta sostanzialmente dicendo che per queste istanze il valore da approvare è il suo, e il protocollo serve solo a raggiungere il consenso su `T1` (non su un generico valore proposto da uno qualsiasi dei `P`). Finora abbiamo assunto che i `P` non fossero in competizione, ma se c'è un altro `P` (diciamo `P2`) a ricevere delle $T$ da far approvare che faccio?\n\n- Opzione 1 - `P2` prenota le istanze `1001-2000`. Non è ottimale, perché magari approvo le istanze `1, 2, 1001` ma da qui in poi non posso fare `commit` perché devo aspettare tutte le istanze tra `2` e `1001`, il che chiaramente è la morte della liveness;\n- Opzione 2 - Tutti i `P` mandano le proprie $T$ a `P1` , visto che il genio si è prenotato `1000` istanze di fila. In questo scenario `P1` diventa il ***coordinatore***.\n\t- Con che logica si sceglie? E se muore? $\\Rightarrow$ ***Leader Election Problem***;\n\t- ***Se esistesse un perfetto protocollo di Leader Election, Paxos diventerebbe live***, perché si eviterebbe il conflitto tra diversi proposer e si romperebbe il meccanismo di inibizione dei round precedenti (***Paxos Liveness Problem***): la `super-prepare` viene mandata una volta sola con `round 1` (i.e. non c'è modo di *inibire* un `round`).\n\t- Non è necessario che ci sia. Alla peggio Multi-Paxos è safe ma non molto live.","x":-1018,"y":2356,"width":797,"height":1054,"color":"4"},
		{"id":"df6f8d2bb2d685d0","type":"text","text":"# Paxos III - Ottimizzazioni\n\nCosì per come è definito, il protocollo è safe. Ma è possibile aumentarne la liveness?","x":-830,"y":4012,"width":421,"height":120,"color":"6"},
		{"id":"6a09d757776030a8","type":"text","text":"# Leader Election\n\n\n\nma quindi essere \"live\" in pratica significa risolvere il problema al primo round??\n\nse ho una sequenza 1...n di nodi ed ognuno deve concordare sul leader come faccio?\n\nassumiamo sistema sincrono e niente faults\n\n- se tutti sanno già chi è l'id più basso e la regola è questa, ho già finito. so chi è il leader.\n\t- se non lo sanno ma io so di esserlo faccio broadcast\n- $1\\to2\\to3\\to...\\to n$ e viceversa. Sulla carta è 2n, in pratica il messaggio diventa sempre più grande, quindi è più $n^2$\n- faccio una struttura ad albero in cui ogni coppia prende il minimo degli ID - $\\log(n)$\n- ogni nodo fa ***broadcast***, e in O(1) ho finito\n\nquesta era la easy mode. ora facciamo che può esistere un fault di tipo crash\n\n- se avviene a $t_0$, il sistema è ancora sincrono, quindi se entro un tot non arriva il messaggio di un nodo so che è morto\n- non è detto che il broadcast sia atomico, quindi magari invio ai primi 4 l'info che sono il leader e poi muoio. Quindi non tutti concordano sul leader.\n- se lo faccio due volte però inizia a funzionare: se il leader crasha dopo t1 ma prima di delta, al secondo giro avrò consenso.\n\nse ci sono al più f faults mi servono in generale f+1 round: il caso peggiore è che in ogni round muoia il leader a metà del broadcast, anche nel caso peggiore esiste un round senza fault.\n\nora, se il sistema è asincrono e può esistere anche solo un fault, allora non esiste soluzione.\n\nreal life i sistemi sono sincroni, ma a volte no, e non sai quando. also, spesso non si elegge un solo leader, ma diversi. c'è un main leader (master) e tanti leader in seconda (slaves).\n","x":238,"y":3534,"width":753,"height":1077},
		{"id":"b51bcda4f72fdde5","type":"text","text":"### Origine del nome \"Byzantine\"\n\nIl termine deriva dal cosiddetto **\"Problema dei Generali Bizantini\"** (_Byzantine Generals Problem_), un problema teorico descritto in un famoso articolo di Leslie Lamport, Robert Shostak e Marshall Pease nel 1982.\n\nL'idea dietro il nome è legata a uno scenario in cui un gruppo di generali di un esercito bizantino deve coordinarsi per attaccare o ritirarsi da una città. Tuttavia, alcuni di questi generali potrebbero essere traditori e potrebbero inviare informazioni false agli altri. Il problema diventa come garantire che i generali leali possano giungere a un accordo su una strategia comune, nonostante la presenza di traditori (ovvero, processi difettosi o maliziosi).\n\nQuesto nome pittoresco viene usato per descrivere l'aspetto complesso e potenzialmente confuso dei fault che possono accadere in un sistema distribuito quando uno o più processi non si comportano in modo affidabile o coerente.","x":-5541,"y":1388,"width":680,"height":442,"color":"#4545ff"},
		{"id":"c2b4396b3b84e4de","type":"text","text":"# Overview\n\nIl problema è totalmente descritto da queste proprietà che deve possedere:\n\n1. All processes that reach a ***decision***, must reach the same one.\n2. Processes cannot reverse their decisions.\n3. The `commit` decisions can only be reached if all processes voted `yes`.\n4. If there are no ***failures*** and all processes voted `yes`, decision must be to `commit`.\n\nI `failure` possono essere di diversi tipi:\n\n- `crash failure` - il processo muore (***fail-stop***), poi eventualmente potrebbe anche risvegliarsi (in tal caso è ***fail-recovery***);\n\t- Un protocollo fail-stop tollera anche i fail-recovery.\n- `omission failure` - il processo non manda un messaggio che secondo il protocollo avrebbe dovuto mandare;\n- `byzantine failure` - il processo continua a funzionare, ma lo fa in modo imprevedibile, inconsistente o malevolo.\n\nI protocolli che seguono non ammettono alcun `byzantine node`.\n\nEssendo il sistema asincrono, è impossibile distinguere un processo che ha avuto un `crash` da uno che è solo molto lento a rispondere. In virtù del Teorema FLP, questo rende il problema in generale irrisolvibile. Tuttavia, nelle situazioni pratiche questo è raramente un problema.\n\nAnche un processo che sulla carta sembra avere liveness molto bassa può rivelarsi efficiente, perché il tasso di `failures` è generalmente basso.","x":-3981,"y":1240,"width":640,"height":722,"color":"4"},
		{"id":"8876ea52fed47dda","type":"text","text":"# Esercizi I - Definizioni e Logic Clock\n\n**Exercise 1:** Let C1 and C2 be two consistent cuts. Show that the intersection of C1 and C2 is a consistent cut.\n\n- By definition, if $e_1'\\in C_1$ and $e_1\\to e_1'$ then $e_1\\in C_1$ (same for $C_2$). If $e'\\in C_1\\cap C_2$, then $e'\\in C_1$ ***and*** $e'\\in C_2$. Since $C_1$ and $C_2$ are consistent, if $e\\to e'$ it is also true that $e\\in  C_1$ ***and*** $e\\in C_2$, which means $e\\in C_1\\cap C_2$, so $C_1\\cap C_2$ is consistent.\n\n\n**Exercise 2:** Let C1 and C2 be two consistent cuts. Show that the union of C1 and C2 is a consistent cut.\n\n- Same as before. If $e'\\in C_1\\cup C_2$, then $e'\\in C_1$ ***or*** $e'\\in C_2$. Since $C_1$ and $C_2$ are consistent, if $e\\to e'$ it is also true that $e\\in  C_1$ ***or*** $e\\in C_2$, which is sufficient for $e\\in C_1\\cup C_2$ to hold, so $C_1\\cup C_2$ is consistent.\n  \n**Exercise 3:** Show that every consistent global state can be reached by some consistent run.\n\n- One may represent a cut as a collection of local states, so a consistent global state can be seen as a consistent cut. That said, a consistent run is a run in which every relation $e\\to e'$ is respected, thus can be seen as a sequence of consistent cuts, i.e. a sequence of consistent global states. <span style=\"color:#FFA500\">Vale??</span>\n  \n**Exercise 4:** Let C1 and C2 be two consistent cuts. If C1 is a subset of C2, then C2 is reachable from C1. (There exists a consistent run that reaches C1 and then reaches C2.)\n\n- It is equivalent to say $C_1\\subset C_2$, $GS_1\\subset GS_2$ and so $TS_1\\subset TS_2$. But that's the strong clock condition, so we can say that $e\\to e'$ for some $e\\in C_1$ and $e'\\in C_2$. <span style=\"color:#FFA500\">I think it doesn't really work, but it's nice</span>\n  \n**Exercise 5:** Label with proper logical clock all the events of the distributed computation in image vector.pdf. (You can consider events that receive a message and immediately send it as single events.)","x":-2095,"y":-3910,"width":695,"height":831,"color":"5"},
		{"id":"570439671731e83f","type":"file","file":"0 - Definitions/Ex5.png","x":-2648,"y":-3605,"width":400,"height":222},
		{"id":"fbc1ef28f3589012","type":"text","text":"# Definizioni Processi\n\nPer l'insieme dei processi che scambiano messaggi definiamo\n\n- ***History*** - $h_i$ è l'insieme di tutti gli eventi sulla timeline del processo $i$;\n\t- $H=\\{h_i\\}$ è la collezione di tutte le storie.\n- ***Prefix*** - $p_k(h_i)$ è l'insieme dei primi $k$ eventi di $h_i$;\n- ***Local State*** (***LS***) - dopo $k$ eventi, il processo $i$ si trova nello stato $\\sigma_i^k$;\n\t- ***Global State*** (***GS***) - Collezione di tutti i LS al tempo $k$;\n- ***Cut*** (***C***) - $C=\\{p_{k_{i}}(h_i)\\}$ è una collezione dei prefissi di ogni processo. Non necessariamente ogni processo deve avere lo stesso numero di eventi $k$ nel proprio prefisso (infatti $k$ è una $k_i$), né è necessario che $k\\neq0$. Ogni C individua un GS;\n- $e_2^1$ è l'evento con cui il processo $2$ manda una `request` al processo $1$, e $e_1^2$ è l'evento con cui il processo $1$ registra il ricevimento di tale `request`, necessariamente $e_2^1\\to e_1^2$ (i.e. $e_2^1$ ***avviene prima di*** $e_1^2$).\n\nDetto questo, l'ordine degli eventi sulla singola $h_i$ viene mantenuto ad ogni ***run*** (i.e. ad ogni esecuzione dell'intero sistema), ma l'ordine relativo degli eventi di diversi processi può variare. Formalmente, \"a ***run*** $k'$ is a reordering of events such that internal order of every process is preserved\", i.e. sono diverse sequenze di computazione che preservano la coerenza del singolo processo.\n\nQuesto da solo non garantisce che, ad esempio, $e_2^1\\to e_1^2$. Una run in cui tutte le relazioni tra eventi di tipo $\\to$ sono rispettate è detta ***consistent run*** (***CR***). In modo analogo, un ***consistent cut*** (***CC***) è un cut tale che se $e\\to e'\\,\\wedge\\,e'\\in C\\Rightarrow e\\in C$.  Una consistent run è una run in cui qualsiasi C è consistente.","x":175,"y":-3958,"width":880,"height":599,"color":"4"},
		{"id":"f9d9980fa75657d5","type":"text","text":"# Causal Delivery\n\nPer ***ricostruire la run di un DS*** possiamo usare un processo $p_0$ al quale tutti i processi $p_i$ inviano i propri eventi.\n\nTuttavia il sistema è asincrono, quindi quando $p_0$ manda le `request` ai singoli processi questi in generale ricevono e rispondono in momenti diversi. Questo può portare $p_0$ a vedere un ***cut inconsistente*** del sistema, e di conseguenza a ricostruire una ***run inconsistente***.\n\nPotrebbe perfino non essere una run, se ad esempio $e_1^2$ arriva prima di $e_1^1$. Questo problema si risolve assumendo che i singoli canali siano FIFO.\n\nPer ovviare a questo problema è necessario sviluppare un criterio di ordinamento robusto rispetto ad asincronia ed unreliability, realizzato tramite un ***clock*** sul quale tutti i processi concordano.\n\nL'idea generale è che $p_0$ faccia `commit` solo e soltanto se l'evento che riceve è l'immediato successivo dell'ultimo registrato, mantenendo in un ***buffer*** tutti gli eventi ricevuti nell'ordine sbagliato.","x":-715,"y":-3910,"width":640,"height":503,"color":"3"},
		{"id":"96363645d77cc1f4","type":"text","text":"# Snapshot Consistency","x":1305,"y":-3909,"width":599,"height":503,"color":"3"},
		{"id":"4fd496a7b455c76c","type":"file","file":"0 - Definitions/SchemaGenerale.png","x":393,"y":-2845,"width":444,"height":303},
		{"id":"e0ba5ac0341a437d","type":"file","file":"0 - Definitions/RequestAndCut.png","x":393,"y":-3280,"width":444,"height":370},
		{"id":"18e22186b729910c","type":"text","text":"# Distributed Mutual Exclusion\n\nLC permette di implementare un algoritmo distribuito (\"di Lamport\", obv) per garantire l'accesso a risorse condivise evitando i deadlock.\n\nL'idea (molto riassunta perché lui non l'ha fatto) è la seguente: chi vuole accedere alla risorsa condivisa fa broadcast con la `request` e con il suo timestamp $t$ . Se riceve le `ACK` di tutti gli altri processi e tutti rispondono con un timestamp $t'>t$, allora il lock è safe.\n\nQuesto protocollo usa $3(N+1)$ messaggi. Si può fare di meglio? Ni. C'è l'algoritmo di Ricart-Agrawala’s, che passa a $2(N+1)$.","x":-685,"y":-5920,"width":580,"height":326,"color":"4"},
		{"id":"3458d02a2952e9c7","type":"text","text":"# Real Global Clock\n\nLa prima tentazione è quella di definire un ***Real Global Clock*** (RC) condiviso da tutti i processi, in modo tale che$$e\\to e' \\Rightarrow RC(e)<RC(e')$$Questa prende il nome di ***Clock Condition***, e notiamo che\n\n- L'implicazione non è un $sse$. Due eventi tali che $RC(e)<RC(e')$ possono banalmente essere scorrelati (***eventi concorrenti***), dunque non c'è alcuna necessità che $e\\to e'$;\n- È possibile costruire clock che rispettino questa condizione senza la strong assumption che siano RC.\n\n\n Se il canale fosse sincrono potrei definire la grandezza $\\delta$ della finestra di buffer per $p_0$ e una ***delivery rule*** del tipo \"At time $t$, deliver all messages in order whose timestamp is smaller than $t -\\delta$\".\n \nUna simile costruzione è problematica non tanto per l'assunzione di avere un clock globale (che si potrebbe risolvere con un Network Time Protocol, NTP), quanto per la necessità di implementare una delivery rule che dipende da una finestra temporale $\\delta$ (dunque siamo costretti ad assumere che il sistema sia sincrono).\n","x":-685,"y":-5281,"width":580,"height":588,"color":"4"},
		{"id":"6dcdd47739ab639e","type":"file","file":"1 - Logic Clock/LamportClock.png","x":417,"y":-5888,"width":397,"height":262},
		{"id":"0b9a16cefb33b26c","type":"file","file":"1 - Logic Clock/VectorClock.png","x":1405,"y":-5888,"width":400,"height":262},
		{"id":"61752071951b1015","type":"text","text":"# Vector Clock\n\nÈ chiaro che a causa degli eventi concorrenti il clock non può essere un semplice numero. Definisco allora una ***strong clock condition***$$TS(e)<TS(e') \\Leftrightarrow e\\to e'$$In pratica voglio che una relazione d'ordine tra i timestamp definisca la dipendenza tra gli eventi. Posso trovare una rappresentazione per i $TS$ che consenta una relazione d'ordine per eventi dipendenti e non la consenta per eventi concorrenti? Sì: usiamo un ***vector clock***.\n\nDiciamo che la nuova condizione di correlazione è$$e_i\\to e_j \\Rightarrow \\forall k\\, VC(e_i)[k] \\leq VC(e_j)[k]\\,\\,\\wedge\\,\\,\\exists\\,k': VC(e_i)[k'] < VC(e_j)[k']$$cioè se $e_i\\to e_j$ allora ogni componente del $VC$ di $e_i$ deve essere minore o uguale del $VC$ di $e_j$, di cui almeno una strettamente minore (se non metto quest'ultimo pezzo è valido anche $e_i\\to e_i$).\n\nIn pratica abbiamo detto che $e\\to e' \\Leftrightarrow TS(e)\\subseteq TS(e')$, ovvero che due eventi sono correlati $sse$ la storia di uno è un sottoinsieme della storia dell'altro (per eventi concorrenti questa condizione è falsa, senza rompere i casi in cui la condizione è vera).\n\nOk, ma come è fatto il $VC$? Esattamente come il $LC$, semplicemente ogni processo ha un vettore con tutti i $LC$ di tutti i processi.\n\nE la delivery rule? Quando $p_j$ invia un messaggio $m_j$ a $p_0$, questo contiene il suo VC, ovvero un $TS(m_j)$. $p_0$ fa una commit se$$D[j] = TS(m_j)[j] - 1\\quad \\wedge\\quad D[k]\\geq TS(m_j)[k]\\quad\\forall k\\neq j$$o più intuitivamente$$TS(m_j)[j] = D[j] + 1\\quad \\wedge\\quad TS(m_j)[k]\\leq D[k]\\quad\\forall k\\neq j$$\ndove $D$ è il VC dell'ultimo `commit`, ovvero se all'arrivo di $m_j$\n\n- il suo $VC[j]$ (i.e. il suo stesso LC) è ***esattamente $+1$ rispetto al dato salvato*** da $p_0$ in $D[j]$. In questo modo, $p_0$ è sicuro di aver già visto tutti gli eventi che precedono questo lungo $p_j$;\n- $p_0$ deve anche essere ***sicuro di conoscere già tutti gli eventi degli altri processi $k\\neq j$ che sono precedenti a quello corrente***, il che si traduce nella disuguaglianza.\n\t- Segue che $D$ non si aggiorna solo quando viene eseguito un `commit`, ma anche quando un messaggio finisce nel buffer di attesa (i.e. basta che $p_0$ ne sia a conoscenza).","x":1305,"y":-5528,"width":599,"height":1082,"color":"4"},
		{"id":"ab73850207f6a30a","type":"text","text":"# Notazione\n\nL'unica informazione utile da specificare sui messaggi è il processo da cui vengono, quindi $m_j$ è inviato dal processo $j$.\n\nUn evento $e_i^k$ è il $k$-esimo evento del processo $i$.\n\nAl contempo, posso riferirmi a generici eventi e dire che $e_i\\neq e_j$. In questo caso sono semplicemente eventi, senza specificare dove (il processo) o quando (VC) siano avvenuti.","x":295,"y":-4320,"width":640,"height":279,"color":"4"},
		{"id":"db4411d94cf78039","type":"text","text":"# Lamport's Clock (LC)\n\nPer realizzare un clock possiamo usare dei ***counter*** che fungono da ***timestamp***. Una definizione che rispetta la clock condition è la seguente:\n\n- Sul singolo processo, ogni evento concorrente o di tipo `sending` incrementa di $1$ il counter, i.e. all'evento $n$ segue l'evento $n+1$;\n- Quando si verifica un evento di tipo `receiving`, il messaggio in arrivo porta con sé il timestamp $m$ dell'evento `sending` dal quale è partito. L'evento `receiving` assume allora valore $\\max(n+1, m)$, ovvero\n\t- $n + 1$ come nel caso precedente se il $m<n$;\n\t- $m$, se $m>n$.\n\nIn questo modo posso mettere come delivery rule \"Deliver all received messages that are stable in timestamp order\", dove \"A message $m$ is stable if no future message with timestamp smaller than $TS(m)$ can be received\".\n\nQuesto non risolve però tutti i problemi.\n\n- Esistono diversi eventi con lo stesso timestamp. Questo significa che $p_0$ non ricostruisce esattamente la run, ma solo una possibile run;\n- Eventi concorrenti diventano di fatto dipendenti: secondo questa logica $TS(e_3^3) = 3$ e $TS(e_2^2)=5$, quindi sembra che $e_3^3\\to e_2^2$. Come risolvo?","x":295,"y":-5281,"width":640,"height":588,"color":"4"},
		{"id":"0aa59bf09c992c32","type":"text","text":"# Chandy-Lamport Protocol\n\nEsiste un metodo più leggero rispetto al notificare $p_0$ di ogni evento?\n\nSe è $p_0$ a mandare tutte le `request` per conoscere i LS, non è detto che riesca a ricostruire un GS coerente (il sistema è asincrono). Allora delego ai processi.\n\n- $p_0$ manda una `request` con un `ID` univoco a tutti i processi;\n- Ogni processo che riceve una `request` può fare due cose:\n\t- la inoltra in ***broadcast***, se è la prima volta che la riceve;\n\t- la ignora se l'ha già ricevuta.\n- $p_0$ vede le risposte dei processi, realizzando un ***consistent cut***.\n\t- Come faccio a saperlo? Ipotizziamo non lo sia, e che quindi esiste una coppia di eventi $e\\to e'$ tali che $e\\notin C$ e $e'\\in C$. Segue che il cut deve essere fatto prima di $e$. Dal momento che i canali sono FIFO e il messaggio di `broadcast` è partito prima del messaggio di $e$, arriverà sicuramente prima l'evento di cut rispetto all'evento $e'$, il che implica $e'\\notin C$, da cui la contraddizione.\n\nÈ facile vedere che per $n$ processi vengono inviati $n$ `broadcast`. Il protocollo termina quando tutti i processi hanno ricevuto esattamente $n$ `broadcast`.","x":2240,"y":-3932,"width":665,"height":549,"color":"4"},
		{"id":"26aab522b9b35b7c","type":"text","text":"# Esercizi II - Chandy-Lamport\n\n**Exercise 6:** Show that the Chandy-Lamport Snapshot Protocol builds a consistent global state.\n\n- Let's assume it doesn't, so $\\exists\\,e\\to e'$ such that $e\\notin C$ and $e'\\in C$ (the cut happens before $e$. Since the channels are FIFO, the `broadcast` message containing the \"`please, cut`\" information will be delivered before the message containing the \"$e\\to e'$\" information. So necessarily $e'\\notin C$, thus the contradiction.\n  \n**Exercise 7:** Show that the Chandy-Lamport Snapshot Protocol can build a global state that never happened.\n\n- Since the system is asyncronous, the `take snapshot` messages may take a long time. Let's say $p_2$ is hard to reach, in some way. Then, in the meantime any number of \"non-receive\" events may happen on $p_2$. \n\n**Exercise 8:** What good is a distributed snapshot when the system was never in the state represented by the distributed snapshot? Give an application of distributed snapshots.\n\n- Even though the global state constructed by the `take snapshot` request isn't a \"real\" global state, still it is consistent. This is enough to grant solution to the distributed mutual exclusion problem, i.e. deadlock avoidance.  <span style=\"color:#FFA500\">Giusto??????</span>\n\n**Exercise 9:** Consider a distributed system where every node has its physical clock and all physical clocks are perfectly synchronized. Give an algorithm to record global state assuming the communication network is reliable. (Note that your algorithm should be simpler than the Chandy–Lamport algorithm.)\n\n- <span style=\"color:#FFA500\">Reliable = Synchronous??</span>\n\n**Exercise 10:** What modifications should be done to the Chandy–Lamport snapshot protocol so that it records a strongly consistent snapshot (i.e., all channel states are recorded empty).\n\n- <span style=\"color:#FFA500\">???</span>\n  \n**Exercise 11:** Show that, if channels are not FIFO, then Chandy–Lamport snapshot algorithm does not work.\n\n- Let's consider a simple situation in which $p_1$ sends in close sequence a `take snapshot` broadcast and a `send` message to $p_2$ (let's call this `sending` event $e$). Let's assume that in order to perform the cut $p_2$ needs the `ts` message from $p_1$ (i.e. all the others `ts` to be delivered to $p_2$ are slower than the one from $p_1$). If the channel is not FIFO, then it is possible that $p_2$ first `receives` the message from event $e$ (let's call this event $e'$) and then the `ts`. Then, $e'\\in C$ and $e\\notin C$, but $e\\to e'$, so $C$ is not consistent.\n  \n**Exercise 12:** Let S0 be the global state when the Chandy-Lamport snapshot protocol starts, S be the global state built by the protocol, and S1 be the global state when the protocol ends. Show that S is reachable from S0 and that S1 is reachable from S. Remember that S might not have happened.\n\n- <span style=\"color:#FFA500\">Mi sembra simile all'Ex5, ma non so trovare una prova formale</span>","x":3082,"y":-4136,"width":740,"height":1291,"color":"5"},
		{"id":"9db64449abbcd63a","type":"file","file":"1 - Logic Clock/ChandyLamport.png","x":2240,"y":-3287,"width":665,"height":384},
		{"id":"60fa63697cdb6647","type":"text","text":"# Fault Tolerance","x":-3600,"y":-2052,"width":427,"height":192,"color":"1"},
		{"id":"ae64a718f65ccd25","type":"text","text":"to do exercises don't use notion of topological order","x":-6532,"y":-1509,"width":250,"height":120},
		{"id":"7e4b0290f28c62f1","type":"text","text":"# Security","x":-2980,"y":-349,"width":666,"height":303,"color":"6"},
		{"id":"f10faebf80b04e16","type":"text","text":"# Failure Detection\n\nmi piacerebbe sapere se un processo muore","x":1986,"y":-257,"width":640,"height":211},
		{"id":"3611e372ae09c45c","type":"text","text":"# Failure Detectors\n\nSarebbe bello sapere per certo se un certo $p_j$ è morto o no. Possiamo immaginare che ad ogni processo sia associato un modulo non condiviso, detto ***failure detector*** $D_p$ (i.e. associato al processo $p$), che come un oracolo è in grado di sapere questa informazione. Che proprietà richiediamo a sto coso?\n\n- Completeness - if a process fails, $D$ detects it (i.e. no false negatives).\n\t- è completo un $D$ che dice sempre che $p_j$ è morto (though not very useful);\n\t- Assumiamo che i fail siano tutti fail-stop. Data una run $\\s$ e i processi $crashed(\\s,t)$ e $up(\\s,t)$ definiamo $D_q(t',\\s)$ il set dei processi che il $D$ del processo $q$ sospetta essere crashati al tempo $t'$. Distinguiamo\n\t\t- strong completeness - when some guy crashes, at some point ***all the other guys*** realize it has crashed:$$\\forall\\,\\s\\forall\\,p\\in crashed\\,\\,\\forall q\\in up(\\s)\\quad \\exists\\,t\\,:\\,\\forall\\,t'\\geq t'\\,\\,p\\in D_q(t',\\s)$$\n\t\t- weak completeness when some guy crashes, at some point ***some other guy*** realizes it has crashed:$$\\forall\\,\\s\\forall\\,p\\in crashed\\,\\,\\exists q\\in up(\\s)\\quad \\exists\\,t\\,:\\,\\forall\\,t'\\geq t'\\,\\,p\\in D_q(t',\\s)$$\n- Accuracy - if $D$ tells me some $p_j$ died, then $p_j$ really died (no false positives).\n\t- è completo un $D$ che dice sempre che $p_j$ è morto (though not very useful);\n\t- Anche qui facciamo distinzioni, in ordine di strongness (ogni proprietà implica tutte quelle sotto)\n\t\t- strong accuracy - se p e q sono processi vivi, nessuno dei due sospetta mai la morte dell'altro. $$\\forall\\,\\s\\,\\,\\forall\\,t\\,\\,\\forall\\,p,q\\in up(t,\\s)\\,\\,p\\notin D_q(t,\\s)$$\n\t\t\t- eventual strong accuracy - dopo un certo tempo si ha strong accuracy$$\\forall\\s\\,\\,\\exists t : \\forall\\, t'\\geq t\\,\\,\\forall p,q... poi\\, è\\, uguale$$\n\t\t- weak accuracy - esiste un p vivo di cui nessun q sospetta la morte.$$\\forall\\,\\s\\,\\,\\exists p\\in up \\,:\\,\\forall\\,t\\forall q\\in up(t,\\s)\\,\\,p\\notin D_q(t,\\s)$$\n\t\t\t- eventual weak accuracy - come eventual strong.\n\nOra, a seconda di queste definizioni abbiamo una tassonomia di $D$. (guarda risma fogli)\n\nSe qualcuno mi dà un ***perfect failure detector*** posso risolvere leader election? sì! in ogni momento so chi è vivo, quindi se ad esempio la logica è il più basso ID non posso avere due leader. potrei avere 0 leader se il vecchio leader muore e gli altri processi ancora non lo sanno, ma solo fino ad un certo tempo $t'$ (dopodiché la strong completeness mi assicura che tutti lo sanno).\n- Se risolvo la leader election posso rendere live paxos ovviamente, quindi paxos è live se esiste un perfect D. ma esiste nella realtà? No!\n\nin effetti, se il sistema è asincrono nessuna di queste proprietà e garantita come coppia!!\nse però mi danno un weak completeness D posso costruire uno strong completeness D. Come? Esiste almeno un q che sa del fallimento di qualcuno, quindi gli faccio fare broadcast a tutti gli altri:\n\n```\nDp <- 0  (qua intende dead processes mi sa, o forse no, intende proprio failure detector con l'idea che all'inizio è vuoto e quando si aggiorna prendo le info da lui e aggiorno la mia lista sospetti)\nloop forever\n\tsuspects <- Dp\n\tsend(p, suspects(p)) to everybody  (p è il sender)\nwhen I receive the list (q, suspects(q)):\n\tDp <- Dp U suspect(q) - {q}   (q è chiaramente vivo se mi manda le cose)\n```\n\n- schemino per i vari \"X può essere usato per costruire Y\"\n\nE se il sistema è sincrono? posso costruire direttamente anche il perfect mi sa. posso mandare i `ping` o schedulare dei broadcast ogni dT (heartbeat). se non ricevo un battito entro dT sospetto.\nc'è solo un problema: non conosco il dT. allora uso un dt.\n\n- se dt > dT va tutto bene, il D è perfetto\n- se dt < dT la completeness funziona, ma si rompe l'accuracy.\n\t- se però ricevo il messaggio dopo dT > dt potrei rimuovere q dalla lista, riottengo una qualche accuracy? no, perché l'eventual accuracy (sia essa strong or weak) deve essere vera da un certo punto in poi. qua se dT>dt continuo a fare questo aggiungi-rimuovi all'infinito. posso però fare di meglio. se mi arriva il messaggio dopo il mio timeout semplicemente mi rendo conto che devo aumentare il timeout. diciamo che lo raddoppio. dopo un certo numero di \\*2 riesco a superare dT e ad ottenere eventual strong accuracy. quindi posso costruire \n\nun modello reale per gestire i fail è avere master-slave in cui slave ascolta i battiti del master e lo sostituisce se muore.\n\n- è possibile che la replica si attiva anche se il master è vivo, se questa non ha accuracy\n- o che non si attivi quando il master muore\n\nmeglio mettere la replica in un DB lontano dal main (e.g. se esplode il DB si friggono entrambi, viceversa la probabilità che entrambi diventino unreachable è bassa. in genere li si mette in posti che hanno proprio rischi diversi. rip per quell'ingegnere che ha messo main in una twin tower e replica nell'altra (dice true story))\n\nIRL i sistemi sono sincroni, smettono di esserlo per un tot per via di problemi sulla rete, ma poi si fixano e si torna sincroni. nel protocollo però continuo a raddoppiare dt! questo potrebbe portare a dei tempi di reazione lunghissimi (e.g. la replica si attiva dopo troppo tempo dalla effettiva morte del master). Che ci inventiamo?\n\n- riduco dt se ricevo l'heartbeat dopo meno della metà del dt attuale (diciamo che lo dimezzo);\n- uso dt diversi per diverse coppie di processi.","x":3320,"y":-1007,"width":1540,"height":1712},
		{"id":"f0f698cee3286316","type":"text","text":"# FLP Theorem\n\n`THM` - In an asynchronous system, even only one crash makes deterministic consensus impossible.\n\n`Sketch of Proof` - Assumo che il protocollo esista, e che rispetti le ipotesi del consensus problem. L'obiettivo è quindi che prima o poi devo raggiungere il consenso su uno dei valori in input.\n\n- Consideriamo i casi estremi in cui l'input è di soli zeri (stato 0-valent, nel senso che se raggiungo un agreement, questo deve essere 0. e di solo 1 (stato 1-valent raggiungerò l'agreement su 1). in mezzo mettiamo tutti gli altri input, listati in modo che tra due righe consecutivi flippi solo un bit. (questa roba è un hamiltonian code?)\n- visto che parto da 0-valent e arrivo a 1-valent, deve esistere una coppia di righe di input consecutive tale che la prima è 0-v e l'altra 1-v (perché è deterministico!! quindi ad una data stringa associo con certezza il risultato).\n- Ma queste due righe differiscono solo di un bit, che è quello che determina se il risultato sarà 0 o 1. E se crasha? O se è molto lento? per la proprietà di termination, prima o poi il protocollo deve finire, quindi farà una scelta senza considerare il voto decisivo, quindi potenzialmente è sbagliata.","x":1896,"y":1352,"width":820,"height":499},
		{"id":"59ebe3510cce29a1","type":"text","text":"# Quindi non si può? Come aggirare FLP\n\nCerto che si può, in due modi\n\n- introduco randomization (e.g. bitcoin)((proof of work-proof of state?))\n\t- questa roba funziona assumendo che per $t\\to\\infty$ la probabilità di fare cose giuste e non fare cose sbagliate tende a zero. il problema però è che ci devo andare abbastanza in fretta, altrimenti rischio di rompere la safeness\n- assumo il sistema sufficientemente sincrono, e se non lo è smetto di essere live (e.g. paxos)","x":3763,"y":1444,"width":654,"height":316},
		{"id":"41f7f87c3fbd9530","type":"text","text":"# An Algorithm Randomized Consensus (Ben-Or)\n\nTo generate randomness: seed, random user input or HW temperature fluctuations, hash functions, all of these are given to some algorithm. We don't care, we assume random numbers do exist.\n\nEach process has a value (let's assume it'a single bit) which is input of the algorithm (and it's its preference). Follows Ben-Or protocol, tolerate $t=\\floor*{n-1\\over2}$\n\n```\npreference <-- input\nround = 1\nwhile true do\n\tsend(1, round, preference) to everybody\n\t\t// Quell'1 è il tipo di messaggio\n\t\t\n\twait a number n-t of answer like (1, round, *)\n\t\t// Devo stare attento a questo tipo di comandi, perché se questi messaggi\n\t\t// non arrivano non termino (è scritto per definizione nella tolleranza)\n\t\n\tif received > n/2 messages like (1, round, v) then:\n\t\tsend (2, round, v, ratify) to everybody\n\t\t\t// i.e. posso testimoniare di aver visto una maggioranza su v\n\telse:\n\t\tsend (2, round, ?) to everybody\n\n\twait n-t (2, round, *) messages\n\tif received even a single (2, round, v, ratify):\n\t\tpreference <- v\n\t\tif received more than t (2, round, v, ratify) messages:\n\t\t\toutput <- v\n\telse:\n\t\tpreference <- flip a coin (0 or 1)\n\t\tround ++\n```\n\nsiamo 5, ne muore 1, gli altri sono divisi 2 e 2 tra 0 e 1. Tutti mandano una `send(2, round, ?)`, nessuno vede maggioranze di alcun tipo quindi nessuna `ratify` e tutti quanti flippano un coin.\n\n- se su 5 ne restano solo 3, devono tutti flippare lo stesso numero;\n- in generale perché tutti flippino lo stesso coin (worst case) è $2^n$.\n- può terminare in un solo round? Perché sia così devo ricevere n-t messaggi tutti concordi. anche ammesso siano 5, ne servono 3. Ma la p di ricevere 3 voti uguali è 1/8 (exp, again). But maybe I wait a little bit more (timeout after reaching n-t messages) assuming the system is syncronous enough. So I can see a majority, send a ratify, see a majority of ratify and eventually ratify.\n\ntypical preference in DDB is 1 (`commit`), so conflicts are rare.","x":3660,"y":2080,"width":860,"height":1102},
		{"id":"4c8cb86213c84578","type":"text","text":"# Randomized Consensus Algorithm - Properties\n\n1. At most one value can get a majority in phase 1 (obv);\n2. If some process A sees t+1 `(2, round, v, ratify)` then everybody sees at least one `(2, round, v, ratify)`;\n\t1. Immaginiamo di essere uno degli everybody. Diciamo che A riceve t+1 ratify, quindi B riceve n-t messaggi di phase 2. tra questi ce n'è almeno uno delle ratify.\n3. If every process has received at least one `(2, round, v, ratify)` then every process wil vote for v in round `round+1`.\n\nquesto protocollo non è molto usato, ma è importante a livello teorico perchè mostra che usare la randomness permette di superare il limite di FLP\n\npoi fa un discorso in cui t=1 ma mantiene n generico, penso sia interpretabile come parametro `t` del codice il cui massimo è $t=\\floor*{n-1\\over2}$.\n\ntypically systems are synchronous and fails are fail-recovery by some fix","x":3710,"y":3400,"width":761,"height":452},
		{"id":"fe2a00bfff5ac68d","type":"text","text":"FLP ci dice che se esiste almeno un crash è impossibile risolvere il consenso con protocolli deterministici. è però possibile farlo tramite algoritmi probabilistici.","x":2127,"y":2080,"width":358,"height":175},
		{"id":"e6fd42bc1097166e","type":"text","text":"exam\n\nmidterm 3 esercizi (+ altro?)\n\nesempio ex\n\n$C_1$ e $C_2$ sono due CC. Show that $C_1\\cap C_2$ is consistent.\n\n$C_1\\cap C_2 \\subseteq C_1$, which is consistent.\n\ndevi usare la definizione per cui un consistent cut è \"se e precede e', ovvero $e \\to e'$ ecc...\n\nanche l'unione è consistente.","x":4100,"y":-3659,"width":430,"height":540,"color":"1"},
		{"id":"ffd7c9469255f9c8","type":"text","text":"# Atomic Commits\n\nFare ***commit*** significa aggiornare lo stato di un DB in modo ***irreversibile***.\n\nIn un DS, ogni DB deve concordare su ogni `commit` che viene eseguito. Se non c'è ***consenso*** unanime, la transazione viene rifiutata (i.e. ***abort***). \n\nIl singolo DB deve possedere sempre una ***copia coerente*** del DB distribuito, i.e. una versione della sequenza di modifiche sulla quale tutti concordano.\n\nQuesta necessità collettiva è più importante del `commit` che vorrebbe eseguire (e far eseguire a tutti) il singolo DB.\n\nDue criteri fondamentali per valutare i protocolli:\n\n- ***safeness*** - quanto il protocollo non permette copie incoerenti (in generale, è un \"il protocollo non deve fare cose sbagliate\");\n- ***liveness*** - quanto il protocollo \"fa qualcosa\" di buono, prima o poi.\n\nUn protocollo che non fa nulla è safe, ma non live. In generale si richiede ***massima safeness cercando di ottimizzare la liveness*** (i.e. fare in modo che il qualcosa di buono avvenga il prima possibile). ","x":-2968,"y":1335,"width":640,"height":532,"color":"6"},
		{"id":"c6a15cbad3909019","type":"text","text":"# Paxos Liveness Problem\n\nPaxos ha un problema con la liveness: anche se i messaggi non vengono persi, è possibile che ci mettano troppo tempo ad arrivare, il che li porterà ad essere ignorati.\n\nPoniamo che le `accept(1, x)` siano così lente da essere ad un certo punto date per perse (i.e. scade un timer). Un altro `P` invierà una `prepare(2)` e gli `A` risponderanno con una `promise(2, -, -)`. Quando le `accept(1, x)` arriveranno agli `A`, questi le ignoreranno in quanto nel frattempo hanno promesso di non partecipare a nessun `round` inferiore a `2`.\n\nQuesto può andare avanti ad oltranza, ed ogni `round` continuerà ad inibire il precedente.\n\nSoluzioni sono un timer adattivo (cf. Failure Detector) o un coordinatore (c.f. Multi-Paxos).","x":-3024,"y":4271,"width":753,"height":339,"color":"4"}
	],
	"edges":[
		{"id":"313f73befec0f227","fromNode":"be570a1920f04449","fromSide":"right","toNode":"23a2f6367ee648bd","toSide":"left"},
		{"id":"ca3b4cbd15b08962","fromNode":"f82b77ddb75926d4","fromSide":"top","toNode":"0ebec2401a36c3ef","toSide":"bottom"},
		{"id":"9a87cb3a68c4a99b","fromNode":"f82b77ddb75926d4","fromSide":"bottom","toNode":"48ebe9989920f9da","toSide":"top"},
		{"id":"867ed467e77ccc6f","fromNode":"f9d9980fa75657d5","fromSide":"top","toNode":"3458d02a2952e9c7","toSide":"bottom"},
		{"id":"4fdd279d6063830f","fromNode":"ffd7c9469255f9c8","fromSide":"bottom","toNode":"00258a5944b2d60e","toSide":"top","label":"Cooperative Protocol\nfor\nFail-Stop Processes"},
		{"id":"c8cb8a307762db2a","fromNode":"4fd496a7b455c76c","fromSide":"top","toNode":"e0ba5ac0341a437d","toSide":"bottom"},
		{"id":"47e11942324f4e4b","fromNode":"3458d02a2952e9c7","fromSide":"right","toNode":"db4411d94cf78039","toSide":"left"},
		{"id":"8176ca8baa3df1d8","fromNode":"db4411d94cf78039","fromSide":"right","toNode":"61752071951b1015","toSide":"left"},
		{"id":"021050bd6bda60c5","fromNode":"db4411d94cf78039","fromSide":"top","toNode":"6dcdd47739ab639e","toSide":"bottom"},
		{"id":"1ebd8a2d6587351a","fromNode":"61752071951b1015","fromSide":"top","toNode":"0b9a16cefb33b26c","toSide":"bottom"},
		{"id":"7f44ba9628882881","fromNode":"61752071951b1015","fromSide":"right","toNode":"0aa59bf09c992c32","toSide":"top"},
		{"id":"0638f572c4714020","fromNode":"0aa59bf09c992c32","fromSide":"bottom","toNode":"9db64449abbcd63a","toSide":"top"},
		{"id":"289181e565bb286c","fromNode":"ffd7c9469255f9c8","fromSide":"left","toNode":"c2b4396b3b84e4de","toSide":"right"},
		{"id":"32f109e47c0d8cff","fromNode":"ffd7c9469255f9c8","fromSide":"bottom","toNode":"93212aed82c21df9","toSide":"top","label":"Cooperative Protocol\nfor\nFail-Stop Processes"},
		{"id":"4fdde0886f2be644","fromNode":"ffd7c9469255f9c8","fromSide":"bottom","toNode":"d582dc2a1cfd5670","toSide":"top","label":"Recovery Protocol\nfor\nFail-Recovery Processes"},
		{"id":"dae91a337ba0d6a9","fromNode":"93212aed82c21df9","fromSide":"left","toNode":"d582dc2a1cfd5670","toSide":"right"},
		{"id":"7dcbbb6e76747b67","fromNode":"93212aed82c21df9","fromSide":"right","toNode":"00258a5944b2d60e","toSide":"left"},
		{"id":"649439e90b638f90","fromNode":"00258a5944b2d60e","fromSide":"bottom","toNode":"98ff8ea31a5611b2","toSide":"top"},
		{"id":"64a03807a065f7f2","fromNode":"98ff8ea31a5611b2","fromSide":"left","toNode":"e69724a397d8eb9b","toSide":"right"},
		{"id":"7a26fd3d9ee0832e","fromNode":"98ff8ea31a5611b2","fromSide":"right","toNode":"df6f8d2bb2d685d0","toSide":"left"},
		{"id":"7c33e5114002150d","fromNode":"98ff8ea31a5611b2","fromSide":"bottom","toNode":"7bd4339e9dc5832c","toSide":"top"},
		{"id":"adbb5bc4ec67a3c4","fromNode":"7bd4339e9dc5832c","fromSide":"left","toNode":"ee7f51e5be213a53","toSide":"right"},
		{"id":"ebaecf82de4e7c0a","fromNode":"db4411d94cf78039","fromSide":"top","toNode":"18e22186b729910c","toSide":"bottom"},
		{"id":"939cc18ee4f9b7a0","fromNode":"4043ace7e5011791","fromSide":"right","toNode":"8d15b0299f305f56","toSide":"left"},
		{"id":"a7ede2cc8bbf9706","fromNode":"48ebe9989920f9da","fromSide":"left","toNode":"ffd7c9469255f9c8","toSide":"right"},
		{"id":"ea39b3505731eb47","fromNode":"48ebe9989920f9da","fromSide":"right","toNode":"f0f698cee3286316","toSide":"left"},
		{"id":"747cfde93541c774","fromNode":"8876ea52fed47dda","fromSide":"left","toNode":"570439671731e83f","toSide":"right"},
		{"id":"051684e10cf8eabf","fromNode":"df6f8d2bb2d685d0","fromSide":"top","toNode":"c604e7d9b7779409","toSide":"bottom"},
		{"id":"7adc25763994d5ba","fromNode":"df6f8d2bb2d685d0","fromSide":"bottom","toNode":"4043ace7e5011791","toSide":"top"},
		{"id":"6913e91fdaba2996","fromNode":"98ff8ea31a5611b2","fromSide":"left","toNode":"c6a15cbad3909019","toSide":"right"},
		{"id":"0e67dca1e55f4b4d","fromNode":"df6f8d2bb2d685d0","fromSide":"right","toNode":"6a09d757776030a8","toSide":"left"},
		{"id":"fee2acc773de912c","fromNode":"c604e7d9b7779409","fromSide":"right","toNode":"6a09d757776030a8","toSide":"left"},
		{"id":"76c2e8e408e77475","fromNode":"48ebe9989920f9da","fromSide":"bottom","toNode":"6a09d757776030a8","toSide":"top"},
		{"id":"4b4428cddd937684","fromNode":"1a4ae7dd4ea788ae","fromSide":"bottom","toNode":"5aaddbdcdf2d4e68","toSide":"top"},
		{"id":"f9f5e6f2f0e0e753","fromNode":"4043ace7e5011791","fromSide":"bottom","toNode":"1a4ae7dd4ea788ae","toSide":"top"},
		{"id":"a5f4a9266a1fd4a3","fromNode":"e69724a397d8eb9b","fromSide":"left","toNode":"d63911fe834dbe22","toSide":"right"},
		{"id":"d38e8054fe033cae","fromNode":"f0f698cee3286316","fromSide":"right","toNode":"59ebe3510cce29a1","toSide":"left"},
		{"id":"5f19fa8cc936087d","fromNode":"59ebe3510cce29a1","fromSide":"bottom","toNode":"41f7f87c3fbd9530","toSide":"top","label":"non-deterministic (random) protocols"},
		{"id":"ff22c76ec9407ce2","fromNode":"0ebec2401a36c3ef","fromSide":"top","toNode":"f9d9980fa75657d5","toSide":"bottom"},
		{"id":"d0272266083b0bf2","fromNode":"96363645d77cc1f4","fromSide":"right","toNode":"0aa59bf09c992c32","toSide":"left"},
		{"id":"1fc47a9dd4bbd1c9","fromNode":"0ebec2401a36c3ef","fromSide":"top","toNode":"96363645d77cc1f4","toSide":"bottom"},
		{"id":"d2b00c0ec97e0f58","fromNode":"fbc1ef28f3589012","fromSide":"left","toNode":"f9d9980fa75657d5","toSide":"right"},
		{"id":"40e22260f52fa8c5","fromNode":"fbc1ef28f3589012","fromSide":"right","toNode":"96363645d77cc1f4","toSide":"left"},
		{"id":"95e7260ddb880781","fromNode":"0ebec2401a36c3ef","fromSide":"top","toNode":"4fd496a7b455c76c","toSide":"bottom"},
		{"id":"72b926d2089ac6a7","fromNode":"e0ba5ac0341a437d","fromSide":"top","toNode":"fbc1ef28f3589012","toSide":"bottom"},
		{"id":"48199d93340adc2d","fromNode":"fbc1ef28f3589012","fromSide":"top","toNode":"ab73850207f6a30a","toSide":"bottom"},
		{"id":"bd0160ae5a7cac8e","fromNode":"0ebec2401a36c3ef","fromSide":"bottom","toNode":"f10faebf80b04e16","toSide":"top"},
		{"id":"4e9dae26103db5cb","fromNode":"f82b77ddb75926d4","fromSide":"right","toNode":"f10faebf80b04e16","toSide":"left"},
		{"id":"9f75d962fea97867","fromNode":"f10faebf80b04e16","fromSide":"right","toNode":"3611e372ae09c45c","toSide":"left"},
		{"id":"891e2d3482cc0679","fromNode":"f10faebf80b04e16","fromSide":"bottom","toNode":"f0f698cee3286316","toSide":"top","label":"FLP assume che non esista un perfect failure detector "},
		{"id":"35ac5a7012998eea","fromNode":"59ebe3510cce29a1","fromSide":"top","toNode":"3611e372ae09c45c","toSide":"bottom","label":"perfect failure detector"}
	]
}