{
	"nodes":[
		{"id":"f82b77ddb75926d4","type":"text","text":"# Distributed Systems\n\nUn DS è un sistema in cui $n$ processi cooperano per completare una task.\n\nUn processo è definito da una ***timeline*** e dagli ***eventi*** (e.g, modifica, decisione) che avvengono su di essa. Quando si verifica un evento sul processo $i$, questo può informare tramite un ***messaggio*** il processo $j$.\n\nLo scambio di messaggi avviene tramite canali FIFO, assumendo che questi siano ***asincroni*** (i.e non esiste un tempo limite entro il quale sono certo che il messaggio arriverà) e ***unreliable*** (i.e. i messaggi si possono perdere).","x":295,"y":-303,"width":640,"height":303,"color":"6"},
		{"id":"b2214078d2cc0c48","type":"text","text":"# L4 (Two Phase Commits)\n\nCINECA?\nnello schemino in cui (nome, voto) viene splittato in (nome) e (voto) su 3 computer diversi serve ovviamente che ci siano protocolli locali tipo login e rollback localmente su ognuno di essi, ma è un sistema distribuito in cui le operazioni {splitta nomevoto, salva nome, salva voto, cancella nomevoto} deve essere atomico, cioè o tutto o niente. Se non implemento nessun protocollo per assicurarmi di ciò, rischio un'inconsistenza (e.g. il nome viene perso, posso votare di nuovo).\n\nquesta roba si realizza con two phase commit: quando parte la transazione, o tutti i siti la fanno o non la fa nessuno.\n\nassumiamo che i processi siano either crash fail, crash stop or crash recovery. sistema asincrono, no byzantine nodes (?).\n\n- A1 - if processes reach a decision (either commit or abort) it must be the same for all of them;\n\t- versione alternativa scritta sul libro: all processes that reach a decision, reach the same one\n- A2 - A process cannot reverse its decision after reaching one;\n- A3 - The commit decision can be reached only if all processes voted `yes`.\n\t- non dico iff perché voglio lasciarmi aperta la pista di abortire anche se tutti hanno votato `yes`.\n\nnota che anche un protocollo che non fa letteralmente nulla soddisfa queste proprietà, quindi mi servono condizioni aggiuntive.\n\n- safety propriety - nothing bad should happen\n- liveness property - \"ok, but do something\"\n\nse hai solo safety è ok se non fai nulla (e.g. A1 2 3), se sei pazzo sei liveness. tipicamente richiediamo di essere safe ma essere as live as you can.\n\n- A4 - ***If there are no failures*** and all processes voted `yes` then decision is `commit`.\n","x":-11040,"y":8160,"width":880,"height":761},
		{"id":"c60727cb76e10ee4","type":"text","text":"\nin un processo del genere solitamente c'è un coordinatore e dei partecipanti. questi ultimi votano, il primo no (oppure sì, not a problem if coordinator is also participant). quindi: sistema distribuito è così\n\n- coordinatore C manda `voteRequest` ai partecipanti P\n- i quali decidono `yes` or `no`\n\t- se votano `no` fanno `abort`, e ciò è safe.\n- e mandano il voto a C\n\t- e se il messaggio si perde o è molto lento? C deve essere sicuro di avere tutti i voti `yes` per, in caso, fare commit. Quindi è safe.\n- se C riceve TUTTI `yes`, allora può decidere di fare commit. sta di fatto che qualcosa decide\n- allora manda la decisione a tutti, i quali eseguono.\n\nse il protocollo non ha failures e sincrono (i.e. i messaggi arrivano in tempi utili) è perfetto. In una situazione reale non è così.\n\nma posso mai aspettare per sempre? ovviamente no, metto un timeout al termine del quale mando a tutti una decisione di abort.\n# Failures\n\n- se si perde `voteRequest`?\n\t- la prima volta al partecipante scade il timeout e chiede a C di mandare `voteRequest`\n\t- la seconda fa un `abort`.\n- se si perde il voto?\n\t- C manda di nuovo la request\n\t- la seconda volta che scade il timeout decido di abortire.\n\t\t- eh ma magari arriva un attimo dopo ed erano tutti `yes`. \"eh, such is life\"\n- se si perde la decisione che C manda a P?\n\t- se ho votato no, easy, tanto ho già abortito\n\t- se ho votato yes non è per niente safe abortire, quindi mi tocca aspettare. magari gli rimando un \"`aoo sta decisione??`\" sperando che arrivi. e magari lo chiedo tante volte e nessuno mi risponde. eh. magari C è morto. allora magari chiedo agli altri P se hanno ricevuto la decisione (assumo che non mentano). gli altri P possono rispondere in due modi\n\t\t- ho ricevuto la decisione, fine\n\t\t- non l'ho ricevuta, e\n\t\t\t- ho votato no - P fa un `abort`\n\t\t\t- ho votato `yes`. cazzo. chiedo a un altro. se chiedo a tutti e TUTTI hanno votato `yes` e NESSUNO ha ricevuto la decisione, tutti fanno `abort`.\n\nin sostanza, two-phase commit è safe ma non live. scopriremo che nessun protocollo è sia safe che live. c'è un teorema di impossibilità di avere entrambe. per questo chiediamo protocolli safe e as live as can be. è anche un cooperative protocol\n","x":-11040,"y":8990,"width":880,"height":1180},
		{"id":"be570a1920f04449","type":"text","text":"\n# Log\n\ntutto questo ha senso se i processi sono fail-stop. e se sono fail-recover? quando recoverano perdono memoria! Tranne? Il ***LOG*** (sulla singola macchina. ovviamente puoi farlo sul DS ma servirebbe un TPC per il log condiviso, il che significa provare a risolvere il problema con il problema).\n\n\"eh ma il log sta su disco, può fallire\". Ho capito, ma mica puoi risolvere tutto. magari fai più dischi ma muoiono tutti. non solo. quando chiamo una syscall per scrivere su disco, magari questa scrive in RAM. allora devo forzare a scrivere su disco (tipo con una `flush`, su linux si chiama `fsync`?). ma magari il disco ha un buffer e salta la corrente, perdo il buffer. si usa un disco solo per il log e lo configuro in modo che non usi il buffer. (ha fatto un commento su journaled filesystem? non ho sentito!).\n\nil punto di tutto sto pippone è che mi serve assumere che il log sia safe in modo che quando recovero leggo il log.\n\ndetto questo. è meglio scrivere sul log e poi mandare il messaggio o viceversa? in entrambi i casi potrei morire nel mezzo! ma è meglio la seconda scelta. se rivivo e leggo sul log che ho mandato il messaggio non lo manderò mai. meglio mandare e loggare, alla peggio mando messaggi duplicati. però puoi ricevere voti di una request che non sai di aver mandato. tu nel dubbio vedi sta roba e dici `nono che è sta roba, abortite tutti`. mi sa che posso scegliere entrambi a seconda delle necessità\n\nstesso problema per P. mando il voto e loggo o viceversa?\n- voto e loggo, morendo in mezzo - quando rivivo non so che ho votato. né posso rimandare il voto, perché potrebbe essere diverso. \n- viceversa - quando rivivo penso di aver mandato il messaggio, ma non posso essere sicuro, quindi nel dubbio lo rimando.\nquindi se ho votato no abort, altrimenti aspetto.\n\ncapiamo che in pratica se voto no non cambia niente, se voto `yes` è meglio log e poi send, alla peggio mando lo stesso messaggio due volte perché so qual è.\n\nStesso problema per C quando manda la decisione. prima safety, quindi\n- log then send is ok, if you die in the middle you see the decision, not sure if has been sent, send it again\n- send then log - safe only if it's an abort.\n\nse quando vedi la decisione presa la rimandi, se non vedi nulla mandi un abort.\n\nyou should be very careful what you log and when.","x":-10983,"y":10296,"width":753,"height":1804},
		{"id":"23a2f6367ee648bd","type":"text","text":"è anche possibile che C faili mentre manda i messaggi, tipo che lo dice a qualcuno e poi crasha. legge il log e rimanda a tutti.","x":-10158,"y":11139,"width":345,"height":118},
		{"id":"604f69980144ca26","type":"text","text":"# Paxos\n\nÈ generale come Two-Phase Commit, ma ha più fault tolerance e più liveness.\n\nProtocollo maggioritario in cui dividiamo i nodi in tre categorie:\n\n- Proposers (`P`) - non una buona idea averne uno solo, se muore il protocollo è morto. non serve una maggioranza però, quindi ne basta uno vivo.\n- Acceptors (`A`) - Sono i nodi più importanti, perché decidono a maggioranza su quale valore deve fare `commit` l'intero sistema;\n\t- Se ci sono $n$ `A`, la tolleranza del protocollo è il fail di $f=\\text{floor}({n-1\\over2})$;\n\t\t- se ci sono più di $f$ failures, il protocollo si ferma (so it's safe)\n\t- In genere $n$ è dispari, perché la tolleranza è la stessa per $n-1$ e a sto punto ne metto uno in più.\n\nin molte applicazioni pratiche, ogni nodo è sia `A` che `P` che `L`.\n\na inizio protocollo vengono distribuiti staticamente tra i `P` un infinito numero di round in modo che nessuno abbia lo stesso.\n\nprimo messaggio: `prepare(1)`. scelgo il round in modo che sia maggiore dei precedenti, se sono anche `A` e so a cosa siamo arrivati sono facilitato. In generale provo a massimizzare la liveness, perché il protocollo in sé già mi garantisce safeness.\n\ngli `A` raccolgono le prepare, votano e mandano le `promise`.\n\ni `P` raccolgono le promise e decidono quale valore deve essere accettato.\n- se si raggiunge il ***quorum*** $Q\\geq n-f$ (i.e. un numero $Q$ di di `A` ha votato lo stesso valore `x`) allora mandano una `accept(i, x)` agli `A`;\n\nQuando gli `A` ricevono una `accept` mandano una `learn` ai `L`, che in pratica è il voto.\n\n- se i `L` vedono un numero $\\geq Q$ di `learn` con lo stesso valore e round, allora fanno `commit` e il valore diventa chosen\n\nl'obiettivo di paxos è che sia impossibile scegliere due valori diversi.\n\nnon è live manco senza failures e con messaggi sincroni. in pratica lo è a meno di non essere unlucky.","x":-9280,"y":10296,"width":753,"height":1030},
		{"id":"0351c84f97899850","type":"text","text":"# L5 (Paxos)\n\nconsesus problema dell'agreement (in questo caso commit o abort). stessa cosa risolta nella blockchain (devi sapere se è accettata o no). Two-phase commit è il primo step verso ***paxos*** (perché basta un solo nodo offline e tutto muore). paxos è il nome di una possibile isola greca (in realtà esiste, ma lamport non lo sapeva) perché racconta sta storia di sti tizi greci con un piccolo parlamento di non professionisti. lo manda così e glielo rifiutano. alla fine dopo lunghe lotte lo accettano ma aggiungono delle note esplicative (le fa Keith Marzullo). nessuno lo capisce, allora pubblica un articolo chiamato \"paxos made easy\". nessuno lo capisce. poi ci sta \"paxos for dummies\".\n\nogni nodo ha un DB. in google filesystem every file is replicated in 5 different places. replication system, paxos is a replication system. se tutto viene fatto in parallelo tra 5 server, ognuno di essi deve avere una copia coerente. sui google doc possono lavorare più persone! quindi serve consenso sull'ordine delle singole operazioni.\n\n- serve consenso non sulla serie di valori, ma sul singolo valore. dobbiamo tollerare i crash failures. sistema asincrono con n processi.\n\nci sono n processi più importanti, detti acceptors. nella storia di lamport sono i membri del parlamento\n\npoi ci sono i proposers. \n\nha senso chiedere che tutti gli acceptor debbano votare su ogni singolo valore perché questo sia accettato, ma così torno al two-phase con i relativi problemi. allora facciamo che serve la maggioranza (perché se così non fosse due proposte opposte possono essere approvate contemporaneamente). se serve maggioranza e ci sono due proposte parallele almeno un computer sarà in entrambe le commissioni, e permetterà coerenza. tolerates $floor({n-1\\over2})$ errors?\n\n- primo messaggio - un proposer manda la proposta con un messaggio di `prepare`. perché non è una `voteRequest`? perché che ne so se mi sono appena svegliato e gli altri proposer hanno proposto altro?\n\t- visto che non abbiamo un orologio usiamo il round. ad ogni processo è preassociato un set di numeri che sono i round in cui può iniziare la conversazione. sono statici e mai condivisi (e.g. se il processo 1 ha il round 1, nessun altro ha il round 1).\n\t- a questo punto capiamo che la `prepare` è una `prepare(3)`, cioè `preparati al round 3`\n- l'acceptor risponde con una `promise(3, lastround in cui ho votato, valore per cui ho votato)` in cui comunicano al processo che ha mandato la `prepare`\n\t- cosa hanno votato l'ultima volta\n\t- quello in mezzo è il numero dell'ultimo round per cui ho votato. posso rispondere `null null` se non ho ancora votato\n\t- perché si chiama promise? perché significa `from now on i will never partecipate to a round smaller than 3`\n- Se il proposer riceve n messaggi in risposta, ha un quadro chiaro della situazione. se la maggioranza risponde di aver votato $5$, è inutile che io proponga $7$, tanto non vincerà mai! (Paxos è per il singolo valore!!).\n\t- anche se volevo proporre 7, propongo 5, o in generale x. (?)\n- mando una `accept(3, x)`, cioè al round 3 dico (agli acceptors? o al proposer? A che mi serve sto passaggio??) che ho accettato x\n- gli acceptors mandano un `learn(3, x)` a tutti gli altri (che di base è il loro ***voto***). Se un learner riceve una maggioranza di voti per quel round (altrimenti no?)\n\t- se sono all'inizio e nessuno ha votato parto con una `accept`.\n\nse dopo il round 3 parte il round 1, gli acceptors non rispondono. allora magari il processo prova con il prossimo numero nel suo array statico, che magari è 4 (se sono 3 processi)\n\nse al round 2 c'era la maggioranza (3/5) ma uno degli acceptors muore, arrivano solo 4 promises, magari 2 voti e due nulli. che faccio? non blocco il protocollo, ma non accetto il nuovo valore.\n\nnon è importante che il processo faccia approvare il proprio valore, ma che tutti concordino.\n\nsta dicendo una cosa circa il fatto che non è possibile cambiare il voto una volta che una maggioranza ha preso una decisione\n\nè possibile che nel round 2 qualcuno voti x ma poi nel round 3 la maggioranza vota y. se la minoranza ha votato x ma al round dopo le loro promise si perdono, il nuovo proposer riceve solo dei \"non ho votato\". è totalmente safe fare accept y.\n\nse le accept ci mettono troppo, il prossimo round comincia. gli acceptor promettono di non partecipare a round inferiori. quando le accept arrivano, vengono rifiutate.\n\nnon è live, ma tanto è impossibile esserlo essendo safe. in pratica lo è, nel senso che quando lo implementi i segnali di norma non rallentano tutti insieme, o simili.\n\nnon tollera malicious activities, quindi non usato nelle blockchain.\n\nfino a qualche centinaio di acceptor paxos va bene, ma blockchain ne ha migliaia.","x":-8516,"y":9911,"width":850,"height":1800},
		{"id":"0084479ff459b8fb","type":"text","text":"# L5 (Paxos II)\n\n`THM` - If acceptors vote for `x` in round $i$, then no value $x'\\neq x$ can be chosen in previous rounds.\n\n`Proof` - Se ho votato (i.e. mandato una `learn`) significa che ho ricevuto un `accept`. Se esiste una `accept` significa che è stato raggiunto $Q$. Procediamo per induzione sul round $i$.\n- Base induttiva (`round 0`) - Il teorema è ovvio;\n- Assumiamo vero per $i-1$ e dimostriamo per $i$ - $Q\\subseteq A$. Diciamo che i facenti parte del $Q$ abbiano votato l'ultima volta al più al round $j$. Segue che nessuno dei $Q$ ha votato tra $j+1$ e $i-1$. Se $j$ valesse $-1$ avremmo finito la dimostrazione. Diciamo che al round $j$ all'interno di $Q$ almeno 1 deve aver votato per $x$ (per definizione è il valore associato al maxround). potrebbe essere l'unico? Sì, perché tutti gli altri accept potrebbero essere stati persi. Ma se almeno uno ha votato x significa che un proposer gli ha detto così. quindi nessun altro `A` può aver scelto $y\\neq x$. Ma posso usare l'ipotesi induttiva, quindi nei round $<j$ nessun valore $x'\\neq x$ può essere stato scelto. La chiede all'esame :)\n\nSe tutte le learn si perdono al round i, dipende\n- se c'era Q, quel valore prima o poi verrà approvato (segue da `THM`)\n- altrimenti \n\nin ogni caso mi serve un timeout sul learner, in modo che dicano ai `P` \"inizia un nuovo round\".\n\nse modifico il protocollo in modo che $A_i$ non cambi mai il suo voto, si rompe tutto perché potrebbe non esiste mai più una maggioranza...\n\nha senso dividere i proposer dal resto nel paradigma client-server.\n\ntutta sta roba non funziona se un qualsiasi nodo è byzantine\n\nOra, in genere vogliamo approvare una sequenza di valori (non uno solo). Allora runno diverse instance. di fila? Ma no, viva le pipeline.\n\n- la `prepare(i)` diventa una `prepare(p, i)`, in cui `p` è il numero dell'istanza.\n\t- posso ottimizzare mandando una singola prepare che dice `prepare(1-1000, 1)` ovvero preparati al primo round di 1000 istanze;\n\t- stessa cosa per la prima `promise`, che dice di essere pronto per le istanze `1-1000`, round `1`;\n\t\t- possiamo chiamarle `super-prepare` e `super-promise`\n\t- e le `accept`? Il sistema è solo pronto alle istanze, non le ha ancora ricevute. Quando `P` riceve $T_1$, cioè il primo valore da approvare (tipo dal client) è sufficiente mandare una `accept(1, 1, T1)` ovvero `accept(istanza, round, value)`. Se ne ricevo un'altra posso mandare direttamente `accept(2, 1, T2)`? Sì, perché l'importante è l'univocità della coppia `istanza, round`.\n\t- finora abbiamo assunto che i `P` non fossero in competizione. se c'è un altro `P` a ricevere delle T che faccio?\n\t\t- il secondo proposer si prenota le istanze `1001-2000`. sta roba non è ottimale, perché magari approvo le istanze `1, 2, 1001` ma da qui in poi non posso fare commit perché devo aspettare tutte le istanze tra `2` e `1001`.\n\t\t- altra opzione è che tutti mandino le T al primo che ha fatto le `1000` richieste, che Lamport chiamava ***coordinator***.\n\t\t\t- e se muore? se ne fa un altro , \"come il papa\"\n\t\t\t- questo è il cosiddetto leader problem, che è difficile come il consenso. la soluzione è che qui non deve funzionare per davvero, perché alla peggio Paxos funziona ma non ottimizzato\n\t\t\t- se per magia esistesse un perfetto protocollo di scelta del leader avremmo un solo proposer. in quel caso paxos diventa live (l'unico problema in questo senso era infatti che ci fossero tanti P)","x":-7608,"y":10071,"width":800,"height":1552},
		{"id":"4d644fa5db15f42d","type":"text","text":"# Fast Paxos Safety Theorem\n\nIn FP, if acceptor a votes\\* for x in round j, then no $x'\\neq x$ can be chosen from previous rounds.\n\nEsattamente come Paxos, per induzione.\n\nin round i qualcuno ha votato xm e c'è un quorum $Q_i\\geq$. j è il maxround.\n\nif $j>0$:\n- if $\\exists\\, x:$ ha abbastanza voti in Q da poter raggiungere una maggioranza contando i voti fuori da Q (che non conosco)\n\t- cioè x compare come uno dei voti ed ha questa proprietà, in questo caso è ovvio\n- ***or*** $x$ è l'unico valore che compare nelle promise:\n\t- se c'è solo x, è ovvio che nessun altro può avere la maggioranza, uso l'ipotesi\n- then choose $x$\n- else:\n\t- choose any value (che comunque può essere l'$x$ del teorema. se sono in questo caso significa che questo x non ha nessuna delle proprietà precedenti)\n\ne se ci sono due valori y e z? il mi sta dicendo che nessun valore diverso da y e da z è stato votato in precedenza. Si escludono a vicenda, quindi nessun valore può essere stato votato in precedenza. mando una accept con y, z oppure il mio valore. qualunque cosa io scelga è comunque safe, e diventa la x del teorema (è ovvio quindi che se scelgo x in questo modo è perché nessuno aveva una maggioranza, i.e. nessun x' era stato votato prima di questo round j)","x":-5632,"y":10790,"width":580,"height":817},
		{"id":"1955f0b3fa813eab","type":"text","text":"# Fast-Paxos\n\ne se una volta che gli acceptors  sono pronti alle prime 1000 istanze ogni volta che arriva un valore ad un qualsiasi proposer questo manda direttamente una accept senza passare per il coordinator?\n\npotenzialmente più P fanno questa cosa in parallelo, ed entrambi mandano `accept(1, 1, X)` dove `X` è diverso a seconda del P. Questo succede perché tutte le prepare sono state mandate per il round 1! Potrebbe succedere che su 5 acceptors\n- due voti si perdono, non so il voto\n- uno ha votato x1\n- uno x2\n- uno non ha votato\nora, io proposer che vedo queste promise quale valore scelgo? non posso mai essere sicuro di un valore safe, perché se scelgo x1 magari tutti gli altri hanno votato x2 e rompo tutto (e viceversa)\n\nuna soluzione è aumentare il quorum a 2/3 anziché 50%+1. Mettiamo che gli A sono così divisi\n\n- promise che non arrivano sono 1/3\n- tra le promise che arrivano\n\t- 1/3 vota x1 = A\n\t- 1/3 vota x2 = B\n\ntolleranza di fast-paxos = $f'=floor({n-1\\over3})$ e il quorum è $n-f'=Q$. Grazie alla funzione floor è safe la scelta di qualsiasi $X_i$ tale che nessun altro $x_j$ di cui mi arriva una promise con il voto non possa mai avere la maggioranza. Il caso peggiore è appunto 1/3 e 1/3, e chiaramente scelgo un numero di acceptors (divisibile per 3) + 1 (e.g. 7). in questo modo almeno uno tra A e B è strettamente minore di 1/3, quindi posso con certezza scegliere un valore safe.\n\ndevo dire al sistema che sto facendo un fast round. il protocollo è infatti\n\n- prepare(1-n, 1)\n- promise(1-n, 1, x, y) (cosa sono x e y??)\n- accept_any(1-n, 1)\n\t- questo prepara per l'accept proveniente da più proposer. bisogna farlo solo quando è safe accettare davvero qualsiasi valore, tipo all'inizio quando nessuno ha votato.\n\nse nessuno ha una maggioranza nel quorum (i.e. se A=7 e Q=5, se nessun valore ha almeno 3 voti) è safe scegliere un qualsiasi valore, quindi il proposer potrebbe anche far partire una `accept_any` per far partire un altro fast-round in cui è possibile proporre qualsiasi valore? Ha senso? non molto! ha senso proporre un valore che ha una maggioranza relativa. Può avere senso che lo si faccia se vedi 5 voti diversi, altrimenti si procede con paxos normale.\n\nin generale fast paxos è più lento di paxos. wow. quand è che invece performa bene?\n\n- non ci sono molti proposer\n- poche transazioni\n- in generale, pochi conflitti all'interno del quourm.\n\t- server molto lontani tra loro non sanno lo stato l'uno degli altri, quindi è più probabile avere conflitti. ha senso in small cluster è quando c'è poco load.\n\nin pratica non è molto usato: riduce la tolleranza per un boost in velocità che spesso con i conflitti neanche esiste (i sistemi reali sono spesso in overload)\n\nspesso inoltre non c'è davvero bisogno di aspettare l'ordine giusto per il commit","x":-6808,"y":10170,"width":797,"height":1461},
		{"id":"48ebe9989920f9da","type":"text","text":"# Consensus Problem\n\nAbbiamo $n$ processi, ognuno dei quali propone a tutti gli altri una decisione. Tutti i processi devono concordare su una singola decisione, e ratificare (i.e. tenere in memoria) solo e soltanto quella.\n\nIl consensus problem è un modello al quale si riconducono molti problemi che riguardano i DS. In generale si richiedono tre proprietà:\n\n- ***Agreement*** (for ***safeness***) - If 2 non-faulty processes decide a value, it must be the same one;\n- ***Validity*** (for ***non-triviality***) - Il valore scelto è uno degli input, ovvero è stato proposto da uno dei processi;\n\t- Avoids trivial protocols like \"`whatever the input, choose 17`\".\n- ***Termination*** (for ***liveness***) - Prima o poi il protocollo deve finire (i.e. prima o poi deve decidere qualcosa).\n\nVista la difficoltà del problema, non ci si cura della complessità. Se trovo un algoritmo $\\NPC$, almeno ne ho trovato uno che funziona.","x":295,"y":1351,"width":640,"height":499,"color":"6"},
		{"id":"d582dc2a1cfd5670","type":"text","text":"# DTLog (Distributed Transaction Log)\n\nSe i processi sono ***fail-recover***, quando resuscitano perdono tutta la memoria tranne il ***log***.\n\nAssumendo che il log non possa avere `fail` e dividendo i nodi in coordinatori e partecipanti, la domanda è: ***meglio scrivere sul log e poi mandare il messaggio o viceversa?*** Se riesco ad eseguire entrambe le operazioni va tutto bene, ma in entrambi i casi potrei morire nel mezzo. L'idea è che, se succede, quando torno operativo leggo il log e riprendo da dove mi ero fermato. Ma è safe fare così?\n\n- `C - voteRequest` - Quando inizia il protocollo scrive sul log `Start2PC`, che contiene la lista dei partecipanti. ***L'ordine è indifferente***:\n\t- se prima scrivo sul log e poi muoio, la lista dei partecipanti è ancora là e nel dubbio rimando la `voteRequest`.\n\t- Viceversa, se risvegliandomi non vedo nulla nel log la scelta migliore è mandare `abort` in `broadcast`;\n- `P - vote` - Qui è cruciale distinguere due casi:\n\t- ***Se decido di votare `yes` devo prima scriverlo sul log***. Viceversa, se mando e muoio poi non so cosa ho votato. Dovrei fare `abort`, ma non posso. Insomma, no.\n\t- **Se decido di votare `no` l'ordine è indifferente**, ma a sto punto faccio come `yes`.\n- `C - Decision` - Anche qui devo distinguere\n\t- ***Se decido `commit` devo prima scriverlo sul log***, per gli stessi motivi;\n\t- **Se decido `abort` l'ordine è indifferente**.\n\nIn pratica per una decisione negativa l'ordine è indifferente, mentre per una positiva è meglio `log then send`, alla peggio mando lo stesso messaggio due volte.\n\n###### Obiezione! Il log è su disco, può fallire!\n\nHo capito, ma mica puoi risolvere tutto. Magari fai più dischi e muoiono tutti. Non solo: quando chiamo una `syscall` per scrivere su disco, magari questa scrive in RAM. Allora dovrei forzare a scrivere su disco (tipo con una `flush`, su linux si chiama `fsync`?). Ma magari il disco ha un buffer e salta la corrente, perdo il buffer. Potresti usare un disco solo per il log e configurarlo in modo che non usi il buffer.\n\nIl punto è che è ragionevole assumere che il log sia safe per sviluppare il protocollo","x":-4043,"y":2356,"width":764,"height":1054,"color":"4"},
		{"id":"d63911fe834dbe22","type":"text","text":"# \\* Chiarimento - Il voto in Paxos\n\nQuando negli statement dico \"`A0` votes for `x`\" intendo dire che manda una `learn(j, x)`.\n\nSe ha mandato una `learn`, ha ricevuto una `accept`. Se è partita una `accept`, significa che è un `P` ha ricevuto un Quorum di `promise`, quindi sa cosa è safe far votare.\n\nQuindi il senso del teorema è \"in questo `round` di cui parliamo, che chiamiamo `j`, se un `A` vota `x` sono sicuro che nei round precedenti nessun `x'` aveva già raggiunto un Quorum\".\n\nIn altre parole, so che ad ogni `round` in cui anche solo un singolo `A0` vota `x` sono sicuro che il valore su cui tutti eventualmente concorderanno (i.e. il valore scelto) può essere alternativamente `x` oppure un valore che verrà scelto nei prossimi `round`.\n\nBene, e a che mi serve? Se così non fosse ed esistesse un valore scelto `x'` nei round precedenti, il fatto che `A0` voti `x` renderebbe il protocollo inconsistente, perché significherebbe che in round diversi vengono scelti diversi valori su cui vengono chiamate le `learn`, e di conseguenza diversi `L` potrebbero apprendere diversi `X`.","x":-3936,"y":3533,"width":551,"height":636,"color":"4"},
		{"id":"93212aed82c21df9","type":"text","text":"# Two-Phase Commit\n\nL'idea è di per sé semplice, poi si complica per farla funzionare in caso di ritardi e/o failures: dividiamo i nodi (processi) in ***coordinatori*** e ***partecipanti***, quindi abbiamo i seguenti step:\n\n- Il coordinatore `C` inizia il protocollo mandando una `voteRequest` ai partecipanti `P`;\n- Ogni partecipante decide cosa votare tra `yes` e `no`;\n\t- Se vota `no` fa direttamente `abort`.\n- La decisione di voto viene inviata a `C`;\n- Se `C` riceve le risposte di tutti i `P` e queste sono tutte `yes`, allora ***può*** prendere la decisione di fare `commit`.\n\t- Notare che non è obbligato a fare `commit`, sta di fatto una decisione viene presa.\n- `C` invia la decisione a tutti, i quali eseguono.\n\nTutto troppo bello. In un sistema reale ***i messaggi possono essere persi***, o fare talmente tanto ritardo che si danno per persi (tramite un timer). Cosa succede in questo caso?\n\n- Se si perde la `voteRequest`,\n\t- la prima volta che scade il suo timer, `P` chiede a `C` di rimandare la `voteRequest`;\n\t- la seconda assume che `C` sia morto e nel dubbio fa un `abort`.\n- Se si perde il voto, a `C` scade il timer per la risposta.\n\t- la prima volta `C` manda di nuovo la `voteRequest`;\n\t- la seconda assume che `P` sia morto e nel dubbio decide per un `abort`.\n- Se si perde la decisione che `C` manda a `P`\n\t- se io `P` ho votato `no` non è un problema, tanto ho già fatto `abort`;\n\t- se ho votato `yes` non è per niente safe scegliere di fare `abort`, quindi mi tocca aspettare. Magari mando a `C` un gentile sollecito come \"`aoo sta decisione??`\" sperando che arrivi. Ma magari lo chiedo tante volte e nessuno mi risponde. Magari `C` è morto. Allora chiedo agli altri `P` se hanno ricevuto la decisione. Questi possono rispondere in tre modi:\n\t\t- `ho ricevuto la decisione` - applico la decisione;\n\t\t- `non l'ho ricevuta, e...`\n\t\t\t- `... ho votato no` - faccio un `abort`;\n\t\t\t- `ho votato yes` - brutta situazione, non posso concludere niente e mi tocca chiedere ad un altro. Se chiedo a tutti, ***tutti*** hanno votato `yes` e ***nessuno*** ha ricevuto la decisione, tutti fanno `abort`.\n\nNotare che è un protocollo safe, ma è molto poco live (ci sono molti più modi per fare `abort` che per fare `commit`). Questa fratellanza tra `P` lo rende un ***cooperative protocol*** nel quale anche un singolo fail (tutti i processi sono fail-stop) fa fare `abort` a tutti.\n","x":-3025,"y":2356,"width":753,"height":1054,"color":"4"},
		{"id":"e69724a397d8eb9b","type":"text","text":"# Paxos Safety Theorem\n\n`THM` - If some acceptor `A0` votes for `x` in round `j`, then no value `x'`$\\neq$`x` can be chosen in previous rounds (i.e. se c'è stata una `learn(j, x)` da parte di `A0`, vuol dire che ha ricevuto una `accept(j, x)` da parte di `P0`. Se `P0` ha mandato una `accept(j, x)` significa che ha ricevuto un numero $\\geq Q$ di `promise` e sa che nessun altro `x'`$\\neq$`x` ha avuto una maggioranza nei round precedenti a `j`, i.e. è certo che `x` sia safe).\n\n`Proof` - Procediamo per induzione sul round $i$.\n\n- Base induttiva (`round 0`) - Il teorema è ovvio, non c'è nessun `round` precedente quindi non può essere stato già scelto nessun valore $\\neq$`x`;\n- Assumiamo vero il teorema per il `round i-1`;\n- Passo induttivo (`round i`) - A `P0` arrivano $|Q|$ messaggi di tipo `promise`. Tra questi, chiamiamo `j` il `maxround`, ovvero il $\\max_k$(`last round I voted`$_k$), dove $k\\in Q\\subseteq A$.\n\t- Segue per definizione che nessuno dei $k\\in Q$ ha votato tra i `round j+1` e`i-1`.\n\t\t- Se `j` valesse `-1` (i.e. `non ho mai votato`) avremmo finito la dimostrazione.\n\t- Segue anche che al `round j` all'interno di $Q$ almeno uno deve aver votato per `x`;\n\t\t- Potrebbe essere l'unico? Sì, perché tutti gli altri `accept` rivolti agli altri `A` potrebbero essere stati persi.\n\t- Ma se almeno uno ha votato `x` significa che un `P` gli ha detto di farlo con una `accept(j, x)`, quindi al `round j` nessun `A` può aver votato `y`$\\neq$`x`.\n\t- E per i `round` precedenti? A questo punto posso usare l'ipotesi induttiva, quindi nei `round`$<$`j` nessun valore `x'`$\\neq$`x` può essere stato scelto.","x":-3025,"y":3533,"width":753,"height":636,"color":"4"},
		{"id":"7bd4339e9dc5832c","type":"file","file":"2 - Atomic Commit/Paxos.png","x":-2021,"y":4728,"width":753,"height":474},
		{"id":"4043ace7e5011791","type":"text","text":"# Fast-Paxos\n\nFast-Paxos richiede che sia già stato scelto il coordinatore `P1`, i.e. tutti ne sono a conoscenza. Gli `A` sanno che possono accettare solo le `accept` provenienti da `P1`, e in qualche modo tutti i `P` conoscono le decisioni di `P1`.\n\nDetto questo, `P1` ha a disposizione due modi per far partire un `round`:\n\n- Può mandare una `prepare(i)` esattamente come in Paxos standard. In questo caso si parla di ***slow round***, ed è sostanzialmente un ***Paxos con leader*** (i.e. un Paxos che è anche live);\n- Può mandare direttamente una `accept(1, ⊥)`, ovvero una speciale `accept` senza un valore associato che comunica agli `A` di accettare le `accept` di qualsiasi `P`. Questa ***accept-any*** fa partire un ***fast round***: ogni volta che ad un qualsiasi `P` arriva un valore `x`, questo manda direttamente una `accept(1, x)`, senza passare per `P1`.\n\nFocalizziamoci sul `fast round`. Diversi `P` possono mandare una `accept(1, X)` dove `X` è diverso a seconda del `P`. Potenzialmente è un delirio! Possono succedere due cose:\n\n- Un singolo `x` raggiunge il Quorum, comunque esso sia definito. Tutti i learner apprendono quel valore, il protocollo termina al primo `round` e noi tiriamo un sospiro di sollievo;\n\t- In generale, la speranza del `fast round` è proprio quella di ***raggiungere il consenso già al primo `round`***, in modo che non vadano risolti conflitti.\n- Il protocollo non termina al primo `round` (i.e. nessun voto raggiunge il Quorum, quantomeno non per tutti i learner), tocca fare il secondo. Qui però nascono i problemi.\n\t- Il secondo `round` non può essere un `fast round`. Questo è ovvio se si pensa al fatto che i vari `P` hanno già espresso la propria opinione, ed ora bisogna solo risolvere le controversie in modo safe. Questo `slow round` viene infatti anche detto ***recovery round***. E cosa bisogna fare in questo `recovery round`? Esattamente quello che si fa in Paxos standard. `P1` sonda gli `A` con una `prepare`, riceve le `promise` con i voti e sulla base di questo sceglie il valore safe da proporre. Ma qual è il valore safe?\n\nPer capire qual è il valore safe da proporre, poniamo $|A|=5$. Siamo nel `recovery round`, dunque al coordinatore `P1` dovrebbero essere arrivate le `promise`. Diciamo che due voti vanno persi, un voto dice `x`, uno dice `y` e un altro non ha mai votato. Se fossimo in Paxos standard sarebbe facile scegliere cosa proporre: basta vedere il `maxround` tra `x` e `y`. Problema: sono entrambi `round 1`, ***la scelta non è univoca***. Cosa deve fare `P1`?\n\nIn questa situazione non posso mai essere sicuro di un valore safe, perché se scelgo `x` e i due voti andati perduti sono per `y` ho rotto il protocollo (e viceversa). La soluzione a questo dilemma è stare zitti, cioè non mandare alcuna `accept`. Se abbiamo capito come funziona Paxos, se il `P` non manda la `accept` significa che ***non ha ricevuto un Quorum di `promise`***.\n\n\"Ma come, $Q=\\ceil*{5\\over2}=3$!\" ... sì, ma evidentemente qui non è più sufficiente. ***Bisogna aumentare il quorum a*** $\\ceil*{2|A|\\over3}$. Scegliamo ***necessariamente*** $|A|=3n+1$ (e.g. 7, in modo che $Q$ sia dispari) e riconsideriamo il dilemma del Quorum, che con questa modifica diventa $Q=\\ceil*{14\\over3}=5$.\n\n- $1\\over3$ delle `promise` non arriva, i.e. due voti vanno persi;\n- Il caso peggiore è che dentro il Quorum ci sia un perfetto equilibrio, e.g. $1\\over3$ dei voti sono per `x` e $1\\over3$ sono per `y`. Ma per la definizione di $|A|$ il Quorum è un numero dispari, quindi è impossibile che ci sia equilibrio perfetto: $Q=\\ceil*{{2\\over3}(3n+1)}=\\ceil*{2n+{2\\over3}} = 2n+1$.\n\t- Diciamo quindi che `x` ha ricevuto tre voti e `y` due. $Q=5$ quindi\n\t\t- Può `y` aver ottenuto un $Q$ di voti? No. Anche se i due voti persi fossero stati per `y`, arriverebbe al massimo a $4$;\n\t\t- `x` invece potrebbe arrivare a $5$, quindi `x` è il valore da proporre.\n\t- Poniamo invece che ci siano due voti a testa per `x` e `y`, e il quinto è un non-voto. Nessuno dei due può aver già ottenuto una maggioranza, quindi è safe scegliere qualsiasi valore (anche `z`$\\neq$`x`,`y`, se lo sceglie `P1`, proprio come in Paxos standard);\n\t\t- Qui `P1` ***potrebbe anche scegliere di far partire un altro `fast round`***. Ha poco senso se vede una maggioranza relativa in $Q$, ne ha di più se vede tutti voti diversi.\n\t- Notare che (come in Paxos standard) non è detto che basti un solo `recovery round`.\n\nLa morale della favola è la seguente: se c'è un valore che potrebbe già aver ottenuto un Quorum di voti, allora quello è il valore da proporre nel `recovery round`. Ma è la stessa regola di Paxos standard! Sì, quello che cambia è solo l'aumento di $Q$, dovuta al fatto che sto provando a \"forzare la mano\" con i `fast round`. Questo ***tentativo di velocizzare*** viene pagato a caro prezzo, perché\n- ***la tolleranza scende*** a $f'=\\floor*{n-1\\over3}$;\n- il nuovo $Q$ ***vale anche per i `fast round`***, il che significa che scende la probabilità di far approvare subito un valore (ogni `L` deve sempre vedere $Q$ `learn`!).\n\nDetto questo, è anche possibile unire Multi-Paxos e Fast-Paxos mandando la `super-prepare` e ricevendo una `super-promise`. Chiaramente, `P1` utilizzerà una `accept any` solo quando è safe accettare davvero qualsiasi valore, e.g. al primo round, quando nessuno ha votato.\n","x":-1018,"y":4728,"width":797,"height":1756,"color":"4"},
		{"id":"1a4ae7dd4ea788ae","type":"text","text":"# Ma vale la pena?\n\nIn una parola: ***no***.\n\nIn generale Fast-Paxos è più lento di Multi-Paxos, ma può avere buone performance se\n\n- Ci sono pochi `P` e poche transazioni (i.e. poche $T$ che arrivano a pochi `P`);\n- Ci sono pochi conflitti all'interno del Quorum, ad esempio in piccoli cluster di server con poco carico (viceversa, i server non conoscono lo stato l'uno degli altri, e sale la probabilità di avere conflitti).\n\nIn pratica non è molto usato: riduce la tolleranza per un boost in velocità che spesso con i conflitti neanche esiste (i sistemi reali sono spesso in overload).\n\n","x":-1018,"y":6588,"width":797,"height":343,"color":"4"},
		{"id":"5aaddbdcdf2d4e68","type":"text","text":"spesso inoltre non c'è davvero bisogno di aspettare l'ordine giusto per il commit\n\n# ?","x":-959,"y":7028,"width":680,"height":117,"color":"1"},
		{"id":"8d15b0299f305f56","type":"text","text":"# Fast Paxos Safety Theorem\n\n`THM` - If some acceptor `A0` votes for `x` in round `j`, then no value `x'`$\\neq$`x` can be chosen in previous rounds (i.e. se c'è stata una `learn(j, x)` da parte di `A0`, vuol dire che ha ricevuto una `accept(j, x)` da parte di `P0`. Se `P0` ha mandato una `accept(j, x)` significa che ha ricevuto un numero $\\geq Q$ di `promise` e sa che nessun altro `x'`$\\neq$`x` ha avuto una maggioranza nei round precedenti a `j`, i.e. è certo che `x` sia safe).\n\n`Proof` - Esattamente come Paxos, per induzione su $i$.\n\n- Base induttiva (`round 0`) - Il teorema è ovvio, non c'è nessun `round` precedente quindi non può essere stato già scelto nessun valore $\\neq$`x`;\n- Assumiamo vero il teorema per il `round i-1`;\n- Passo induttivo (`round i`) - Se `maxround = j`$=0$, il teorema è ovvio. Se `j`$>0$ \n\t- per scegliere il valore da proporre ho due vie:\n\t\t- ***se*** $\\exists$`x` t.c. ha abbastanza voti in $Q$ da poter raggiungere una maggioranza contando i voti fuori da _$Q$_ (che non conosco) ***oppure*** `x` è l'unico valore che compare nelle `promise`, ***allora*** scegli `x`;\n\t\t- ***altrimenti***, scegli un qualsiasi valore.\n\t- Nota che, mentre il `se` è ovvio per definizione, le scelte date da `oppure` e da `altrimenti` sono giustificate dal teorema stesso, che uso per ipotesi induttiva.\n\t\t- Se compare solo `x`, secondo il teorema non può essere stato già scelto alcun valore diverso da `x`, quindi `x` è safe;\n\t\t- Se non siamo in nessuno dei due casi del `se`, e.g. ci sono due o più valori, posso comunque applicare il teorema, il quale mi dice che nessun valore diverso da `y` è stato votato in precedenza, né da `z`, e via dicendo. In pratica si escludono tutti a vicenda, quindi nessun valore può essere stato votato in precedenza, quindi qualsiasi valore è safe.","x":238,"y":5242,"width":762,"height":729,"color":"4"},
		{"id":"ee7f51e5be213a53","type":"text","text":"se hai tempo fai delle gif","x":-2773,"y":4935,"width":250,"height":60,"color":"5"},
		{"id":"00258a5944b2d60e","type":"text","text":"# Paxos I - Definizione e Primo Round\n\nProtocollo maggioritario a turni in cui dividiamo i nodi in tre categorie:\n\n- ***Proposers*** (***`P`***) - Ad inizio protocollo, ad ognuno di questi nodi viene assegnato un array statico di valori interi tali che nessuna coppia di `P` abbia un numero in comune. Tali interi (detti ***round***) definiscono in che ordine i `P` potranno proporre un singolo valore `v` da fare accettare al sistema (i.e. un valore sul quale potenzialmente tutti alla fine concorderanno e sul quale tutti potranno fare `commit`);\n\t- Non è una buona idea averne uno solo, se muore l'intero protocollo è morto. Al contempo, perché il protocollo non muoia ne basta uno vivo.\n- ***Acceptors*** (***`A`***) - Sono i nodi più importanti, perché decidono a maggioranza su quale valore deve fare `commit` l'intero sistema (questo compito sta ai nodi di tipo `L`). Se ce ne sono $n$, il protocollo tollera il fail (***resilience***) di al più $f=\\floor*{n-1\\over2}$ nodi di tipo `A`;\n\t- Se ci sono più di $f$ fail sugli `A`, il protocollo si ferma (\"*safe but not live*\");\n\t- In genere $n$ è dispari, perché in questo modo la tolleranza resta la stessa anche se muore un nodo. Tanto vale metterne uno in più.\n- ***Learners*** (***`L`***) - Sono quelli che alla fine del protocollo danno il via libera al `commit` se tutto è andato bene (i.e. se vedono una maggioranza di voti da parte degli `A`).\n\nDetto questo, il ***primo round*** del protocollo funziona nel seguente modo:\n\n- Un qualsiasi `P` (e.g. `P0`) può iniziare il protocollo tramite un messaggio `prepare`, associando alla sua richiesta il più piccolo valore di `round` presente nel suo vettore statico. Per costruzione, non è detto che tale valore sia proprio `1`. Scegliamo quindi `3` per guardare ad un generico primo `round`. Con una `prepare(3)`, ***un singolo `P` dice a tutti gli `A`*** di prepararsi per il round `3`;\n- Gli `A` che ricevono la `prepare` (il sistema è asincrono, i messaggi si possono perdere o metterci un tempo talmente lungo da essere dati per persi!) rispondono a `P0` con una `promise(3, -, -)`, che significa `prometto di partecipare al round 3 e che in futuro non parteciperò ad alcun round < 3. Inoltre non ho mai votato`;\n- Se a `P0` arriva un numero $\\geq Q = n-f$ (detto ***quorum***) di `promise`, è in grado di dire con certezza che ***non c'è alcun valore già votato in un precedente round*** (e a maggior ragione nessuno con una maggioranza), ***quindi propone il proprio valore*** `v` tramite una `accept(3, v)`;\n\t- Se ne arrivano meno di $Q$ semplicemente scatta il timeout e si ricomincia.\n- Gli `A` che ricevono `accept(3, v)` mandano una `learn(3, v)` (i.e. il ***voto***) ai `L`;\n- Ogni `L` riceve in generale un diverso numero di `learn`. Se tale numero di `learn(3, v)` (i.e. stesso round e stesso valore) è $\\geq Q$, allora il generico ***`L0` accetta il valore `v`***.\n\t- Se tutti i `L` accettano, il protocollo è finito;\n\t- In caso contrario, il protocollo ricomincia con un altro round.","x":-2021,"y":2356,"width":753,"height":1054,"color":"4"},
		{"id":"98ff8ea31a5611b2","type":"text","text":"# Paxos II - Secondo (e generico) Round\n\n- Un qualsiasi `P` (e.g. `P1`) ricomincia il protocollo tramite una `prepare`, associando alla sua richiesta il più piccolo `round` nel suo vettore statico, diciamo `5`.\n\t- In generale è ottimale scegliere il round in modo che sia $>$ del massimo round già giocato dall'intero sistema. Il singolo `P` per come è stato descritto finora non può avere questa informazione, ma in alcuni casi è possibile che i `P` siano anche `A`. Questa ottimizzazione mira a massimizzare la *liveness*, perché il protocollo in sé già mi garantisce *safeness*.\n- Gli `A` che ricevono le `prepare` rispondono a `P1` con una `promise(5, 3, v)`, che significa `prometto di partecipare al round 5 e che in futuro non parteciperò ad alcun round < 5. L'ultima volta che ho votato era il round 3, e ho votato v`;\n\t- Se `P1` avesse estratto `2` come `round`, la `prepare(2)` sarebbe stata ***ignorata*** da tutti gli `A` che hanno già inviato una `promise(n, *, *)` con `n > 2`. Se tali `A` sono $\\geq Q$, Paxos finirà per attendere il timeout e riprovare con un altro `round`.\n- Se a `P1` arriva un numero $N\\geq Q=n-f$ di `promise`, ci sono diversi possibili scenari:\n\t- Al secondo `round` ci sono due scenari verosimili:\n\t\t- Nessuno degli $N$ `A` ha mai votato (i.e. il primo `round` è terminato per via dello scadere del timer, quindi è possibile che sia andata perduta la maggioranza di una qualsiasi tipologia di messaggio del protocollo). In questo caso torniamo all'algoritmo del primo `round`;\n\t\t- Qualcuno degli $N$ `A` ha votato `v`. ***`P0` non può proporre il proprio valore*** `x`, ma è costretto a riproporre `v`.\n\t- Al generico `round n`, ***`P1` propone il valore associato alla `promise` avente il più grande valore di `ultimo round a cui ho votato`***.\n- A questo punto abbiamo capito che se è partita una `accept` significa che `P1` ha ricevuto un numero $N \\geq Q$ di `promise`, quindi in pratica se siamo qui è perché\n\t- Viene proposto un nuovo valore `x` perché nelle `promise` non c'era alcun voto;\n\t- Viene riproposto il valore `v` più recentemente votato.\n- Gli `A` mandano una `learn` con il voto corrente.\n\t- Notare che se partono $M\\geq Q$ `learn` (i.e. $M\\geq Q$ `accept` dello stesso `round` arrivano a destinazione senza perdersi ed entro il timer) allora sicuramente esiste una ***maggioranza di `A` su un singolo valore `v`***. Questo necessariamente implica che nessun valore che non sia `v` potrà mai essere approvato per il `commit`, perché dallo step successivo qualsiasi maggioranza $N\\geq Q$ vista dal `P1` di turno conterrà sicuramente il valore `v`, che sarà anche quello con `round` maggiore. Questa osservazione si formalizza in un ***safety theorem***.\n- Ogni`L` riceve in generale un diverso numero di `learn`. Se il numero di `learn` con stesso round e stesso valore è $\\geq Q$, allora il generico ***`L0` accetta il valore `v`***.\n\t- Come conseguenza del teorema precedente, se un singolo `L0` accetta un valore `v` è impossibile che qualsiasi altro `L1` accetti un valore `x`$\\neq$`v`.","x":-2021,"y":3533,"width":753,"height":1077,"color":"4"},
		{"id":"c604e7d9b7779409","type":"text","text":"# Multi-Paxos\n\nOgni run di Paxos raggiunge il consenso su un singolo valore. Tuttavia, in una situazione reale è verosimilmente necessario approvare una sequenza di valori, non uno solo. La soluzione ovvia è eseguire diverse instance del protocollo una dopo l'altra.\n\nMa si può fare meglio? Spoiler: sì. Consideriamo la seguente modifica al protocollo: invece di mandare una `prepare(i)` in cui diciamo \"preparati al round $i$...\" sottintendendo \"... per una singola istanza che farà approvare un singolo valore\" proviamo a dire \"***preparati al round $i$ di $N$ istanze, ognuna delle quali farà approvare un singolo valore***\".\n\n- La `prepare(i)` diventa una `prepare(N, i)`, in cui `N` è il numero dell'istanza. Posso quindi ottimizzare inizializzando il protocollo con una singola `prepare(1-1000, 1)` ovvero \"`preparati al primo round di 1000 istanze`\", di fatto evitando $999$ messaggi `prepare(i)`.\n\t- Possiamo chiamare questo tipo di messaggio \"`super-prepare`\".\n- Stessa cosa per la prima `promise(1-1000, 1, -, -)`, con cui ogni `A` comunica di essere pronto per il primo `round` delle istanze `1-1000`;\n\t- Possiamo chiamare questo tipo di messaggio \"`super-promise`\".\n- E le `accept`? Il sistema è solo *pronto* alle istanze, non le ha ancora ricevute. Quando `P1` riceve $T_1$, cioè il primo valore da approvare (e.g. dal client) è sufficiente mandare una `accept(1, 1, T1)` ovvero `accept(istanza, round, value)`.\n\t- Se ne riceve un'altra può mandare direttamente `accept(2, 1, T2)`? Sì, perché l'importante è l'univocità della coppia `istanza, round`;\n\t- E se si perdono tutte le `accept`? Banalmente, `P1` la rimanda.\n\nBene, se si perde l'`accept` di `P1` è `P1` stesso a rimandarla. Perché non qualcun altro? Prenotando le istanze `1-1000`, `P1` sta sostanzialmente dicendo che per queste istanze il valore da approvare è il suo, e il protocollo serve solo a raggiungere il consenso su `T1` (non su un generico valore proposto da uno qualsiasi dei `P`). Finora abbiamo assunto che i `P` non fossero in competizione, ma se c'è un altro `P` (diciamo `P2`) a ricevere delle $T$ da far approvare che faccio?\n\n- Opzione 1 - `P2` prenota le istanze `1001-2000`. Non è ottimale, perché magari approvo le istanze `1, 2, 1001` ma da qui in poi non posso fare `commit` perché devo aspettare tutte le istanze tra `2` e `1001`, il che chiaramente è la morte della liveness;\n- Opzione 2 - Tutti i `P` mandano le proprie $T$ a `P1` , visto che il genio si è prenotato `1000` istanze di fila. In questo scenario `P1` diventa il ***coordinatore***.\n\t- Con che logica si sceglie? E se muore? $\\Rightarrow$ ***Leader Election Problem***;\n\t- ***Se esistesse un perfetto protocollo di Leader Election, Paxos diventerebbe live***, perché si eviterebbe il conflitto tra diversi proposer e si romperebbe il meccanismo di inibizione dei round precedenti (***Paxos Liveness Problem***): la `super-prepare` viene mandata una volta sola con `round 1` (i.e. non c'è modo di *inibire* un `round`).\n\t- Non è necessario che ci sia. Alla peggio Multi-Paxos è safe ma non molto live.","x":-1018,"y":2356,"width":797,"height":1054,"color":"4"},
		{"id":"df6f8d2bb2d685d0","type":"text","text":"# Paxos III - Ottimizzazioni\n\nCosì per come è definito, il protocollo è safe. Ma è possibile aumentarne la liveness?","x":-830,"y":4012,"width":421,"height":120,"color":"6"},
		{"id":"6a09d757776030a8","type":"text","text":"# Leader Election (Heuristics)\n\n$n$ processi devono concordare su quale di essi è il leader (e.g. il coordinator di Multi-Paxos). \n\nCome si fa? Visto il risultato del teorema FLP, assumiamo sistema sincrono e niente fault. I processi hanno un identificativo, e il leader è quello con l'`ID` più basso.\n\nSe tutti sanno già chi è l'`ID` più basso, il protocollo è già finito. Se il processo `1` sa di esserlo gli altri non lo sanno, gli è sufficiente mandare questa notizia in `broadcast`.\n\nDetto l'ovvio, poniamo che nessuno sa ancora chi è il leader. Quanti messaggi servono affinché tutti acquisiscano abbastanza informazioni da poter concordare sul leader?\n\n- $1\\to2\\to3\\to...\\to n$ e viceversa - Sulla carta l'overhead è di $2n$ messaggi, in pratica ogni messaggio diventa sempre più grande, perché porta l'informazione di tutti gli altri nodi precedenti, quindi somiglia più ad $O(n^2)$;\n- Potrei ottimizzare prendendo solo il minimo tra ogni coppia di `ID`, implementando di fatto una struttura ad albero binario il cui costo sarà $\\log(n)$;\n- Se ogni nodo fa ***broadcast*** finisco in un tempo $O(1)$, al costo di un overhead $O(n^2)$.\n\nIn tutto questo non stiamo considerando i crash. Ammettiamo che ne esista uno solo, ma il sistema è ancora sincrono: se entro $\\delta$ non arriva il messaggio di un nodo, so che è morto.\n\nNon è comunque detto che il broadcast sia atomico. È possibile che `P0` invii ai primi $4$ nodi l'informazione di essere il leader (i.e. di esistere), e poi muoia. Non tutti concorderanno su chi è leader: per i nodi da `P1` a `P4` è `P0`, per i nodi da `P5` in poi è `P1`.\n\nSe però ripeto questo processo due volte, il leader può comunque crashare solo una volta. In almeno uno dei due giri avrò consenso. In generale, se ci sono al più $f$ faults mi servono $f+1$ round: il caso peggiore è che in ogni round muoia il leader a metà del broadcast, e anche nel caso peggiore esiste un round senza fault.\n\nOra assumiamo che il sistema sia asincrono, i.e. non posso distinguere un messaggio lento da un fault. Se può esistere anche solo un fault, allora non esiste soluzione (***FLP Theorem***).\n\nOra, nelle applicazioni reali i DS sono spesso sincroni, ma a volte smettono di esserlo. Per ovviare al problema si attuano strategie come\n\n- Quando il sistema funziona, si elegge un main leader e diversi leader in seconda;\n- Si implementano dei meccanismi di ***failure detection***.","x":238,"y":3534,"width":762,"height":1077,"color":"4"},
		{"id":"c2b4396b3b84e4de","type":"text","text":"# Overview\n\nIl problema è totalmente descritto da queste proprietà che deve possedere:\n\n1. All processes that reach a ***decision***, must reach the same one.\n2. Processes cannot reverse their decisions.\n3. The `commit` decision can only be reached if all processes voted `yes`.\n4. If there are no ***failures*** and all processes voted `yes`, decision must be to `commit`.\n\nI `failure` possono essere di diversi tipi:\n\n- `crash failure` - il processo muore (***fail-stop***), poi eventualmente potrebbe anche risvegliarsi (in tal caso è ***fail-recovery***);\n\t- Un protocollo fail-stop tollera anche i fail-recovery.\n- `omission failure` - il processo non manda un messaggio che secondo il protocollo avrebbe dovuto mandare;\n- `byzantine failure` - il processo continua a funzionare, ma lo fa in modo imprevedibile, inconsistente o malevolo.\n\nI protocolli che seguono non ammettono alcun `byzantine node`.\n\nEssendo il sistema asincrono, è impossibile distinguere un processo che ha avuto un `crash` da uno che è solo molto lento a rispondere. In virtù del Teorema FLP, questo rende il problema in generale irrisolvibile. Tuttavia, nelle situazioni pratiche questo è raramente un problema.\n\nAnche un processo che sulla carta sembra avere liveness molto bassa può rivelarsi efficiente, perché il tasso di `failures` è generalmente basso.","x":-3981,"y":1240,"width":640,"height":722,"color":"4"},
		{"id":"fbc1ef28f3589012","type":"text","text":"# Definizioni\n\nPer l'insieme dei processi che scambiano messaggi definiamo\n\n- ***History*** - $h_i$ è l'insieme di tutti gli eventi sulla timeline del processo $i$;\n\t- $H=\\{h_i\\}$ è la collezione di tutte le storie.\n- ***Prefix*** - $p_k(h_i)$ è l'insieme dei primi $k$ eventi di $h_i$;\n- ***Local State*** (***LS***) - dopo $k$ eventi, il processo $i$ si trova nello stato $\\sigma_i^k$;\n\t- ***Global State*** (***GS***) - Collezione di tutti i LS al tempo $k$;\n- ***Cut*** (***C***) - $C=\\{p_{k_{i}}(h_i)\\}$ è una collezione dei prefissi di ogni processo. Non necessariamente ogni processo deve avere lo stesso numero di eventi $k$ nel proprio prefisso (infatti $k$ è una $k_i$), né è necessario che $k\\neq0$. Ogni C individua un GS;\n- $e_2^1$ è l'evento con cui il processo $2$ manda una `request` al processo $1$, e $e_1^2$ è l'evento con cui il processo $1$ registra il ricevimento di tale `request`, necessariamente $e_2^1\\to e_1^2$ (i.e. $e_2^1$ ***avviene prima di*** $e_1^2$).\n\nDetto questo, l'ordine degli eventi sulla singola $h_i$ viene mantenuto ad ogni ***run*** (i.e. ad ogni esecuzione dell'intero sistema), ma l'ordine relativo degli eventi di diversi processi può variare. Formalmente, \"a ***run*** $k'$ is a reordering of events such that internal order of every process is preserved\", i.e. sono diverse sequenze di computazione che preservano la coerenza del singolo processo.\n\nQuesto da solo non garantisce che, ad esempio, $e_2^1\\to e_1^2$. Una run in cui tutte le relazioni tra eventi di tipo $\\to$ sono rispettate è detta ***consistent run*** (***CR***). In modo analogo, un ***consistent cut*** (***CC***) è un cut tale che se $e\\to e'\\,\\wedge\\,e'\\in C\\Rightarrow e\\in C$.  Una consistent run è una run in cui qualsiasi C è consistente.","x":175,"y":-3958,"width":880,"height":599,"color":"4"},
		{"id":"f9d9980fa75657d5","type":"text","text":"# Causal Delivery\n\nPer ***ricostruire la run di un DS*** possiamo usare un processo $p_0$ al quale tutti i processi $p_i$ inviano i propri eventi.\n\nTuttavia il sistema è asincrono, quindi quando $p_0$ manda le `request` ai singoli processi questi in generale ricevono e rispondono in momenti diversi. Questo può portare $p_0$ a vedere un ***cut inconsistente*** del sistema, e di conseguenza a ricostruire una ***run inconsistente***.\n\nPotrebbe perfino non essere una run, se ad esempio $e_1^2$ arriva prima di $e_1^1$. Questo problema si risolve assumendo che i singoli canali siano FIFO.\n\nPer ovviare a questo problema è necessario sviluppare un criterio di ordinamento robusto rispetto ad asincronia ed unreliability, realizzato tramite un ***clock*** sul quale tutti i processi concordano.\n\nL'idea generale è che $p_0$ faccia `commit` solo e soltanto se l'evento che riceve è l'immediato successivo dell'ultimo registrato, mantenendo in un ***buffer*** tutti gli eventi ricevuti nell'ordine sbagliato.","x":-715,"y":-3910,"width":640,"height":503,"color":"6"},
		{"id":"4fd496a7b455c76c","type":"file","file":"0 - Definitions/SchemaGenerale.png","x":393,"y":-2845,"width":444,"height":303},
		{"id":"e0ba5ac0341a437d","type":"file","file":"0 - Definitions/RequestAndCut.png","x":393,"y":-3280,"width":444,"height":370},
		{"id":"18e22186b729910c","type":"text","text":"# Distributed Mutual Exclusion\n\nLC permette di implementare un algoritmo distribuito (\"di Lamport\", obv) per garantire l'accesso a risorse condivise evitando i deadlock.\n\nL'idea (molto riassunta perché lui non l'ha fatto) è la seguente: chi vuole accedere alla risorsa condivisa fa broadcast con la `request` e con il suo timestamp $t$ . Se riceve le `ACK` di tutti gli altri processi e tutti rispondono con un timestamp $t'>t$, allora il lock è safe.\n\nQuesto protocollo usa $3(N+1)$ messaggi. Si può fare di meglio? Ni. C'è l'algoritmo di Ricart-Agrawala’s, che passa a $2(N+1)$.","x":-715,"y":-5920,"width":640,"height":326,"color":"4"},
		{"id":"3458d02a2952e9c7","type":"text","text":"# Real Global Clock\n\nLa prima tentazione è quella di definire un ***Real Global Clock*** (RC) condiviso da tutti i processi, in modo tale che$$e\\to e' \\Rightarrow RC(e)<RC(e')$$Questa prende il nome di ***Clock Condition***, e notiamo che\n\n- L'implicazione non è un $sse$. Due eventi tali che $RC(e)<RC(e')$ possono banalmente essere scorrelati (***eventi concorrenti***), dunque non c'è alcuna necessità che $e\\to e'$;\n- È possibile costruire clock che rispettino questa condizione senza la strong assumption che siano RC.\n\n\n Se il canale fosse sincrono potrei definire la grandezza $\\delta$ della finestra di buffer per $p_0$ e una ***delivery rule*** del tipo \"At time $t$, deliver all messages in order whose timestamp is smaller than $t -\\delta$\".\n \nUna simile costruzione è problematica non tanto per l'assunzione di avere un clock globale (che si potrebbe risolvere con un Network Time Protocol, NTP), quanto per la necessità di implementare una delivery rule che dipende da una finestra temporale $\\delta$ (dunque siamo costretti ad assumere che il sistema sia sincrono).\n","x":-715,"y":-5281,"width":640,"height":588,"color":"4"},
		{"id":"6dcdd47739ab639e","type":"file","file":"1 - Logic Clock/LamportClock.png","x":417,"y":-5888,"width":397,"height":262},
		{"id":"0b9a16cefb33b26c","type":"file","file":"1 - Logic Clock/VectorClock.png","x":1405,"y":-5888,"width":400,"height":262},
		{"id":"61752071951b1015","type":"text","text":"# Vector Clock\n\nÈ chiaro che a causa degli eventi concorrenti il clock non può essere un semplice numero. Definisco allora una ***strong clock condition***$$TS(e)<TS(e') \\Leftrightarrow e\\to e'$$In pratica voglio che una relazione d'ordine tra i timestamp definisca la dipendenza tra gli eventi. Posso trovare una rappresentazione per i $TS$ che consenta una relazione d'ordine per eventi dipendenti e non la consenta per eventi concorrenti? Sì: usiamo un ***vector clock***.\n\nDiciamo che la nuova condizione di correlazione è$$e_i\\to e_j \\Rightarrow \\forall k\\, VC(e_i)[k] \\leq VC(e_j)[k]\\,\\,\\wedge\\,\\,\\exists\\,k': VC(e_i)[k'] < VC(e_j)[k']$$cioè se $e_i\\to e_j$ allora ogni componente del $VC$ di $e_i$ deve essere minore o uguale del $VC$ di $e_j$, di cui almeno una strettamente minore (se non metto quest'ultimo pezzo è valido anche $e_i\\to e_i$).\n\nIn pratica abbiamo detto che $e\\to e' \\Leftrightarrow TS(e)\\subseteq TS(e')$, ovvero che due eventi sono correlati $sse$ la storia di uno è un sottoinsieme della storia dell'altro (per eventi concorrenti questa condizione è falsa, senza rompere i casi in cui la condizione è vera).\n\nOk, ma come è fatto il $VC$? Esattamente come il $LC$, semplicemente ogni processo ha un vettore con tutti i $LC$ di tutti i processi.\n\nE la delivery rule? Quando $p_j$ invia un messaggio $m_j$ a $p_0$, questo contiene il suo VC, ovvero un $TS(m_j)$. $p_0$ fa una commit se$$D[j] = TS(m_j)[j] - 1\\quad \\wedge\\quad D[k]\\geq TS(m_j)[k]\\quad\\forall k\\neq j$$o più intuitivamente$$TS(m_j)[j] = D[j] + 1\\quad \\wedge\\quad TS(m_j)[k]\\leq D[k]\\quad\\forall k\\neq j$$\ndove $D$ è il VC dell'ultimo `commit`, ovvero se all'arrivo di $m_j$\n\n- il suo $VC[j]$ (i.e. il suo stesso LC) è ***esattamente $+1$ rispetto al dato salvato*** da $p_0$ in $D[j]$. In questo modo, $p_0$ è sicuro di aver già visto tutti gli eventi che precedono questo lungo $p_j$;\n- $p_0$ deve anche essere ***sicuro di conoscere già tutti gli eventi degli altri processi $k\\neq j$ che sono precedenti a quello corrente***, il che si traduce nella disuguaglianza.\n\t- Segue che $D$ non si aggiorna solo quando viene eseguito un `commit`, ma anche quando un messaggio finisce nel buffer di attesa (i.e. basta che $p_0$ ne sia a conoscenza).","x":1305,"y":-5528,"width":599,"height":1082,"color":"4"},
		{"id":"ab73850207f6a30a","type":"text","text":"# Notazione\n\nL'unica informazione utile da specificare sui messaggi è il processo da cui vengono, quindi $m_j$ è inviato dal processo $j$.\n\nUn evento $e_i^k$ è il $k$-esimo evento del processo $i$.\n\nAl contempo, posso riferirmi a generici eventi e dire che $e_i\\neq e_j$. In questo caso sono semplicemente eventi, senza specificare dove (il processo) o quando (VC) siano avvenuti.","x":295,"y":-4320,"width":640,"height":279,"color":"4"},
		{"id":"db4411d94cf78039","type":"text","text":"# Lamport's Clock (LC)\n\nPer realizzare un clock possiamo usare dei ***counter*** che fungono da ***timestamp***. Una definizione che rispetta la clock condition è la seguente:\n\n- Sul singolo processo, ogni evento concorrente o di tipo `sending` incrementa di $1$ il counter, i.e. all'evento $n$ segue l'evento $n+1$;\n- Quando si verifica un evento di tipo `receiving`, il messaggio in arrivo porta con sé il timestamp $m$ dell'evento `sending` dal quale è partito. L'evento `receiving` assume allora valore $\\max(n+1, m)$, ovvero\n\t- $n + 1$ come nel caso precedente se il $m<n$;\n\t- $m$, se $m>n$.\n\nIn questo modo posso mettere come delivery rule \"Deliver all received messages that are stable in timestamp order\", dove \"A message $m$ is stable if no future message with timestamp smaller than $TS(m)$ can be received\".\n\nQuesto non risolve però tutti i problemi.\n\n- Esistono diversi eventi con lo stesso timestamp. Questo significa che $p_0$ non ricostruisce esattamente la run, ma solo una possibile run;\n- Eventi concorrenti diventano di fatto dipendenti: secondo questa logica $TS(e_3^3) = 3$ e $TS(e_2^2)=5$, quindi sembra che $e_3^3\\to e_2^2$. Come risolvo?","x":295,"y":-5281,"width":640,"height":588,"color":"4"},
		{"id":"9db64449abbcd63a","type":"file","file":"1 - Logic Clock/ChandyLamport.png","x":2240,"y":-3287,"width":665,"height":384},
		{"id":"0ebec2401a36c3ef","type":"text","text":"# Sincronizzazione degli Orologi\n\nCome fanno processi distribuiti a concordare su un orologio globale?\n\nQuando i canali sono asincroni è difficile (ma non impossibile) realizzare un ***real clock*** condiviso da tutti i processi. Spesso tuttavia non è necessario. Il ***vector clock*** risolve due problemi:\n\n- ***Causal Delivery*** - Sono in grado di ricostruire l'intera ***run*** del DS;\n- ***Snapshot Consistency*** - Sono in grado di ricostruire un ***consistent cut*** del DS, anche se non necessariamente rappresenta un reale stato del DS ad un certo istante.","x":295,"y":-2080,"width":640,"height":332,"color":"6"},
		{"id":"96363645d77cc1f4","type":"text","text":"# Snapshot Consistency\n\nÈ possibile verificare lo stato del sistema senza per forza ricostruire l'intera run. Questo può essere utile per creare dei checkpoint, verificare la coerenza di un calcolo distribuito o rilevare anomalie.","x":1305,"y":-3734,"width":599,"height":150,"color":"6"},
		{"id":"f0f698cee3286316","type":"text","text":"# FLP Theorem\n\n`THM` - In an asynchronous system, even only one crash makes deterministic consensus impossible.\n\n`Sketch of Proof` - Assumo che il protocollo esista, e che rispetti le ipotesi del consensus problem. L'obiettivo è quindi che prima o poi devo raggiungere il consenso su uno dei valori in input.\n\n- Consideriamo i casi estremi in cui l'input è di soli zeri (stato `0-valent`, i.e. se raggiungo un agreement questo deve essere `0`) e di soli uni (stato `1-valent`, i.e. se raggiungo un agreement questo deve essere `1`);\n- In mezzo a questi due casi estremi esistono tutti gli altri possibili input. Ordiniamoli in modo che tra due righe consecutivi \"flippi\" solo un `bit`;\n- Visto che parto da `0-valent` e arrivo a `1-valent`, deve esistere una coppia di righe di input consecutive tale che la prima è `0-valent` e l'altra è `1-valent`;\n\t- Essendo l'algoritmo deterministico per definizione, ad ogni stringa associo con certezza il risultato.\n- Queste due righe differiscono solo di un `bit`, che è quello che determina se il risultato sarà `0` o `1`;\n- Ora, cosa succede se quel processo va in crash, o se il messaggio è molto lento? Per la proprietà di termination prima o poi il protocollo deve finire, quindi sarà costretto a prende una decisione senza considerare quel voto. Al $50\\%$ la scelta sarà sbagliata.","x":1844,"y":1351,"width":924,"height":499,"color":"4"},
		{"id":"ae64a718f65ccd25","type":"text","text":"to do exercises don't use notion of topological order","x":-618,"y":-2753,"width":250,"height":120},
		{"id":"eead1953cd38ffd8","type":"text","text":"# L1 (Lamport Clock)\n\nMettiamo processi da 0 a 3. Ogni volta che su un processo da 1 a 3 avviene un e, questo lo notifica a $p_0$, il quale può ricostruire la run (è un osservatore). visto che non so quanto ci mettono i singoli messaggi ad arrivare a p_o, potrei avere delle inconsistenze! Non solo potrebbe non essere consistente, ma potrebbe non essere nemmeno una run!! come risolvo? Con un canale FIFO! weak assumption, very easy to implement. ma questo canale è per ogni coppia $(i;0)$ non mi dice niente sull'ordine dei diversi processi. Quindi così è una run, ma non necessariamente consistente. what if i give you a global clock (aka Real Clock RC), assume every process can use it. very strong assumption, cause no god gives us a clock, but i can use it to build a consistent run.\n\ndiciamo che $\\delta$ è un upper bound per il tempo impiegato da ogni canale per deliverare il messaggio a p_0. Ogni messaggio ha un timestamp, e p_0 ha una finestra di osservazione larga $\\delta$. entro questa finestra ha un buffer in cui aspetta di essere sicuro di ricevere tutti i messaggi di quella finestra.\n\n\n$e\\to e' \\Rightarrow RC(e)<RC(e')$. **Clock condition**. Non serve RC, basta qualsiasi clock con questa proprietà. LA freccia non è al contrario! $e\\to e'$ implica una necessità di ordine, che non è necessaria per **eventi concorrenti** (segnati con una freccia sul grafico, i singoli punti sono eventi indipendenti)\n\nDefiniamo un sapienza clock SC local to every p. se $e_1^1\\to e_2^1$ e il primo ha timestamp 1, il secondo deve avere timestamp maggiore, quindi sarà 2. dopodiché procedo con gli eventi in ordine, finché 7 sulla linea 2 non manda un messaggio alla linea 1. a qualsiasi numero sono arrivato sulla linea 1, ricevere un messaggio da 2 deve avere timestamp maggiore sia della linea 2 che di quella 1.\n\n- se 1 era arrivata a 5, il massimo +1 è 8;\n- se era a 12, è 13.\n\nQuesta roba è il **Lamport (or logical) Clock**\n\nora, sul singolo canale FIFO sono sicuro di avere i timestamp in ordine. ma prima di aggiungere $e_2^3$ devo aspettare tutti gli eventi precedenti? Non proprio, solo quelli che precedono logicamente. vedi grafico. e come entra $\\delta$ in tutto questo? se ricevo $e_2^3$ con timestamp 4, allora devo aspettare tutti gli eventi degli altri processi con timestamp fino a 4. this might make me wait. e se p_3 non ha nessun evento per un bel po'? ...boh si è dimenticato?...\n\n","x":-875,"y":-9120,"width":640,"height":1180},
		{"id":"dd46627802841779","type":"text","text":"# L3\n\nesempio foto\n\ndefiniamo $|\\Theta(e_i)|=\\sum_k VC(e_i)[k]$, ovvero la \"misura\" del vector clock.\n\nvoglio sapere se esiste $e_k$ t.c. $e_k$ NON è avvenuto prima di $e_i$, ma è avvenuto prima di $e_j$. In pratica voglio sapere se esiste un evento $e_k$ tra $e_i$ e $e_j$ (non letteralmente nel diagramma spaziotempo, basta che sia vero nella run). A livello di VC lo traduco come? $VC(e_i)[k]$ è il numero di eventi del processo $k$ che sono avvenuti prima dell'evento $e_i$, quindi$$VC(e_i)[k] < VC(e_j)[k]$$\n\nora, inizio a registrare. Se mi arriva $[001]$ posso registrarlo? Sì, perché sono sicuro che non può arrivare un messaggio che sia avvenuto prima di questo. e se poi arriva $[232]$? Ovviamente non posso registrarlo. Cosa discrimina tra i due casi, intuizione a parte? posso deliverare solo se una delle componenti dell'incoming message è esattamente +1 rispetto a quello che ho salvato (nel qual caso la aggiorno). riguardati come funziona il VC, funziona. Nota che è facilitato dal fatto che il singolo canale è FIFO, ma non è strettamente necessario che lo sia.\n\n\n\n- Se è $p_0$ che requesta i local states, non è detto che riesca a ricostruire un global state coerente (i messaggi arrivano ai processi in momenti diversi)\n- potremmo fare che quando il processo $p_i$ riceve la request fa broadcast su tutti gli altri processi. se un processo riceve lo snapshot del local state da un altro processo, fa partire il suo broadcast senza aspettare la notifica di $p_0$.\n- il Cut sui punti in cui i processi fanno broadcast è consistente. questo perché i canali sono FIFO, quindi una qualsiasi freccia che implica una relazione $e_i\\to\\ e_j$ avviene prima della comunicazione broadcast. vabbè lo dimostra per contraddizione con la definizione.\n- sta roba si chiama protocollo chandy-lamport\n- se fai così non ci stanno deadlock (continua ad accennarlo senza entrare nel dettaglio)\n\nTutta questa era la fase 1, poi passeremo agli atomic commit (blockchain e cose varie)","x":1237,"y":-8991,"width":780,"height":923},
		{"id":"941a38f0264b5e2e","type":"text","text":"# L2 (Lamport vettoriale - vector clock)\n\nse ci sono eventi concorrenti (quindi non c'è un ordine tra loro). i loro timestamp non possono essere numeri. allora posso modificare un po' la definizione dei timestamp, e renderli non numeri ma con la loro history. in questo modo$$e\\to e' \\Leftrightarrow TS(e)\\subseteq TS(e')$$\nin questo modo due eventi senza freccia sono semplicemente due eventi le cui storie non sono una un sottoinsieme dell'altra. ma la history di tutta la run! è un vettore. ad esempio, la timestamp di $e_2^3$ è $[1,4,2]$, ovvero il vettore dei lamport clock di ogni processo, così non pesa 5 terabyte dopo un'ora.\n\nvedi secondo grafico.\n\np_0 riceve $[141]$, quindi sa che deve ricevere prima una notifica da 1, 3 da 2 (ma di questo sono sicuro, perché il singolo canale è FIFO) e 1 da 3. nota che così non serve $\\delta$.\n\nquindi la condizione di correlazione $e_i\\to e_j$ è $$\\forall\\,k\\, VC(e_i)[k] \\leq VC(e_j)[k]\\quad\\wedge\\quad\\exists\\,k': VC(e_i)[k'] < VC(e_j)[k']$$se non metto la seconda condizione potrebbero essere lo stesso evento. ovviamente se so che i due eventi appartengono a processi differenti non serve la seconda condizione, anzi, basta confrontare il clock associato ad un singolo processo $k$ (e.g. \\[310\\] sul processo 1 avviene prima di \\[240\\] sul processo 2. controllando NON SONO CONVINTO DI QUESTA ROBA)\n\nvector clock usato nei DB distribuiti. così vediamo anche i deadlock (how?)\n\nreachable = run at some point is equal to the cut","x":-135,"y":-8990,"width":620,"height":923},
		{"id":"120f425563a323e0","type":"text","text":"run = ogni possibile esecuzione degli eventi in modo che l'ordine del singolo processo sia rispettato\n\ntopological order = consistent run?\n\ncioè tipo che su un grafo in generale non c'è topological order perché ci possono essere cicli (qui proviamo che non possono esserci cicli perché sono diagrammi space-time)","x":613,"y":-8725,"width":328,"height":393},
		{"id":"f10faebf80b04e16","type":"text","text":"# Failure Detection\n\nSarebbe bello sapere per certo se un processo è morto o no. È possibile?\n\nUn ***Failure Detector*** (***FD***) è un dispositivo che si occupa proprio di questo. Ci riesce? ... non sempre. Nella realtà i sistemi sono quasi sempre sincroni, eccetto brevi periodi in cui non lo sono. Come conseguenza, i FD producono risultati spesso consistenti ma in generale non perfetti.\n\nL'esistenza di un Perfect Failure Detector (***PFD***, i.e. un FD che in ogni istante evita \"falsi allarmi\" e che dopo un certo tempo finito realizza con certezza il *fail-stop* di un altro processo), risolverebbe gran parte dei problemi dei DS.\n\nPurtroppo un tale oracolo perfetto non esiste, ma ci possiamo avvicinare.","x":1986,"y":-333,"width":640,"height":363,"color":"6"},
		{"id":"41f7f87c3fbd9530","type":"text","text":"# Ben-Or - An Algorithm for Randomized Consensus\n\nOgni processo ha una preferenza che dà in input all'algoritmo (assumiamo sia un singolo `bit`).\n\nIl protocollo di Ben-Or risolve il consenso lanciando una moneta, e tollera ***fino a*** $t=\\floor*{n-1\\over2}$ ***failures***. Di contro, assume che ***i messaggi non possono essere persi***.\n\n- `send(1, round, preference)` - Ogni processo fa ***broadcast*** della propria preferenza;\n\t- Potremmo chiamare `1` la \"fase\" dell'algoritmo, è dovuta al fatto che il broadcast può essere di due tipi a seconda del punto del codice in cui mi trovo. Potrebbe arrivare una `send` di tipo `1` mentre sono in `wait` di `send` di tipo `2`, e questo numero mi permette di non conteggiarla. Avrei potuto chiamare i messaggi `send1` e `send2`? Probabile, fintantoché funziona.\n- `wait a number n-t of answers like (1, round, *)` - Aspetto di ricevere `n-t` messaggi del punto precedente. Per definizione di tolleranza, se ci sono più di `t` failures il protocollo si ferma;\n\t- Per il prosieguo del protocollo, sarebbe comodo provare a riceverne il più possibile. Per non rompere la liveness posso far partire un timer quando arriva l'`n-t`-esimo messaggio.\n- `if received > n/2 messages like (1, round, v) then` - Se vedo una maggioranza assoluta (i.e. $\\ceil*{n\\over2}$) per un singolo valore, allora...\n\t- `send(2, round, v, ratify) to everybody` - ... mando in broadcast una `send` di tipo `2` (i.e. siamo nella fase `2` del protocollo, in cui ho già ricevuto) con la quale certifico di aver visto una maggioranza per `v`. Non faccio `commit` finché non sono sicuro che tutti concorderanno su `v`;\n\t- Nota che se `t` è piccolo diventa più probabile vedere una maggioranza, ma ho poca tolleranza. Viceversa, valori di `t` vicini al limite della tolleranza riducono la probabilità di prendere una decisione, sia al primo round sia ai successivi (i.e. rallenta la convergenza).\n- `else: send(2, round, ?) to everybody` - Se non vedo una maggioranza mando in broadcast un messaggio del tipo `... quindi che dobbiamo fare?`;\n- `wait n-t (2, round, *) messages` - Aspetto di capire cosa hanno visto gli altri processi.\n- `if received even a single (2, round, v, ratify):` - Se almeno uno ha visto una maggioranza per `v`, ha senso supportare quella maggioranza. Cambio il mio valore in `v` (`preference <- v`);\n\t- `if received more than t (2, round, v, ratify) messages:` - Se poi più di `t` processi hanno visto una maggioranza, significa che quelli che potrebbero non averla vista sono meno di `n-t`. Segue che ogni processo che riceve `n-t` messaggi di tipo `2` vedrà almeno una `ratify` e cambierà il suo voto in `v`. In questo caso il consenso è garantito, quindi `output <- v`.\n- Se non vedo nessuna `ratify`, significa che per qualche motivo ogni processo è indeciso. Forse questa distribuzione di `0` e `1` non va bene, cambiamola in modo casuale tirando una moneta e andiamo al prossimo `round` sperando che la situazione migliori.\n","x":4575,"y":2080,"width":860,"height":932,"color":"4"},
		{"id":"0aa59bf09c992c32","type":"text","text":"# Chandy-Lamport Protocol\n\nEsiste un metodo più leggero rispetto al notificare $p_0$ di ogni evento?\n\nSe è $p_0$ a mandare tutte le `request` per conoscere i LS, non è detto che riesca a ricostruire un GS coerente (il sistema è asincrono). Allora delego ai processi.\n\n- $p_0$ manda una `take snapshot` con un `ID` univoco a tutti i processi;\n- Ogni processo che riceve una `take snapshot` può fare due cose:\n\t- la inoltra in ***broadcast***, se è la prima volta che la riceve;\n\t- la ignora se l'ha già ricevuta.\n- $p_0$ vede le risposte dei processi, realizzando un ***consistent cut***.\n\t- Come faccio a saperlo? Ipotizziamo non lo sia, e che quindi esiste una coppia di eventi $e\\to e'$ tali che $e\\notin C$ e $e'\\in C$. Segue che il cut deve essere fatto prima di $e$. Dal momento che i canali sono FIFO e il messaggio di `broadcast` è partito prima del messaggio di $e$, arriverà sicuramente prima l'evento di cut rispetto all'evento $e'$, il che implica $e'\\notin C$, da cui la contraddizione.\n\nÈ facile vedere che per $n$ processi vengono inviati $n$ `broadcast` (i.e. complessità di comunicazione $O(n^2)$). Il protocollo termina quando tutti i processi hanno ricevuto esattamente $n$ `broadcast`.","x":2240,"y":-3943,"width":665,"height":569,"color":"4"},
		{"id":"ffd7c9469255f9c8","type":"text","text":"# Atomic Commits\n\nFare ***commit*** significa aggiornare lo stato di un DB in modo ***irreversibile***.\n\nIn un DS, ogni DB deve concordare su ogni `commit` che viene eseguito. Se non c'è ***consenso*** unanime, la transazione viene rifiutata (i.e. ***abort***). \n\nIl singolo DB deve possedere sempre una ***copia coerente*** del DB distribuito, i.e. una versione della sequenza di modifiche sulla quale tutti concordano.\n\nQuesta necessità collettiva è più importante del `commit` che vorrebbe eseguire (e far eseguire a tutti) il singolo DB.\n\nDue criteri fondamentali per valutare i protocolli:\n\n- ***safeness*** - quanto il protocollo non permette copie incoerenti (in generale, è un \"il protocollo non deve fare cose sbagliate\");\n- ***liveness*** - quanto il protocollo \"fa qualcosa\" di buono, prima o poi.\n\nUn protocollo che non fa nulla è safe, ma non live. In generale si richiede ***massima safeness cercando di ottimizzare la liveness*** (i.e. fare in modo che il qualcosa di buono avvenga il prima possibile). ","x":-2968,"y":1332,"width":640,"height":539,"color":"6"},
		{"id":"c6a15cbad3909019","type":"text","text":"# Paxos Liveness Problem\n\nPaxos ha un problema con la liveness: anche se i messaggi non vengono persi, è possibile che ci mettano troppo tempo ad arrivare, il che li porterà ad essere ignorati.\n\nPoniamo che le `accept(1, x)` siano così lente da essere ad un certo punto date per perse (i.e. scade un timer). Un altro `P` invierà una `prepare(2)` e gli `A` risponderanno con una `promise(2, -, -)`. Quando le `accept(1, x)` arriveranno agli `A`, questi le ignoreranno in quanto nel frattempo hanno promesso di non partecipare a nessun `round` inferiore a `2`.\n\nQuesto può andare avanti ad oltranza, ed ogni `round` continuerà ad inibire il precedente.\n\nSoluzioni sono un timer adattivo (cf. Failure Detection) o un coordinatore (c.f. Multi-Paxos).","x":-3024,"y":4267,"width":753,"height":343,"color":"4"},
		{"id":"47a9499f82f58b66","type":"text","text":"# Esercizi III - Atomic Commit Protocols, Paxos\n\n**Exercise 13:** Give an ACP that also satisfies the converse of condition AC3 (\"The `commit` decision can only be reached if all processes voted `yes`\"). That is, if all processes vote Yes, then the decision must be Commit. Why is it not a good idea to enforce this condition?\n\n- Suppose we assume some $p_0$ has crashed, because we received all votes (all of them were `yes`) but his. Then if we apply this condition, we surely choose to `commit`. Then, we receive the vote from $p_0$. It was a `no`. Sad music. <span style=\"color:#FFA500\">Not really sure about this answer...</span>\n\n**Exercise 14:** Consider 2PC with the cooperative termination protocol. Describe a scenario (a particular execution) involving site failures only, which causes operational sites to become blocked.\n\n- Let's suppose the coordinator `C` fails after sending the `vote request`. All the processes have received the `vote request`, but one of them died. All the others voted `yes`. Eventually, each process will notice that no decision has arrived, so they will start asking each other. Since they all voted `yes` but one (whose vote is unknown despite the requests), it's impossible to discriminate the following cases:\n\t- The dead process `P0` voted `yes`. `C` decided to `commit` and sent the decision to `P0` only, then he died. `P0` received the decision, applied the `commit`, then died;\n\t- `P0` voted `no`, so he `aborted`, then died. `C` simply died at some point.\n\n**Exercise 15:** Show that Paxos is not live.\n\n- `accept` messages are so slow the timer expires at every round. `Acceptors` receive a new `prepare`, follows a `promise`, then the `accept` from a previous round arrive. Since the `Acceptors` made a `promise` referred to an higher round, any previous `accept` will be ignored. No `learn` will ever be sent, thus no vote and no chosen value at any round.\n  \n**Exercise 16:** Assume that acceptors do not change their vote. In other words, if they vote for value v in round i, they will not send learn messages with value different from v in larger rounds. Show that Paxos, with this modification, is safe. Unfortunately, the modification introduces a severe liveness problem (the protocol can reach a livelock).\n\n- Suppose each round a single `acceptor` decides a value to vote (i.e. a Quorum of `promises` is reached, then all `accept` messages get lost but one). It is possible to reach a \"stable\" situation in which all acceptors have decided a value but no value has a Quorum, thus no learner will ever learn anything (still the protocol goes on!).\n  \n**Exercise 17:** How many messages are used in Paxos if no message is lost and in the best case? Is it possible to reduce the number of messages without losing tolerance to failures and without changing the number of proposers, acceptors, and learners?\n\n- Theoretically, the minimum number of messages is given by\n\t- $Q$ `prepare` to obtain $Q$ `promise` (so $2Q$);\n\t- $Q$ `accept` to obtain $Q$ `learn` for every learner (so $Q+LQ$);\n\t- So it's $Q(3+L)$.\n- So yes it is, since in a real situation both `prepare` and `accept` are broadcasted to all the acceptors, so even in the best case we have $A(3+L)$ messages (of course, $A>L$).\n  \n**Exercise 18:** Assume that you remove the property that every round is associate to a unique proposer. After collecting a quorum of n-f promises (where n is the number of acceptors and f is such that n=2f+1), the proposer chooses one of the values voted in max round in the promises (of course it is not unique, the proposer chooses just one in an arbitrary way). Show that Paxos is not safe any more.\n\n- This is basically Fast Paxos with the wrong quorum. So another way of posing the question is: why $Q=\\text{floor}({n+1\\over2})$ is not enough anymore? In standard Paxos, $Q$ is enough for the following reason: since each and every round is associate to a unique proposer, the vote associated to the `maxround` is unique too. Thus, there's no ambiguity in choosing `x`by the rule of `maxround`. Why is this ambiguity a problem?\n\t- Suppose $n = 7$. At first round, two proposers start the protocol. All the acceptors reply with an \"`I never voted`\" statement, so `P1` feels free to send an `accept(1, x)` and same `P2` with `accept(1, y)`. For some reason, 4 acceptors receive the `accept(1, x)` and 3 the `accept(1, y)`. Let's suppose some learner (but not all of them!) learns `x`, since 4 is actually a quorum. Not all the learners have chosen the value, so the protocol goes on with, say, `round 2`. Let's assume a single proposer sends a `prepare(2)` and receives back 4 `promises` (which is actually the Quorum). Only, three of them say `I voted y` and only one says `I voted x`. Also, they have the same `maxround`. Since the only rule is to choose the value associated with `maxround`, the proposer may choose `y` and deliver to all the acceptors a `accept(2, y)`. A Quorum of acceptors may eventually tell the remaining learners to `learn(2, y)`. We're done with the protocol! Every learner has learnt a value, only for some of them it's `x` while for someone else it's `y`. This version of Paxos is not safe, but like we said  it is sufficient to rise the Quorum to ${2\\over3} n$ to achieve safeness (Fast-Paxos).\n\n**Exercise 19:** Assume that all proposers are learners as well. Let even rounds be assigned to proposers with the rules that we know. Moreover, if round 2i is assigned to proposer p, then also round 2i+1 is assigned to proposer p. Odd rounds are “recovery” rounds. If round 2i is a fast round and if the proposer of round 2i sees a conflict (it is also a learner), then the proposer immediately sends an accept for round 2i+1 with the value that has been most voted in round 2i, without any prepare and any promise. Is safety violated? If yes, show an example. If not, demonstrate safety. \n\n- I assume that the `learn` messages to the `Proposer/Learner` may be lost (and by that I also mean that they take so long they're eventually considered lost by some timeout event). Depending on the interpretation of the question (more specifically, of the word \"conflict\") I have two different answers. \n\t- `Counterexample` - Since no Quorum is ever mentioned in the question, even with, say, $|A| = 22$ the `Proposer/Learner` may receive only $3$ messages, where $2$ votes are a majority. In this case, it is absolutely ***not safe*** to force an `accept(2i+1, x)`, and this is quite obvious: in the remaining $19$ `learn` messages it is fairly possible that some other `x'` has reached a Quorum, which the `P/L` can't see. So, forcing `x` may result in some `L` learning `x` and some others learning `x'`;\n\t- `Safety Proof` - If it's implied that the conflict is within a Quorum of received `learn` messages, it is trivially safe:\n\t\t- If such majority is $>{1\\over3}|A|$, then not only it's safe, it's the only right value to be chosen;\n\t\t- If it isn't, by using the mutual exclusion in the spirit of the Fast Paxos Safety Theorem proof we can conclude that any value is safe. So not only this variation of the protocol is safe, it's optimal too.\n\n**Exercise 20:** You are an optimization freak. You realize that in Fast Paxos, in some cases, it is not necessary that the proposer collects n-f’ (the Fast Paxos quorum) promises to take a decision. Which is the minimum quorum and under what hypothesis this minimum quorum is enough to take a decision?\n\n- If, as it happens in real cases, the `proposer` is also a `learner`and he has learnt some value `x`, for the Fast Paxos Safety Theorem he doesn't need no `promise` at all, he just knows `x` is the value to `accept`;\n- Also if it's the first round, instead of `accept any` he could easily `accept(1, x)`, where `x` is its own value;\n- Finally, if the proposer receives $Q=\\ceil*{n+1\\over2}<\\ceil*{2n\\over3}=Q'$ promises where `x` is the only voted value, that's enough to `accept(x)`.","x":-3025,"y":5376,"width":1757,"height":1555,"color":"5"},
		{"id":"2456e7bc85395650","type":"text","text":"# Provare a costruire un PFD\n\nSe qualcuno mi dà un ***perfect failure detector*** posso risolvere la Leader Election. In ogni momento so chi è vivo, quindi se prendiamo ad esempio la logica del più basso ID è impossibile avere due leader. Potrei avere $0$ leader se il vecchio leader muore e gli altri processi ancora non lo sanno, ma solo fino ad un certo tempo $t'$ (dopodiché la Strong Completeness del PFD mi assicura che tutti lo sapranno).\n\nRisolvere Leader Election significa rendere live Paxos, quindi se esiste un PFD Paxos è live.\n\n... ma è possibile costruire un PFD?\n\nin effetti, se il sistema è asincrono nessuna delle proprietà può essere garantita come coppia. Tuttavia, partendo da $\\T$ o $W$ posso costruire $P$ o $S$. Come? Per definizione di Weak Completeness, esiste almeno un $p$ che sa del fallimento di $q$, quindi gli faccio fare broadcast a tutti gli altri.\n\nSia `Dp` l'elenco dei processi che $D_p$ sospetta essere morti. Inizialmente è vuota, ma quando si aggiorna $p$ aggiorna la propria lista dei sospetti e la manda in broadcast. Se ricevo i sospetti di `q`, aggiorno i miei sospetti aggiungendo i suoi (escludendo `q`, dato che se mi manda roba è chiaramente vivo). \n\n```\nDp <- 0\nloop forever\n\tsuspects <- Dp\n\tsend(p, suspects(p)) to everybody\n\twhen I receive the list (q, suspects(q)):\n\t\tDp <- Dp U suspect(q) - {q}\n```\n\nIn questo modo, $\\blacklozenge\\T\\to\\blacklozenge P$ e $\\blacklozenge W\\to\\blacklozenge S$. Ma posso arrivare a costruire $\\T\\to P$, $W\\to S$?\n\nSe il sistema è ***sincrono***, posso schedulare dei `ping` in `broadcast` ogni `T` secondi (***heartbeat***). Se non ricevo un battito entro `T`, sospetto. C'è solo un problema: non conosco `T`. Allora uso un mio `t`.\n\n- se `t` > `T` va tutto bene: il D è già un PFD;\n- se `t` < `T` la Completeness funziona, ma si rompe l'Accuracy. Se però ricevo il messaggio dopo `t'` $>$ `t` potrei rimuovere `q` dalla lista.\n\t- Da solo questo accorgimento non basta, perché l'Eventual Accuracy (sia essa Strong o Weak) deve essere vera da un certo punto in poi. Se `T`>`t` continuo ad aggiungere e rimuovere all'infinito;\n\t- Se però il messaggio arriva dopo il timeout, mi sta comunicando una cosa ben precisa: ***devo aumentare il timeout***. Diciamo che lo raddoppio. Dopo un certo numero di $\\times2$ supero `T` ed ottengo Eventual Strong Accuracy. Ma questo significa che da un certo punto in poi indovinerò il timeout giusto per tutte le coppie di processi, quindi da un certo punto in poi avrò un PFD! \n","x":4575,"y":178,"width":860,"height":1002,"color":"4"},
		{"id":"3611e372ae09c45c","type":"text","text":"# Proprietà dei Failure Detectors\n\nConsideriamo il ***failure detector*** $D_p$ (i.e. associato al processo $p$) che come un ***oracolo*** è in grado di sapere se un processo è morto o meno. Che proprietà richiediamo ad un simile oggetto?\n\n- ***Completeness*** - \"If a process fails, $D$ detects it\" (i.e. ***no false negatives***).\n\t- È completo un $D$ che dice sempre che $p_j$ è morto (though not very useful!);\n\t- Assumiamo che i fail siano tutti fail-stop. Data una run $\\s$ e gli insiemi $C(\\s,t)$ e $U(\\s,t)$ rispettivamente dei processi $\\text{crashed}$ e $\\text{up}$), definiamo $D_q(\\s,t')$ il set dei processi che il $D$ del processo $q$ sospetta essere crashati al tempo $t'$. Distinguiamo\n\t\t- ***Strong Completeness*** - \"When some guy crashes, ***at some point all the other guys*** realize it has crashed\":$$\\forall\\s\\quad\\forall\\,p\\in C(\\s,t)\\quad\\forall q\\in U(\\s,t)\\quad \\exists\\,t_0\\,:\\,\\forall\\,t_1\\geq t_0\\quad p\\in D_q(t_1,\\s)$$Formalmente, data una qualsiasi run ed un qualsiasi $p$ crashato ad un qualche tempo $t$, allora ***ogni $q$ ancora vivo*** si renderà ***definitivamente*** conto della morte di $p$, i.e. dopo un certo istante $t_0$, in tutti gli istanti $t_1$ successivi a $t_0$ $D_q$ classifica $p\\in C(\\s,t_1)$;\n\t\t- ***Weak Completeness*** - \"When some guy crashes, ***at some point some other guy*** realizes it has crashed\":$$\\forall\\s\\quad\\forall\\,p\\in C(\\s,t)\\quad\\exists q\\in U(\\s,t)\\quad \\exists\\,t_0\\,:\\,\\forall\\,t_1\\geq t_0\\quad p\\in D_q(t_1,\\s)$$Come prima, ma questa proprietà garantisce solo che ***almeno un $q$ ancora vivo*** si renderà ***definitivamente*** conto della morte di $p$.\n- ***Accuracy*** - \"If $D$ tells me some $p_j$ died, then $p_j$ really died\" (i.e. ***no false positives***).\n\t- È accurato un $D$ che dice sempre che $p_j$ è vivo (though not very useful!);\n\t- Anche qui facciamo distinzioni, in ordine di \"forza\" (ogni proprietà implica tutte quelle sotto).\n\t\t- ***Strong Accuracy*** - \"Nessun processo vivo sospetta mai la morte di un altro processo vivo\":$$\\forall\\s, t\\quad\\forall\\,p,q\\in U(\\s,t)\\quad p\\notin D_q(\\s,t)$$Se $p$ e $q$ sono processi vivi, nessuno dei due sospetta mai la morte dell'altro\n\t\t\t- ***Eventual Strong Accuracy*** - Dopo un certo tempo si ottiene la Strong Accuracy.$$\\forall\\s\\quad\\exists t_0 : \\forall\\, t_1\\geq t_0\\quad\\forall\\,p,q\\in U(\\s,t)\\quad p\\notin D_q(\\s,t_1)$$\n\t\t- ***Weak Accuracy*** - \"Esiste un processo vivo di cui nessun altro processo vivo sospetta la morte\".$$\\forall\\s,t\\quad\\exists p\\in U(\\s,t) \\,:\\,\\forall q\\in U(\\s,t)\\quad p\\notin D_q(\\s,t)$$\n\t\t\t- ***Eventual Weak Accuracy*** - Dopo un certo tempo si ottiene la Weak Accuracy.$$\\forall\\s\\quad \\exists t_0\\,:\\,\\forall t_1\\geq t_0\\quad\\exists p\\in U(\\s,t) \\,:\\,\\forall q\\in U(\\s,t)\\quad p\\notin D_q(\\s,t)$$\n\n","x":3293,"y":-682,"width":860,"height":1061,"color":"4"},
		{"id":"22eaf9203b8b3be2","type":"text","text":"# Tassonomia dei Failure Detectors\n\nA seconda di quali di queste proprietà sono rispettate, abbiamo una tassonomia di $D$.\n\n| Completeness\\Accuracy | **Strong **         | **Weak **  | **Eventual Strong **                   | **Eventual Weak ** |\n| --------------------- | ------------------- | ---------- | -------------------------------------- | ------------------ |\n| **Strong **           | $P$ (***Perfect***) | S (Strong) | $\\blacklozenge P$ (Eventually Perfect) | $\\blacklozenge S$  |\n| **Weak **             | $\\T$                | $W$ (Weak) | $\\blacklozenge\\T$                      | $\\blacklozenge W$  |\n\nOvviamente vorremmo un PFD. L'idea è ***costruirlo*** a partire dalle proprietà meno forti. Passo da Weak a Strong Completeness con i protocolli di broadcast, e costruisco l'Accuracy sotto l'assunzione che il sistema sia sufficientemente sincrono.","x":3293,"y":505,"width":860,"height":348,"color":"4"},
		{"id":"59ebe3510cce29a1","type":"text","text":"# Quindi non si può risolvere il consenso?\n\nCerto che si può, in due modi\n\n- Introduco randomization (e.g. bitcoin, proof of work, proof of stake). Questa idea assume che per $t\\to\\infty$ la probabilità di fare cose giuste e non fare cose sbagliate tende a zero. Il problema però è che ci devo andare abbastanza in fretta, altrimenti rischio di rompere la safeness\n\n- perfect failure detector? I mean if I only care about non-faulty processes then the vote of a failed process is not needed, I may take a decision with only the non-failed ones\n\t- Questo include il l'ipotesi di sistema sufficientemente sincrono, e se non lo è smetto di essere live (e.g. paxos)","x":4678,"y":1419,"width":654,"height":364,"color":"3"},
		{"id":"c85d7ed0a40f14b3","type":"text","text":"# Ben-Or - Esempio\n\nDiciamo che $n=5$ e un processo muore. I voti dei processi vivi sono $\\{0,0,1,1\\}$.\n\nTutti manderanno una `send(2, round, ?)`, perché nessuno vede maggioranze di alcun tipo (non ne esiste una!), quindi nessuna `ratify` viene mandata e tutti lanciano una moneta. Si ricomincia sperando di ottenere $\\{0,1,1,1\\}$ o $\\{0,0,0,1\\}$.\n\n- Se su $5$ ne restano $3$, per raggiungere il consenso è necessario che esca a tutti lo stesso numero quando lanciano la moneta;\n- Può terminare in un solo round? Perché sia così devo ricevere `n-t` messaggi tutti concordi (e.g. `1`), così sono certo che anche se i restanti `t` non concordano (i.e. votano `0`) nessun altro processo può aver visto una maggioranza diversa dalla mia. \n\t- Se $n= 5$, ne servono $3$. Ma $p (3\\text{ voti uguali})={1\\over8}$. In generale$$p(k\\text{ voti uguali})={1\\over 2^k}$$\n\t- Ma magri aspetto un po' di più. Metto un timer alla ricezione dell'$(n-t)$-esimo messaggio, e assumendo che il sistema sia \"abbastanza sincrono\" potrei riuscire a vedere una maggioranza. A quel punto mando una `ratify`, spero di osservare una maggioranza di `ratify` per poi, infine, giungere al consenso.\n","x":4205,"y":3280,"width":740,"height":543,"color":"4"},
		{"id":"4c8cb86213c84578","type":"text","text":"# Ben Or - Properties\n\n1. At most one value can get a majority in phase 1;\n2. If some process `A` sees $t+1$ `(2, round, v, ratify)` then everybody sees at least one `(2, round, v, ratify)`;\n\t1. `Proof` - Immaginiamo di essere uno degli everybody. Diciamo che A riceve t+1 ratify, quindi B riceve n-t messaggi di phase 2. tra questi ce n'è almeno uno delle ratify.\n3. If every process has received at least one `(2, round, v, ratify)` then every process will vote for `v` in round `round+1`.\n\nquesto protocollo non è molto usato, ma è importante a livello teorico perchè mostra che usare la randomness permette di superare il limite di FLP\n\npoi fa un discorso in cui t=1 ma mantiene n generico, penso sia interpretabile come parametro `t` del codice il cui massimo è $t=\\floor*{n-1\\over2}$.\n\ntypically systems are synchronous and fails are fail-recovery by some fix\n\ntypical preference in DDB is 1 (`commit`), so conflicts are rare.","x":5065,"y":3280,"width":740,"height":543,"color":"3"},
		{"id":"bcdd06dbc6b09ef9","type":"text","text":"# In sostanza:\n\nRiassumendo, la logica è questa:\n\n- Il numero di failures che posso tollerare è ***al più*** $t_{max}=\\floor*{n-1\\over2}$, mentre la maggioranza è ***esattamente*** $M=\\ceil*{n\\over2}$;\n- Se scelgo un quorum $Q_{min}=n-t_{max}$ significa che devo aspettare $\\sim M$ messaggi. La probabilità di osservare una maggioranza nel quorum è quindi circa uguale alla probabilità di ricevere $M$ voti concordi su $\\sim M$ voti totali, che è $\\sim {2^{-M}}$;\n- Scegliere un quorum $Q>Q_{min}$ significa tollerare $t<t_{max}$ failures, ma incrementa la liveness del protocollo. La probabilità di ricevere $M$ voti uguali su $N>M$ voti è$$\\sum_{k=M}^N\\binom{N}{k}p^kq^{N-k}=\\sum_{k=M}^N\\binom{N}{k}2^{-N}$$che si può dimostrare formalmente essere $>2^{-M}$ ma non lo farò in questa sede, ci accontentiamo dell'intuizione;\n- In alternativa, lascio $Q_{min}$ e semplicemente quando il protocollo mi consentirebbe di proseguire (i.e. quando ricevo $Q_{min}$ messaggi) aspetto invece di riceverne $Q>Q_{min}$. In questo modo prendo il *best of both worlds*: la tolleranza resta la massima possibile, ma la probabilità di osservare una maggioranza diventa un po' più grande.","x":4635,"y":3980,"width":740,"height":544,"color":"4"},
		{"id":"26aab522b9b35b7c","type":"text","text":"# Esercizi II - Chandy-Lamport\n\n**Exercise 6:** Show that the Chandy-Lamport Snapshot Protocol builds a consistent global state.\n\n- Let's assume it doesn't, so $\\exists\\,e\\to e'$ such that $e\\notin C$ and $e'\\in C$. The cut happens before $e$. Since the channels are FIFO, the `broadcast` message containing the \"`please, cut`\" information will be delivered before the message containing the \"$e\\to e'$\" information. So necessarily $e'\\notin C$, thus the contradiction.\n  \n**Exercise 7:** Show that the Chandy-Lamport Snapshot Protocol can build a global state that never happened.\n\n- Since the system is asyncronous, the `take snapshot` messages may take a long time. Let's say $p_2$ is hard to reach, in some way. Then, in the meantime any number of \"non-receive\" events may happen on $p_2$. \n\n**Exercise 8:** What good is a distributed snapshot when the system was never in the state represented by the distributed snapshot? Give an application of distributed snapshots.\n\n- Even though the global state constructed by the `take snapshot` request isn't a \"real\" global state, still it is consistent. This is enough to grant solution to the distributed mutual exclusion problem, i.e. deadlock avoidance.  <span style=\"color:#FFA500\">Giusto??????</span>\n\n**Exercise 9:** Consider a distributed system where every node has its physical clock and all physical clocks are perfectly synchronized. Give an algorithm to record global state assuming the communication network is reliable. (Note that your algorithm should be simpler than the Chandy–Lamport algorithm.)\n\n- $p_0$ sends $N$ messages at time $t_0$ saying to `take snapshot` at time $t_1\\geq t_0+\\d$ (assuming $\\d$ is the upper time bound for the synchronous messages to be delivered). Then, receives back $N$ snapshot, making the complexity $O(N)$ instead of $O(N^2)$.\n\n**Exercise 10:** What modifications should be done to the Chandy–Lamport snapshot protocol so that it records a strongly consistent snapshot (i.e., all channel states are recorded empty).\n\n- Let's assume that to every `receive` event follows an `ACK` to the `sender`.\n\t- When $p_1$ receives a `take snapshot`, he must be sure that his ***outgoing*** asynchronous channel is empty, i.e. he must wait all the `ACK` messages from the previous messages he sent. In the meantime, the events are put to `sleep` (or better, to `wait`);\n\t\t- One may note that he's not sure about the delivery of his `ACKs`. We'll see it later.\n\t- Eventually all the `ACKs` will arrive, and $p_1$ will be able to send a `clear` message in broadcast to all the other processes;\n\t- But that's the same mechanism for any $p_i$;\n\t- When a process receives $N$ `clear` messages he's sure that his `ACKs` had been delivered too, so this is the right moment to send to $p_0$ his local state;\n\t- We're sure that eventually every process will provide a strongly consistent snapshot.\n- When $p_1$ receives the `take snapshot 2`, he `waits` and `broadcasts` a `take snapshot 2`. Any process that receives the `take snapshot 2` from $p_1$ knows that $p_1$ won't send any message anymore. So, say, $p_2$ `waits` and `broadcasts` the `take snapshot 2`. What are they waiting for? Every process should respond to the sender of `take snapshot 2` with an `ACK` message. When $p_1$ receives the `ACK` from $p_2$, he's sure $p_2$ won't send any message to him anymore. When every process is sure that no other process can send any message (i.e. $p_i$ has received either an `ACK` or a `take snapshot 2` from any other $p_j$), then he can take the snapshot. Channels are empty.\n**Exercise 11:** Show that, if channels are not FIFO, then Chandy–Lamport snapshot algorithm does not work.\n\n- Let's consider a simple situation in which $p_1$ sends in close sequence a `take snapshot` broadcast and a `send` message to $p_2$ (let's call this `sending` event $e$). Let's assume that in order to perform the cut $p_2$ needs the `ts` message from $p_1$ (i.e. all the others `ts` to be delivered to $p_2$ are slower than the one from $p_1$). If the channel is not FIFO, then it is possible that $p_2$ first `receives` the message from event $e$ (let's call this event $e'$) and then the `ts`. Then, $e'\\in C$ and $e\\notin C$, but $e\\to e'$, so $C$ is not consistent.\n  \n**Exercise 12:** Let S0 be the global state when the Chandy-Lamport snapshot protocol starts, S be the global state built by the protocol, and S1 be the global state when the protocol ends. Show that S is reachable from S0 and that S1 is reachable from S. Remember that S might not have happened.\n\n- $S_0$ and $S_1$ are real global states. By definition of local state, after $i$ events the process $p$ is in the state $\\s_p^i$. Let's call $\\s_p^i(S_0)$ the local state seen by global state $S_0$ and let's suppose we also have $\\s _p^j(S)$ and $\\s_p^k(S_1)$. Then $\\forall\\,i$ it must be true that $i\\leq j\\leq k$. Thus we conclude that $S_0\\sse S\\sse S_1$. But we proved (by induction) that if $C_1\\ss C_2$ are consistent cuts there exists some consistent run that reaches $C_2$ from $C_1$.\n\t- If both the $\\sse$ are equalities, the thesis is trivial. If one or both are $\\ss$ one can use the proof by induction.","x":3100,"y":-4456,"width":940,"height":1553,"color":"5"},
		{"id":"570439671731e83f","type":"file","file":"0 - Definitions/Ex5.png","x":-2358,"y":-5370,"width":400,"height":222},
		{"id":"8876ea52fed47dda","type":"text","text":"# Esercizi I - Definizioni e Logic Clock\n\n**Exercise 1:** Let C1 and C2 be two consistent cuts. Show that the intersection of C1 and C2 is a consistent cut.\n\n- By definition, if $e_1'\\in C_1$ and $e_1\\to e_1'$ then $e_1\\in C_1$ (same for $C_2$). If $e'\\in C_1\\cap C_2$, then $e'\\in C_1$ ***and*** $e'\\in C_2$. Since $C_1$ and $C_2$ are consistent, if $e\\to e'$ it is also true that $e\\in  C_1$ ***and*** $e\\in C_2$, which means $e\\in C_1\\cap C_2$, so $C_1\\cap C_2$ is consistent.\n\n\n**Exercise 2:** Let C1 and C2 be two consistent cuts. Show that the union of C1 and C2 is a consistent cut.\n\n- Same as before. If $e'\\in C_1\\cup C_2$, then $e'\\in C_1$ ***or*** $e'\\in C_2$. Since $C_1$ and $C_2$ are consistent, if $e\\to e'$ it is also true that $e\\in  C_1$ ***or*** $e\\in C_2$, which is sufficient for $e\\in C_1\\cup C_2$ to hold, so $C_1\\cup C_2$ is consistent.\n  \n**Exercise 3:** Show that every consistent global state can be reached by some consistent run.\n\n- One may represent a cut as a collection of local states, so a consistent global state can be seen as a consistent cut. That said, a consistent run is a run in which every relation $e\\to e'$ is respected, thus can be seen as a sequence of consistent cuts, i.e. a sequence of consistent global states. <span style=\"color:#FFA500\">Vale??</span>\n- By induction on the number $n$ of events in the global state.\n\t- Case $n=0$ is trivial, if GS has no events then the empty run reaches it;\n\t- If case $n$ is true, consider the case $n+1$ and call $e$ the last event ($e\\in S$) If we remove $e$ then we may use the inductive hypothesis. Then, we just add $e$ to the run. Since $e$ was the last event in a consistent GS, there's no event $e'\\in S$ such that $e\\to e'$, so the resulting run is consistent.\n  \n**Exercise 4:** Let C1 and C2 be two consistent cuts. If C1 is a subset of C2, then C2 is reachable from C1. (There exists a consistent run that reaches C1 and then reaches C2.)\n\n- By induction on the number $n$ of events after the last event in $C_1$.\n\t- In the case $n=1$ in which $C_2$ differs from $C_1$ by only one event $e$ and $C_1$ is a consistent cut, then the run adding $e$ is consistent no matter the process it happens on;\n\t- Let's assume it true for the case $n$ and consider the case $n+1$. If we remove the last event $e$, by inductive hypothesis we obtain a consistent cut $C_2'$ reached from $C_1$ by a consistent run. The run obtained by adding $e$ to that consistent run is again a consistent run (same reasoning as in the base case).\n  \n**Exercise 5:** Label with proper logical clock all the events of the distributed computation in image vector.pdf. (You can consider events that receive a message and immediately send it as single events.)","x":-1790,"y":-5824,"width":695,"height":1131,"color":"5"},
		{"id":"2b4be9d6d8ebf9a6","type":"text","text":"# Hashing e Computational Puzzles\n\nDopo aver effettuato un pagamento (che quindi è stato approvato dal sistema) e ricevuto un bene, un singolo miner prova a modificare la transazione in modo da recuperare i coin spesi. \n\nSi può fare? In teoria sì, ma viene reso estremamente difficile dall'***hashing*** dei blocchi. Ogni blocco ha un valore di hash che dipende dalle transazioni al suo interno, ma anche dall'hash del blocco precedente. Modificare un blocco significa non solo modificare il suo hash, ma rendere inconsistenti tutti gli hash di tutti i blocchi successivi. Un attaccante dovrebbe quindi modificare una sequenza di hash, che è notoriamente un problema difficile da risolvere ma di cui è facile da verificare la soluzione (tuttavia non è formalmente un problema $\\NPH$).\n\nIn virtù della difficoltà del problema, le Blockchain ***PoW*** sfruttano l'hashing per implementare il ***computational puzzle***: solo chi riesce a trovare il giusto hash per il blocco che vuole proporre può aggiungerlo alla Blockchain.\n\nUn'idea di come funziona è il meccanismo usato da ***Bitcoin***: per essere un blocco valido, l'hash in uscita deve avere i $k$ bit meno significativi uguali a $0$.\n\nOra, la funzione di hash (e.g. ***SHA-256*** per Bitcoin) prende in input le transazioni del blocco che voglio far approvare, l'hash del blocco precedente e (oltre ad altra roba che non stiamo a specificare in quanto non utile alla trattazione) un ***nonce***. In generale non ho idea di cosa esca fuori come hash di output, posso solo cambiare il nonce e guardare il risultato.\n\nLa probabilità di trovare proprio quello che voglio (i.e. i $k$ zeri finali) richiede un numero di lanci quantomeno esponenziale per diventare di ordine $1$, da cui la difficoltà probabilistica del problema (si capisce meglio perché non è classificabile come $\\NPH$). Una volta risolto il problema (i.e. trovato il nonce), è facile per gli altri miner verificare la soluzione (basta far girare SHA-256 con i parametri indicati dal miner risolutore).","x":9910,"y":2194,"width":780,"height":711,"color":"4"},
		{"id":"ac892db9755dc4d6","type":"text","text":"# Creare Cash Digitale\n\nPochi accetterebbero come pagamento il carbone, ma molti accetterebbero i diamanti. Eppure hanno la stessa composizione chimica. Cosa cambia? I diamanti sono rari.\n\nIn generale, se si vuole creare una moneta digitale si deve tenere presente che il suo valore è legato alla sua quantità. Deve quindi esserci in qualche modo \"difficile\" creare nuovo cash.\n\nUna soluzione a questo problema viene direttamente da una tecnica anti-spam per email che risale al 1992 (Dwok, Naor), basata su un principio molto semplice: per inviare una mail devo risolvere un ***computational puzzle*** legato in modo univoco a quella specifica mail, altrimenti questa non partirà. Se risolvere il problema per una mail `A` non dà alcun aiuto per la risoluzione del problema per la mail `B`, allora diventa computazionalmente difficile inviare grandi quantità di mail tutte insieme.\n\nQuesto stesso principio si applica in generale alle Blockchain ***PoW***: il cash viene creato ogni volta che un miner aggiunge un blocco alla blockchain, i.e. solo quando questo è stato in grado di risolvere il computational puzzle. In particolare, la prima transazione di ogni blocco assegna nuovi coin all'indirizzo del miner che sta costruendo quel blocco. \n\n\"Ma così continuo ad erogare all'infinito!\". Ni. In genere esiste un meccanismo di riduzione del reward. In ***Bitcoin***, ad esempio, il reward iniziale era di $50\\,\\BTC$ per blocco, ma il sistema stabilisce che ogni $210\\cdot10^3$ blocchi aggiunti alla Blockchain (in che si traduce in un intervallo di tempo di qualche anno) questo valore si dimezza. Questo consente di dire che in una prima \"era\" sono stati prodotti $21\\cdot10^3\\times50=10.5$ milioni di bitcoin, poi $\\times 25$ e via dicendo. Questa è ovviamente una serie convergente: se le regole restano queste, il massimo numero di Bitcoin in circolo sarà $21$ milioni. Ad oggi il reward è di $3.125\\,\\BTC$. ","x":8880,"y":2194,"width":780,"height":711,"color":"4"},
		{"id":"082491dd288619d2","type":"text","text":"peer-to-peer file-sharing networks must deal with the problem of\n“freeloaders,” that is, users who download files without sharing in turn. While swapping files might work, there is also the issue of coordination: finding the perfect person who has exactly the file you\nwant and wants exactly the file you have. In projects like MojoNation and academic proposals like\nKarma, users get some initial allocation of virtual cash that they must spend to receive a file and earn\nwhen they send a copy of a file to another user. In both cases, one or more central servers help keep\ntrack of users’ balances and may offer exchange services between their internal currency and\ntraditional currency. While MojoNation did not survive long enough to implement such an exchange,\nit became the intellectual ancestor of some protocols used today: BitTorrent and Tahoe-LAFS.","x":-2880,"y":-3407,"width":840,"height":394},
		{"id":"7e4b0290f28c62f1","type":"text","text":"# DS su Rete Internet","x":-3981,"y":-303,"width":640,"height":303,"color":"6"},
		{"id":"c900344b43c66363","type":"text","text":"# Privacy /w TOR\n\nDiversi tool permettono un certo livello di privacy nei confronti di diverse entità\n\n- incognito mode dà privacy rispetto ad users dello stesso computer, ma evita anche di salvare i cookies. https ti encripta il pacchetto (evita deep packet inspection) , ma non gli IP.\n- VPN, Proxy - actually we don't use VPN for privacy, but for appearing in some other place. you don't get any privacy with the VPN company \n- TOR routing - ne parleremo\n\nthe great firewall of china? if you bought a pc in china there was a filter in the DNS system of the computer. initially, there only was a file with a blacklist (find the file, remove the firewall), now i'ts more complicated. VPN solve it, but not all of them.\n\nTOR non è utilizzabile in china.\n\nTOR è una rete di relays runnati da volontari (no reward). Ne scegli tre a caso e setup the connection. quando il packet parte, le informazioni del percorso sono tutte là. allora devo crittarle. ho una key con ognuno dei tre server (la loro public?)\n\ndiciamo che i relay sono mosca-rio-sidney. Applico a catena sul pacchetto un'encription con le chiavi sidney-rio-mosca. mosca (guard node). mosca decritta e sa\n\n- di me, tutto. IP e cazzi vari\n- di dove voglio andare, solo che voglio andare a rio.\n\nse mi connetto dalla sapienza, questa sa che mi sto connettendo con mosca, ma non sa nient'altro. E rio?\n\n- su di me non sa nulla\n- su dove voglio andare, legge Sidney (i.e. non sa davvero dove voglio andare).\n\nsidney (exit node) non sa chi sono, ma sa che voglio andare, diciamo, alla CNN.\n\nessendo il nodo più importante, perché tu venga selezionato come guard devi essere nella rete da un bel po', perché in quel caso posso registrare gli IP di chi usa TOR. allo stesso modo, l'exit node registra quali server sono visitati (of course you can't match input-output).\n\nI paesi in cui TOR va più di moda sono free countries, tipo germania, italia e USA. e viene usato tendenzialmente per cose normalissime (e.g. ricerche su google, newspapers, ...).\n\nnon mi serve una public key per usare tor (quella del relay sì?), posso usare Diffle-Hellman, che è un protocollo per concordare su una chiave con qualcuno di cui non so nulla (e lui non sa nulla di me). \n\n- Alice sceglie un numero casuale x e bob stessa cosa con y (x e y $\\in Z_g$, i.e. modulo g, devono ovviamente concordare su g. tutti usano lo stesso g a quanto ho capito);\n- Alice manda a bob $g^x$ (tutti lo vedono, ma il logaritmo a quanto pare è difficilissimo, quindi entro una certa approssimazione solo alice conosce x)\n- Stessa cosa bob con $g^y$\n- Alice crea la chiave come $K=(g^y)^x$ e bob stessa cosa con esponenti al contrario.\n- l'unico modo di ricostruire la chiave è di conoscere uno dei due messaggio (e.g. $g^x$) e separatamente l'altro numero casuale (e,g, y).\n\nquesta roba è suscettibile a MitM. se può solo vedere, non c'è problema. se però\n\n- intercetta $g^x$ e manda a bob $g^{x'}$, stessa cosa con Alice\n- a quel punto alice costruisce una key $(g^{y'})^x$ e stessa cosa bob.\n- alice e bob non condividono una chiave comune, ma la condividono separatamente con il MitM, il quale può continuamente intercettare, decrittare, leggere, crittare con l'altra chiave  e dare all'altro la sensazione di star parlando con alice\n\nma su tor conosco la public key del relay. allora critto $g^x$ con la public key del relay. ma quando non ho una chiave pubblica, il relay non può crittare con quella.\n\ne allora? il relay ha costruito K, allora manda indietro $g^y$ ***e*** (K, \"`handshake`\") crittata con K. Alice ottenendo $g^y$ può ottenere K e verificare che decrittando si ottenga \"`handshake`\".\n\nil relay non sa nulla di me, a parte il mio IP. quindi in pratica il protocollo è\n\n- mi connetto con mosca e uso DH per ottenere le chiavi;\n- tramite mosca con cui ho la connessione crittata negozio le chiavi con rio;\n- stessa cosa con sidney.\n\novviamente anche ogni coppia di relay negozia una chiave e critta ulteriormente \nattacchi plausibili?\n\n- traffic analysis - se per qualche motivo possiedi sia guard che exit. non sai che lo sei per entrambi, ma\n\t- quando mandi un pacchetto con la guard, ne osservo uno all'exit\n\t- quando non lo mando, non vedo niente all'exit\n\t- inizio ad assumere che siano correlati\n\t\t- per evitare questo la rete deve usare metodi per evitare questa cosa, tipo aggiungere delay fittizi. questo però appesantisce la rete;\n\t\t- la cosa migliore è scegliere sempre guard ed exit che appartengono a diversi AN. il governo USA ne possiede un botto, se arrivasse a possederne più del 50% ci sarebbe il 25% di possibilità che sia guard che exit appartengano a loro, e che quindi la connessione venga decrittata.\n\nora, l'utente è protetto, ma il target server no. come si dà privacy anche a lui? se proteggo il web server sono ufficialmente entrato nel dark web, in cui non so dov'è il server. prendiamo come esempio Silk Road. se ti ci vuoi connettere, non sai qual è il suo IP, ma usi un onion address (`.onion`). Come funziona (sketch)?\n\n- Silk Road sceglie alcuni relay e li chiama Introduction Points (IP). si connette separatamente ad essi tramite circuiti tor diversi, i quali non sanno l'ip di chi sta dall'altra parte. Silk Road comunica agli IP il servizio che offre;\n- l'utente si connette con tor ad un lookup server che gli fornisce l'IP dell'IP.\n- l'utente sceglie il rendez-vous, genera un cookie casuale e comunica tutto questo all'IP\n- l'IP comunica con Silk Road con il canale di prima, e gli dice che c'è uno che vuole connettersi con lui tramite un certo rendez-vous con un certo cookie;\n- silk road si connette con TOR al rendez-vous col cookie, la connessione è stabilita.\n\ncercati la storia di silk road\n\nIl governo cinese le sta provando un po tutte. blocca qualsiasi connessione verso i relay (che sono noti). ma ci sono degli hidden nodes! allora prova a fare analisi per capire se una connessione sembra simile ad una connessione tor. se lo sembra, nel dubbio la blocca. \n\nvelocità della connessione? sicuramente tanto delay, bandwidth dipende da che nodi becco sul circuito.","x":-7760,"y":-1130,"width":1740,"height":1956},
		{"id":"b51bcda4f72fdde5","type":"text","text":"### Origine del nome \"Byzantine\"\n\nIl termine deriva dal cosiddetto **\"Problema dei Generali Bizantini\"** (_Byzantine Generals Problem_), un problema teorico descritto in un famoso articolo di Leslie Lamport, Robert Shostak e Marshall Pease nel 1982.\n\nL'idea dietro il nome è legata a uno scenario in cui un gruppo di generali di un esercito bizantino deve coordinarsi per attaccare o ritirarsi da una città. Tuttavia, alcuni di questi generali potrebbero essere traditori e potrebbero inviare informazioni false agli altri. Il problema diventa come garantire che i generali leali possano giungere a un accordo su una strategia comune, nonostante la presenza di traditori (ovvero, processi difettosi o maliziosi).\n\nQuesto nome pittoresco viene usato per descrivere l'aspetto complesso e potenzialmente confuso dei fault che possono accadere in un sistema distribuito quando uno o più processi non si comportano in modo affidabile o coerente.","x":-5040,"y":1381,"width":680,"height":442,"color":"#4545ff"},
		{"id":"e5d0dbbdf0c65fa4","type":"text","text":"# Esercizi IV - Failure Detector, Randomized Consensus\n\n**Exercise 21:** Show a possible implementation of a failure detector and discuss its properties in the model of distributed system of your choice. \n\n- Every process in the DS may `broadcast` its `heartbeats`, i.e. periodically send to everyone some sort of \"`I'm alive`\". Any process `p` who receives not the `heartbeat` of `q` within some time `t` (so `p` is assuming the system is somewhat synchronous) proceeds to `suspect` the process `q` to be dead, i.e. `p` adds `q` to the dead processes list. This implementation only assures Weak Completeness. To achieve Strong Completeness, one may `broadcast` the list of its `suspects`. Eventually all the processes will know some `q` is dead. We built Strong Completeness. But what if `q` is not dead? We said nothing about Accuracy, because so far this system has none. How do we build it?\n\t- If `q` is alive, `p` may delete him from the list of `suspects`. But this event also tells `p` he should increase its `t`, al least for communications with `q`. Under the assumption that the channel is somewhat synchronous, doubling `t` will eventually reach the $\\d$, making the system at least $\\blacklozenge S$. By means of `broadcasting` we may build $\\blacklozenge P$.\n\t- If the system is \"synchronous enough\", adjusting all the `t`-values for each `p-q` couple one may end up building a $P$ failure detector. We're daydreaming. Real systems alternate synchronous and asynchronous phases, so we may accept $\\blacklozenge P$ as a good approximation. Moreover, it's possible that doubling `t` during an asynchronous phase may lead to an overestimate during the synchronous phases. One may consider halving `t` if a message from `q` arrives in less than half `t`.\n  \n**Exercise 22:** Show that you can make Paxos live with a failure detector in W ($W$ is both weak completeness and accuracy, ndA).\n\n- Solving Leader Election means making Paxos live. A weak failure detector means that\n\t- at least one alive process is never suspected to be dead (weak accuracy);\n\t- after some time $t$, at least one process knows if someone crashed (weak completeness).\n\t\t- Riflettendo su come usare questa seconda condizione mi rendo conto del perché i FD con weak completeness non sono neanche presi in considerazione...\n- If every process `broadcasts` the list of his `non-suspected` processes, by means of weak accuracy we're sure that at least one process is in each of them. Thus, when a process receives all the `broadcasts`, he may consider the intersection of all the `non-suspected` processes and be sure it won't be empty. By means of weak completeness, we know that at least one process knows that someone crashed, so when we consider the intersection all dead processes are eliminated. The smallest ID of the intersection is the leader.\n  \n**Exercise 23:** Show how to solve consensus in a synchronous distributed system with at most f crash failures.\n\n- It is sufficient to `broadcast` your own ID $f+1$ times. If we consider the single round, the worst case is that the leader dies in the middle of the `broadcast`, making half of the processes think, say, `P0` is the leader and the other half `P1` (half of the processes don't know `P0` exists). So, a sequence of $f$ worst cases would still produce $2$ leaders. Round $f+1$ solves the job. \n\n**Exercise 24:** Build a run of the Ben-Or randomized consensus algorithm that never terminates.\n\n- A number $\\floor*{n-1\\over2}$ of processes die, ?????????????????????????????\n  \n**Exercise 25:** Consider an asynchronous system of 5 processes that run the Ben-Or randomized consensus algorithm. The number of failures that the system allows is 2. Show that, if at most 2 failures occurs, than the probability that the protocol terminates after x rounds (or more) is smaller  than alpha^x, for some alpha.\n\n- In the worst case of $2$ failures, to achieve a majority all the remaining processes must vote for the same value. We may assume both initial votes and coin flip `0` or `1` with probability $1\\over2$, thus the probability for the votes to be all `1` or all `0` is $p_3={1\\over2}^3$. Then, the probability of ***not*** reaching consensus is $\\b=1-{1\\over2}^3$. One reaches consensus\n\t- at `round 1` with probability $p_3$;\n\t- at `round 2` with probability $\\b p_3$\n\t- ...\n\t- at `round x` with probability $\\b^{x-1}p_3$\n- So we can conclude that the probability of terminating at `round x` is smaller than $\\a^x$ for some $\\a$.","x":2983,"y":5512,"width":1480,"height":1284,"color":"5"},
		{"id":"35db17127a9f9fb0","type":"text","text":"# Formulazioni equivalenti del Consenso\n\n### Byzantine Agreement (BA)\n\nn processes, one process has a value (typically called source process). Coniughiamo le proprietà del consenso nel seguente modo:\n\n- agreement - all non-faluty processes must agree on the same value\n- validity - if the source is non-faulty, then the value must be its value\n- termination - every non--faulti process must eventually choose a value\n\n### Consensus Problem (CP)\n\nn processes, each process has a value. agreement come prima, validity è un po' strana: \"if all values (of the non-faulty processes?) are the same, then all non-faulty processes must agree on that value\". abbastanza debole! perché? non lo dice, chissà. Termination è la stessa.\n\n### Interactive Consistency (IC)\n\nn processes, each process has a value, processes must agree on a vector of n values.\n\n- Agreement - [...] same vector $v$\n- Validity - if process $i$ is non-faulty, then $v[i]$ must be the input value at process $i$.\n- Termination - same\n\n\n### ... e che ci faccio?\n\nPossiamo vedere che possiamo operare delle riduzioni da una definizione all'altra (i.e. sono tutti lo stesso problema). scegliamo un setting, e.g. crash e synchronous system e vale per tutti gli step delle riduzioni. Facciamone una (c'è all'esame!!). Funziona così: se riduco da A a B ho l'input di A, l'algoritmo che risolve B e la soluzione deve essere quella di A. La riduzione da BA a CP (si indica con una sorta di $\\sse$ ma sopra è quadrato aperto e non a cerchio) si fa nel seguente modo:\n\n- source fa `broadcast` a tutti, ora ogni processo ha un value, ma per la validity so già che devo arrivare a quell'agreement, quindi la soluzione che trovo con CP è la stessa a cui arrivo con BP.\n- what if there are faulty processes? non ci sono problemi, a meno che non fallisca il source. in questo caso validity BA non è rispettata. quindi CP può scegliere qualsiasi valore. \n\nfacciamo da IC a CP. a turno, il nodo $i$ fa broadcast del suo valore, tutti lo ricevono, si runna il CP, visto che hanno tutti lo stesso valore concorderanno su quello. procedo così finché uno ad uno non costruisco il vettore.\n\nCP to IC. ogni process ha un valore, runno IC e raggiungo il consenso su un vettore. a questo punto posso prendere qualsiasi elemento $i$ di questo vettore condiviso e sceglierlo come valore condiviso.\n- problemino, se metto quella condizione (of the non-faulty processes?) nella validity del CP potrebbe succedere che l'elemento i faila, si raggiunge un consenso in cui $v[i]$ ha un valore a caso, e se scelgo proprio questo sto raggiungendo il consenso su un valore che non è stato inizialmente scelto da alcun processo. A lui non piace che che ci sia questa condizione, dice che forse è sbagliata. Massì, togliamola;\n- potrei prendere la maggioranza dei voti, funziona fintantoché ci sono meno di n/2 fails.\n- l'esempio che fa sono due in realtà\n\t- i processi in CP hanno valori 3777. Faccio girare IC ma il primo processo è faulty ed esce fuori un 4. se metto la condizione dei (of the non-faulty processes?) e scelgo 4 ho sbagliato, perché dovevo fare 7\n\t- anche se però la tolgo è un problema: se invece sono 7777 e girando IC il primo è faulty ed esce 4 e scelgo quello non va bene perché dovevo scegliere 7.\n\nQuelle opposte te le devi trovare a casa :)\n\nrecap sul consenso:\n\n- no faults: con sistema sincrono o asincrono basta un round;\n- se ho al più f faults mi servono al più f+1 round con sistema sincrono. se il sistema è asincrono FLP mi taglia le gambe\n- se ho nodi bizantini: dipende. se ce ne sono al più $f\\leq\\ceil*{n-1\\over3}$ riesco nel caso di sistema sincrono in f+1 round (cercati dettagli su sta roba!!). asincrono FLP.\n\nquesto è parte dell'esame. check PPFT e protocolli simili. fa una lezione il 16 gennaio. uffa.","x":1661,"y":2652,"width":1291,"height":1492},
		{"id":"8f33ffedf1004276","type":"text","text":"# Privacy II? Seminari (all'esame ne chiederà una overview)\n\nDue resoconti di suoi lavori\n\n### WiFi Privacy\n\nAnno 2013. Efficienza energetica più importante della privacy. lo smartphone si sveglia ogni minuto e manda un beacon in cerca di AP, in caso negativo torna a dormire. In questo beacon c'era il MAC Address di tutti gli AP con cui mi ero connesso in passato. Non molto safe. Se vado in giro a connettermi a vari AP, sto divulgando i miei movimenti.\n\nal tempo i wifi usavano un SSID univoco (e.g. Fastweb 958e8r98h4h38dqs12), ed esiste un servizio online che geolocalizza questo ID univoco (ma sul serio????? chi glieo dice??? risposta: lo smartphone ha il gps, ma si connette anche a queste reti. fai 2+2. ma basta anche una di quelle google car per street view, che va in giro a raccogliere ID di AP). E quindi niente, intercetto il beacon, leggo il nome, potrei sapere dove abiti.\n\n\n... potevo! a seguito di questa cosa hanno cambiato i protocolli. Ora il messaggio non manda la lista. Perde un po' di performance, ma guadagno in privacy.\n\nstill, c'è ancora il mac address. Apple ha iniziato a mascherarli con un MAC casuale, anche gli altri dopo mi sa.\n\nil sunto di tutto ciò: avendo in mente solo energy efficiency hanno fatto un disastro sulla privacy, e sono serviti anni per rimediare.\n\n\n### Dark Web\n\ngeolocalizzare gli utenti di un sito dagli orari dei post. ovviamente esce una distribuzione probabilistica.\npraticamente hanno provato a stimare la utc zone dagli orari dei post del singolo utente. hanno testato il modello su utenti di cui sapevano la nazionalità (con l'italia), e funziona abbastanza bene (ovviamente esce gaussiano con picco sull'italia), quindi lo hanno usato per studiare la distribuzione complessiva degli utenti dei siti del dark web. un sito dichiaratamente russo usciva piccato sulla russia, ecc..\n\nse ben ricordo puoi anche profilare osservando i cambi di ora legale, con tutta l'attività che si shifta","x":-9360,"y":-466,"width":960,"height":886},
		{"id":"6a2265d74be7e3b8","type":"text","text":"# Bitcoin\n\nIl sistema Bitcoin consta di $O(10^3)$ miner sparsi per il mondo, e il gossiping avviene tra $O(10)$ primi vicini. Dal momento che non sono previsti Smart Contracts, esiste un unico coin: $\\BTC$.\n\nNasce con un primo blocco hard-coded contenente una sola transazione che ha fornito $50\\,\\BTC$ al suo fondatore, dopodiché diversi server si sono proposti come miner ed è iniziato il gioco. Il numero di transazioni per blocco è aumentato con il tempo, e al prima transazione di ogni blocco costituisce il reward per il miner che risolve il problema computazionale, motivo per cui la storia di ogni $\\BTC$ è tracciabile fino alla sua origine.\n\nQuando si esegue una transazione, è consigliato (ma non obbligatorio) pagare una ***fee*** al miner che fa approvare il blocco. Questo perché di fatto è un guadagno per il miner, il quale tenderà a dare priorità di approvazione alle transazioni in mempool con le fee più alte.\n\nIl computational puzzle è \"l'hash in uscita deve avere i $k$ bit meno significativi uguali a $0$\".  la funzione di hash (e.g. ***SHA-256*** per Bitcoin) prende in input le transazioni del blocco che voglio far approvare, l'hash del blocco precedente e (oltre ad altra roba che non stiamo a specificare in quanto non utile alla trattazione) un ***nonce***. In generale non ho idea di cosa esca fuori come hash di output, posso solo cambiare il nonce e guardare il risultato.\n\nLa probabilità di trovare proprio quello che voglio (i.e. i $k$ zeri finali) richiede un numero di lanci quantomeno esponenziale per diventare di ordine $1$, da cui la difficoltà probabilistica del problema (si capisce meglio perché non è classificabile come $\\NPH$). Una volta risolto il problema (i.e. trovato il nonce), è facile per gli altri miner verificare la soluzione (basta far girare SHA-256 con i parametri indicati dal miner risolutore).","x":9910,"y":260,"width":780,"height":663,"color":"6"},
		{"id":"2edf1431ee3dc2e7","type":"text","text":"# HowToBlockchain - An Overview\n\nSe ho un file che rappresenta del cash digitale, potrei in linea di principio darlo a più utenti (i.e. ***double spending***). Per ovviare a questo problema, si costruisce un sistema distribuito di server che in ogni istante posseggono una copia aggiornata del libro mastro (***ledger***) di tutte le transazioni, tipicamente implementato tramite una ***blockchain***. Concordare in ogni istante sull'intero storico di tutte le transazioni significa risolvere un problema di ***randomized consensus*** in un ***asynchronous system***.\n\nOgni ***coin*** è in ogni istante di proprietà di una singola persona (i.e. un singolo ***wallet***). A certificare questa cosa c'era inizialmente una Central Authority (e.g. una banca), dopodiché si è passati al seguente sistema decentralizzato (detto ***UTXO***, ***Unspent Transaction Output***):\n\n- La creazione di un coin avviene come output di una ***transazione*** sulla blockchain. Questa è l'unica entità che salva le informazioni relative a \"chi possiede cosa\", dove il \"chi\" consta di un ***hash*** derivato dalla ***public key*** di un wallet, il cosa è un ***amount***;\n\t- Segue che il wallet di per sé non contiene nulla. Piuttosto, la sua ***private key*** serve a certificare che questo ha **potere di cedere i coin associati alla sua public key** sulla blockchain.\n- Quando viene ceduto, l'operazione prende in input il vecchio hash, la private key del vecchio proprietario e la public key del nuovo (oltre a cose tecniche tipo l'hash del blocco in cui rientra tale operazione, un nonce, ...) generando una transazione che certifica il nuovo proprietario;\n\t- L'amount in output in genere è minore dell'input: la differenza costituisce la ***fee*** per il ***miner*** che fa approvare il proprio blocco con il randomized consensus della blockchain (e.g. PoW, PoS, ...).\n- Quando un server riceve una richiesta di transazione, questa viene inserita nella ***mempool*** (i.e. il set di tutte le transazioni in attesa) e divulgata in ***broadcast*** (o più comunemente facendo ***gossiping***) a tutti gli altri server. I server inseriscono le transazioni della mempool in un ***blocco*** che cercano di far approvare tramite il randomized consensus all'intero sistema distribuito.\n\nQuesto sistema, adottato ad esempio in ***Bitcoin***, consente di tracciare l'intera storia di ogni singolo coin.\n\nEsistono sistemi alternativi, come ***Account-Based*** (e.g. ***Ethereum***). Qui i wallet hanno un saldo effettivo (che sostituisce il concetto di \"coin\" di per sé), ed è questo valore che viene aggiornato nelle transazioni. \n\nPiù in generale, ogni blockchain ha un funzionamento unico. Una grande differenza in questo senso è data dalla presenza o meno degli ***Smart Contracts*** (i.e. la possibilità di eseguire codice direttamente sulla blockchain stessa), introdotti da Ethereum ed oggi largamente utilizzati.","x":6934,"y":1783,"width":860,"height":917,"color":"4"},
		{"id":"23ecd415532232a9","type":"text","text":"# Listone delle Blockchain\n\n- Algorand - fa transactions ogni pochi secondi, fee prederterminate e basse\n- Arbitrum - Gas fee\n- Avalanche - Gas fee\n- ***Bitcoin*** - Fee proporzionali alla dimensione in byte della transazione, posso scegliere fee più alte per ottenere priorità entro la mempool\n\t- Litecoin, Dogecoin\n- Binance Smart Chain (BSC) - Supporta SC, gas fee\n- Cardano - \n- C-Chain - Gas fee\n- Cosmos - \n- ***Ethereum*** - Inizialmente nata PoW (2015) tramite un investimento iniziale di circa 18 milioni di dollari, introduce il concetto di ***Smart Contract***. Per risolvere problemi legati ad efficienza energetica ed eccessivo volume di transazioni crea nel 2020 una Blockchain PoS parallela (Beacon Chain). A seguito dell'evidente successo di quest'ultima, nel 2022 le due chain vengono unificate in un evento cosmico che prende il nome di \"***The Merge***\". Abbandonare il mining riduce i consumi energetici del sistema del 99.5%;\n- Hedera Hashgraph - In realtà non è una Blockchain. È basata invece su una struttura dati a **grafo aciclico diretto (DAG)**, ed usa un algoritmo di consenso simile al PoS;\n- Monero - \n- Optimism\n- Polkadot - \n- Solana - \n- Tezos - \n- Tron - ","x":6934,"y":600,"width":860,"height":819},
		{"id":"68fff16453912ff2","type":"text","text":"# Tracciabilità\n\nSe è vero che ogni $\\BTC$ è unico, cosa succede se ho $3\\BTC$ e ma voglio spenderne solo uno?\n\nIl punto è che i coin non sono veramente dei coin. Il sistema ***UTXO*** (***Unspent Transaction Output***) intende l'output di una transazione come \"somma non ancora spesa\", piuttosto che come \"moneta\" aggiunta al tuo wallet.\n\nQuesto significa che il saldo del tuo wallet altro non è che la somma degli output di una collezione di transazioni sparse in giro per la blockchain di Bitcoin che hanno in comune il fatto di poter essere cedute a qualcuno usando la private key associata al tuo wallet stesso.\n\nIn tutto questo, ogni output di ogni transazione è detto \"UTXO\", e se (tornando all'esempio di prima) vuoi spendere un $\\BTC$ avendo nel wallet $3\\BTC$ ci sono tre scenari principali:\n\n- Il tuo wallet consta di diversi UTXO, tra cui uno da $1\\BTC$. Ho risolto (sto ignorando le fee);\n- Nel tuo wallet c'è una miriade di UTXO di valore infimo. Il sistema ne \"aggrega\" fino a raggiungere la somma che serve e tramite la transazione li distrugge tutti, creando un nuovo UTXO da $1\\BTC$ utilizzabile dal destinatario;\n- Il tuo wallet consta di un solo UTXO da $3\\BTC$. Il sistema distrugge tale UTXO e ne crea due: uno da $1\\BTC$ per il destinatario ed uno da $1.99\\BTC$ per te (... ci stanno le fee). \n\nOra, manteniamoci nel caso facile in cui la gente si scambia sempre $1\\BTC$. È chiaro che in questo caso è immediato risalire la catena di transazioni che distruggono/creano i vari UTXO fino alla transazione che ha generato quel particolare $\\BTC$.\n\nUn po' meno ovvio è se nel frattempo sono avvenute diverse operazioni di aggregazione e divisione. Resta comunque possibile (anche se più difficile) risalire alla storia di ogni pezzo che compone quel particolare UTXO. Diciamo che un conto sono io che sono curioso di sapere la storia dei miei $\\BTC$ ed un altro conto è l'FBI che cerca di ricostruire i giri di soldi di Silkroad.\n\nIl punto è che per quanto possa essere oscurata da pratiche di ***coin mixing*** o frammentazione, la storia di un UTXO è comunque scritta nella blockchain. Esistono SW come ***Chainalysis*** o ***CipherTrace*** in grado di semplificare molto le cose per gli addetti ai lavori.","x":8880,"y":-798,"width":780,"height":823,"color":"4"},
		{"id":"de9132038252d5ae","type":"text","text":"# Cash Digitale e Blockchain\n\nL'idea di realizzare un sistema di ***cash digitale*** affonda le sue radici verso la seconda metà degli anni 80. Se le prime implementazioni si riducevano a tentativi di implementare modi sicuri per eseguire le transazioni, è con DigiCash (1989) che compaiono i primi coin.\n\nLe moderne criptovalute si basano sulle ***blockchain***: ogni transazione viene inviata ad un server del sistema distribuito, il quale la inoltra agli altri. Dall'insieme delle transazioni in attesa (***mempool***) condiviso (idealmente) da tutti i server viene estratto mediante delle logiche variabili un sottoinsieme (che prende il nome di ***blocco***) da aggiungere alla blockchain. Un singolo server guadagna volta per volta il diritto di farsi portavoce di un blocco (e di ottenere di conseguenza un ***reward***) principalmente in due modi (ne esistono $N$, ovviamente):\n\n- Risolvendo un ***computational puzzle***, se la Blockchain stabilisce un meccanismo di consenso di tipo ***Proof-of-Work*** (***PoW***). In questo caso, i server vengono chiamati ***miner***;\n- \"Bloccando\" una certa quantità di criptovaluta, se si applica un meccanismo ***Proof-of-Stake*** (***PoS***). I server possono avere nomi (i.e. ruoli) variabili a seconda del protocollo.\n\nDeve essere semplice per gli altri server verificare che colui che propone il blocco sia legittimato a farlo (i.e. raggiungere il ***consenso*** su quel server). \n\nLa sequenza dei blocchi approvati costituisce il \"libro mastro\" (***ledger***) delle transazioni.\n\nEsistono ledger la cui implementazione non è una Blockchain (e.g. IOTA).","x":9395,"y":1304,"width":780,"height":595,"color":"6"},
		{"id":"d760a9aae20b7b65","type":"text","text":"# Smart Contracts (SC)\n\nQuando si fa una transazione, il problema è sempre lo stesso: io vorrei ricevere il pagamento prima di cedere il bene e l'acquirente vorrebbe essere sicuro di ricevere il bene dopo aver pagato. Questo problema si traduce spesso nella presenza di un intermediario (e.g. un notaio) al quale entrambi i membri dell'accordo delegano la propria fiducia.\n\nGli ***Smart Contracts*** (***SC***) servono ad eliminare l'intermediario e ad automatizzare i contratti, e.g. `Se Andrea versa 100 ETH a Mario, la proprietà del token di Mario passa ad Andrea`. Questa roba è garantita dal fatto che gli SC sono ***codici*** eseguiti direttamente sulla blockchain. Sono ***pubblici***, ***immutabili*** ed ***auto-eseguenti*** al verificarsi delle condizioni.\n\nQuesto rende la blockchain una sorta di *trusted worldwide virtual computer*, in cui posso\n\n- Creare ***asset*** secondari rispetto al ***coin*** principale, detti comunemente ***Token***;\n- Creare dei ***marketplace*** (detti ***Liquidity Pool***) per scambiare gli asset direttamente all'interno della blockchain, senza dover passare per intermediari esterni come Binance.\n\t- Da questi ***Decentralized Exchanges*** (***DEX***) nasce la ***Decentralized Finance*** (***DeFi***).\n\nIl costo computazionale di un'operazione su ***Ethereum*** (e.g. trasferimento di $\\ETH$, creazione di Token) viene quantificato in unità chiamate ***gas***. Esiste quindi un ***prezzo del gas*** legato al valore di $\\ETH$ rispetto al dollaro. Il prodotto tra il prezzo del gas e la quantità di gas richiesta dall'operazione che si vuole effettuare risulta nella ***gas fee*** (i.e. la fee) da pagare alla BC.","x":13480,"y":1304,"width":780,"height":595,"color":"6"},
		{"id":"d968f873f92b2c28","type":"text","text":"# Liquidity Pool (LP)\n\nIn un CEX è chiaro dove devo andare per la compravendita di asset. In un DEX la piattaforma centralizzata viene sostituita dai singoli utenti, che all'interno di una stessa BC possono mettere sul tavolo, ad esempio, $1000\\,\\ETH$ e $1000$ USDT (***Tether***) e proporsi come Exchange.\n\nUna ***Liquidity Pool*** (***LP***) è un particolare tipo di Smart Contract che permette di fare proprio questo. Un utente che aggiunge fondi (i.e. coppie di token in modo proporzionale rispetto al loro valore attuale) alla LP è detto ***Liquidity Provider***. Ciò include ovviamente il creatore, ma anche tutti gli investitori che contribuiscono. Tutti ricevono una piccola percentuale sugli scambi che avvengono all'interno della LP, proporzionalmente alla quantità di ***LP Tokens*** in loro possesso (i.e. fanno le veci delle quote azionarie per la LP). Tutto ciò è regolato da un modello matematico chiamato ***Automated Market Maker*** (***AMM***).\n\nQuando creo la LP con un rapporto $1:1$ (e.g. metto $1000\\,\\ETH$ e $1000$ USDT) e qualcuno compra $100$ USDT, nella LP restano $1100\\,\\ETH$ e $900$ USDT. Questo però significa che il rapporto non è più $1:1$: USDT è diventato più forte di $\\ETH$. Il prossimo utente che vuole comprare $100$ USDT dovrà pagarli $122\\,\\ETH$.\n\nOra, uno potrebbe chiedersi: ma se USDT è l'equivalente del dollaro, come fa ad acquisire valore così rapidamente? Ecco, se qualcuno vuole venderti $100$ USDT a $122\\,\\ETH$ è sicuramente una truffa. Perché? L'USDT autentico è un token ERC-20 associato ad una ed una sola LP originale (gestita da Tether Limited). Siccome però chiunque può creare una LP ed un ERC-20 con un nome qualsiasi (anche duplicato), è facile restare fregati. Poi magari questo fa un bel ***rug pull*** e gli finanziamo la vacanza ai Caraibi.\n\nOvviamente il modo di controllare se lo scambio è uno scam esiste: confrontare l'indirizzo ufficiale di USDT (e.g. su piattaforme come Etherscan) con quello della LP.","x":13480,"y":2194,"width":780,"height":705,"color":"4"},
		{"id":"3bd19f158ca150ea","type":"text","text":"# ERC-721 - NFT","x":14720,"y":1304,"width":780,"height":595},
		{"id":"e9eb9f5f2c7957ad","type":"text","text":"# Pump and Dump\n\nCEX focus, ERC-20 (in alto), pump and dump","x":10690,"y":3360,"width":780,"height":468},
		{"id":"1e7a53303f8b24f6","type":"text","text":"# Marketplaces\n\nCon le blockchain è possibile quindi implementare degli ***asset*** digitali. Come in finanza, anche qui il valore di un asset è proporzionale alla fiducia del mercato in esso. Ma come li scambio?\n\n- ***Centralized Exchange*** (***CEX***) - Funzionano come le tradizionali borse. Quando vuoi comprare o vendere parte un ordine (e.g. \"`Vendo 0.1 BTC a chi è disposto a pagarlo 1000$`\") e aspetti che qualcuno dall'altra parte lo accetti;\n\t- L'insieme degli ordini costituisce l'***order book***;\n\t- I wallet sono di proprietà del CEX (i.e. investire $1\\,\\BTC$ su ***Binance*** equivale a darlo alla piattaforma), il che costituisce un importante point of failure (i.e. se il CEX fallisce o viene hackerato perdi i soldi);\n\t- Spesso sono soggetti a regolamentazioni come ***Know Your Customer*** (***KYC***), i.e. sono costretto a fornire i miei dati reali alla piattaforma per investire;\n\t- Si portano dietro tutti i problemi dell'entità centralizzata (e.g. potere di ban di utenti);\n\t- Ne sono esempi ***Binance***, Coinbase e Kraken.\n- ***Decentralized Exchange*** (***DEX***) - All'interno di una BC che supporta gli SC, ogni utente in possesso di una coppia di token (e.g. $\\ETH$ e ***USDT***, affermatosi come l'equivalente digitale su Ethereum del ***dollaro***) può depositarli in una LP diventando un ***Liquidity Provider***.\n\t- Anonimo, in quanto non c'è alcuna politica KYC;\n\t- Il wallet è tuo, nessuno può bloccare i tuoi soldi o bloccare le transazioni;\n\t- Il rischio di ***scam*** è discretamente alto;\n\t- Ne sono esempi ***Uniswap*** su Ethereum e PancakeSwap su Binance Smart Chain.\n\nNe esistono poi altre tipologie, come ***NFT Marketplace*** (e.g. ***Open Sea***, che opera su Ethereum, Polygon e Solana) e Hybrid Exchange (una sorta di mix tra CEX e DEX).\n","x":11470,"y":2194,"width":780,"height":705,"color":"4"},
		{"id":"1e17cf703d6fb30f","type":"text","text":"# NFT Wash Trading\n\nfai prima NFT (in alto)","x":12250,"y":3360,"width":780,"height":468},
		{"id":"adcea0fee440c20a","type":"text","text":"# P2P - Bit-Torrent\n\n[ho perso i primi 20 minuti] (in cui ha spiegato DNS mi sa)\n\nin DNS, col caching stai scambiando un po' di consistency in favore di un po' di availabiliy. c'è il teorema che dice che non posso avere queste due più un'altra insieme (network partitioning tolerance)\n\nnon c'è alcuna sicurezza su DNS, potrei essere un server DNS malevolo e rispondere con una roba che voglio io, o sfruttare il cambio di IP in un sito per metterci il mio clone malevolo e rubare le password, ...\n\npuoi usarlo per bloccare l'accesso agli utenti (cf gutenberg project, siti di scommesse esterni all'italia perché non pagano le tasse in italia, ...)\n\nin cina mi sa the great damp (diga o come se chiama).\n\ncontent delivery network (systems optimized for delivering stuff to users). MIT (by professors like Tom Layton, algoritmo che si chiama come? CORM?) fa una startup chiamata AKAMAI. L'idea è la seguente: se tutti ci colleghiamo ad uno stesso server per un servizio (e.g. CNN), lo mandiamo in overload. What if siamo in grado di distribuire il contenuto di CNN in giro per il mondo, su diversi server. CNN paga un tot ad AKAMAI e costoro mettono i server in giro per il mondo con la copia di CNN. Assumendo si possa fare è più efficiente, ma consistency è un problema (fino a na certa con la CNN, un po' di più se lo fa una banca). Esempio: sito che vende biglietti aerei. quando ho tanti biglietti da vendere non mi importa molto della consistency, ma quando diventano pochi non voglio rischiare di vendere i biglietti 100 e 101 se ci sono solo 100 posti.\n\nora, come fa AKKAMAI? Quando fai request http a CNN ti arriva un file .html. Questo ha dei link interni a contenuto che sta sul server CNN. quello che fa akkamai è cambiare gli URL di queste risorse da\n\n- `http://www.cnn.com/roba` a `http://www.cnn.com.akkamai.com/roba` (i.e. this html file has been \"akkamized\")\n\na questo punto DNS indirizza la richiesta al server AKKAMAI che è un loro pseudo DNS con una lista di IP dei loro server. Siccome sanno dove sei ti connettono al server più vicino. In pratica l'utente fa sempre la prima richiesta a CNN, dopodiché i contenuti \"pesanti\" vengono gestiti dai server distribuiti AKKAMAI. Ci hanno fatto i soldi con sto sistema. Ora ce ne sono un sacco di queste cose come akkamai.\n\nse CNN cambia qualcosa deve mandarlo a tutti i server AKKAMAI. Potrebbe esserci del delay, ma finché è CNN who cares\n\ncon ragionamenti simili puoi implementare load balancing.\n\nNei primi anni 2000 erano molto in voga le reti P2P: il content è distribuito su N peer server senza central authority (e.g. gruthela? emule, bittorrent).\n\nParliamo di BitTorrent. The big idea is people can share content without a central authority che ti dice cosa puoi fare e cosa no. In genere nessuno pensa agli usi malevoli. Facciamo che in un certo punto della rete c'è la release di un nuovo SO. Un botto di gente lo vuole alla release, quindi sovraccarica quel punto. What if quando A fa il download magari B può andarlo a prendere da A invece che dal nodo iniziale. E magari prende un pezzetto da A, uno da C...\n\nFamo che non c'è proprio l'autorità, così siamo davvero p2p. Tutti i peer hanno una copia dello stesso file. Se io non ce l'ho posso scaricarlo da diversi peer in simultanea, aumentando molto la velocità. Come? Devi cercare un TorrentFile. Cerco ad esempio Ubuntu23.TorrentFile. Dentro questo file ci stanno tutte le info per scaricarlo:\n\n- nome del file e sua size\n- lista dei tracker - server che sanno chi ha cosa, in particolare qui chi ha ubuntu23. in genere sono volontari, ma sono separati dai peer.\n- lista delle partizioni - visto che i file sono grandi, sono organizzati in pezzi con relative size\n- hash del file - controlliamo se il file che abbiamo scaricato è integro o se è stato modificato dai peer. Actually si fa su ogni partizione. Evita la pollution (e.g. sono Bill Gates e non voglio che la gente abbia linux, allora corrompo le mie partizioni), o comunque la mitiga. Nelle nuove implementazioni ci stanno i ***merkel trees***.\n\t- un altro modo di fare pollution è prendere un file `videodigattini` e caricarlo con il nome di `Interstellar4K`. Si distribuisce molto rapidamente e quando la gente lo scarica... sorpresa. In genere non lo fanno proprio con i `videodigattini`.\n\nil protocollo in genere prevede un meccanismo `tit-for-tat` (this-for-that), per evitare gente che fa solo download e zero upload. Va quantomeno scoraggiato. Non serve necessariamente andare in ordine con le partizioni. Ora, se posso iniziare da qualsiasi pezzo, da quale inizio?\n\n- least available (rarest) - favorire la distribuzione dei pezzi più rari permette di non bloccare il sistema. se poi questo pezzo ce l'ha solo un server è una situa da evitare (e se va offline? e se c'è overloading? capita che i file vadano perduti per questo motivo).\n- random - niente overload (load balancing automatico). Pensa se andassimo in ordine. Overload sui primi e quasi niente sugli ultimi.\n\t- se faccio random però matematicamente scopro che 1/3 delle partizioni non le scarica nessuno, almeno all'inizio. poi c'è una roba logn che però non ho capito cos'è\n- meglio ancora un mix delle due strategie: start with rarest then random (this is what BitTorrent does).\n\nOgni server ha un unchoke algorithm per connettersi con altri server. Cioè lo \"sblocco\" della connessione. In genere ce ne sono pochi (tipo 4) e si basano sul `tit-for-tat` (i.e. sblocco chi ha qualcosa da darmi). E se non ho niente da dare? nessuno accetterà la mia connessione? Non proprio, esiste l'optimistic unchoke, i.e. apro una connessione con te anche se non dai nulla in cambio, con l'idea che prima o poi contribuirai anche tu al sistema.\n\nhai mai notato che verso metà file il download è velocissimo e poi arrivato al 99% ci mette una vita? magari c'è un pezzo molto lento, quando arrivo alla fine manca solo lui, che è lentissimo. O magari sto aspettando l'unchoke dall'unico peer che ha quel fragment. ... e anche per questo si usa rarest first.\n\nse mi manca solo un pezzo sono autorizzato a chiedere sub-fragments a diversi server in parallelo (\"because you're deperate\"). Si chiama EndgameMode, e permette anche di distribuire i pezzi più lenti su più peer. \n\nposso accendere la Endgame mode prima del previsto? Sì, ma ci sono controlli per vedere se qualcuno si comporta male (e.g. dovrei convincere il tracker che mi manca solo un pezzo, ma allora lui mi indica come provider per scaricare roba che non ho, la gente lo vede e io sono tipo `ehm....`)\n\nscordati la liveness, in generale.\n\ndopo che faccio download è polite restare un po' online a fare solo upload. si chiama seeding. \n\nmette paper con le prime idee di bit torrent, bastano le idee generali non i dettagli dell'ultima versione.\n\nsta roba è piracy-friendly, nel senso che i contenuti illegali sono difficili da controllare. In genere se vengono smerciati contenuti protetti da copyright e sei un (tracker? peer?) ti arriva una mail da Paramount che ti dice tipo `ti prego smettila`.","x":-4281,"y":-3400,"width":1240,"height":1960},
		{"id":"ae4aea8b012be806","type":"text","text":"# Blockchain\n\nLe blockchain risolvono un problema di ***randomized consensus*** in un ***asynchronous system***.\n\n\n\npublic encryptor???? generi 2 key (ah ok quello che genera le chiavi della critto asimmetrica)\n\ncome rendo un file un digital cash? ha la firma dell'owner. come con gli assegni, c'è un primpo proprietario che poi lo cede a qualcuno con una roba del tipo \"lo do a tizio, questa è la mia firma\". La parte difficile è evitare di duplicare il bitcoin (double spending). all'inizio c'era una central authority che approvava le transazione, \"but we don't like it, we're libertarians\". Allora tocca costruire un DS! deve tenere traccia delle transazioni di ogni singolo bitcoin. Viene realizzato da una moltitudine di server in giro per il mondo (in modo dinamico, servers come and go). Chiaro che tutti i server devono concordare sul consenso: chi è l'owner di un certo bitcoin?\n\n- Posso usare paxos? No, perché non gestisce i byzantine nodes. Né posso usare 2PC, anche perché non è live manco per sbaglio. ","x":6934,"y":3120,"width":860,"height":499},
		{"id":"fdf11a88bd803705","type":"text","text":"# Bitcoin\n\nOgni bitcoin è una chain of transaction. Quando il proprietario dà il suo hash al nuovo, potrebbe darlo a più di uno. Come si garantisce che non lo faccia? Ogni server in giro per il mondo ha\n- una copia dell'intera blockchain\n- e del set delle waiting transactions (mempool)\n\t- nota che possono essere diverse tra diversi server\n\nquando un client ordina ad un server una transaction, il server fa broadcast a tutti gli altri. ci sono $\\sim$ 1000 server, posso mai davvero fare broadcast? No, mando a un tot server vicini (eg 10) e delego a loro, in modo da non intasare la rete (i.e. faccio gossip protocol). esiste una piccola probabilità (ma veramente piccola) che qualcuno non riceva il messaggio.\n\nTutti i server sono in competizione tra loro per far approvare la propria transazione. l'idea di base è che ha il diritto di far approvare la propria transazione il primo che risolve un compito computazionale difficile.\n\nquale? l'hash in uscita dal blocco deve avere i k bit meno significativi uguali a 0, altrimenti il block non è un good block. Ma questo hash è una roba che esce a caso dalla roba che ho nel blocco come faccio a rendere questo hash in questo modo tramite un salt (o un nonce, nel senso un \"fill\" da inserire da qualche parte????)?  Posso solo provarci tante volte. PRovo con nonce=0, poi 1, poi 2, ... Questa roba ha difficoltà $2^k$. solo a questo punto posso fare broadcast facendo claim di aver risolto il problema. tutti gli altri possono verificare la soluzione, perché hanno l'hash del blocco precedente a quello, le varie transazioni e il nonce.\n\ni server si chiamano miners, e quando verificano il risultato aggiungono il blocco alla blockchain. e se ci sono due miners che risolvono insieme? due possibilità:\n\n- uno dei due è stato più veloce. l'altro fallisce e non c'è problema.\n- potrebbero crearsi situazioni di incoerenza in cui alcuni server approvano uno e altri l'altro. cioè, fanno una fork creando due branch. O meglio, lo so io ad alto livello. il singolo miner vede la propria branch. ma questo crea casino, perché metà dei server usa l'hash del primo e metà del secondo. come si riconcilia?\n\t- una delle due catene si estenderà prima dell'altra, mandando broadcast a tutti. a quel punto queli della catena più corta invalidano le transazioni che avevano accettato e si riuniscono alla catena più lunga (***regola della catena più lunga***)\n\t- potrebbero rifare la stessa cosa, ed arrivare a due branch di lunghezza 2. l'idea è che ad un certo punto uno dei due si renda conto dell'errore\n\t- la rule of thumb è che prima di approvare le transazioni si aspetta di impilare altri 6 blocchi (rendi praticamente impossibile sbagliare, ma non 0).\n\t- si aggiunge circa un blocco ogni 10 minuti, quindi prima di essere certi di un pagamento in bitcoin tocca aspettare un'ora\n\nOra, perché un server (miner) dovrebbe mai fare una cosa del genere? whoever is able to solve the problem receive a bitcoin reward, which is the first transaction of the block. all'inizio questo reward valeva 50BTC. Ad ogni 210000 blocchi (quindi ogni few years) questo valore si dimezza. ora siamo a 3.125BTC. quindi nella prima era sono stati prodotti $210000\\times50$ bitcoin. Poi $\\times 25$ ecc... Questa è una serie convergente a $21000000$, che se le regole restano queste sarà il numero massimo di bitcoin nella storia.\n\nOra, se risalgo la blockchain che succede?\n\n- ogni singolo bitcoin è tracciato fino alla nascita, i.e. fino alla prima transazione di un blocco\n- i blocchi originano al blocco originale, che è stato hardcoded (no mining) con una sola transazione che servì a dare 50BTC al creatore della blockchain.\n\nQuando parte la blockchain ci sono solo 50BTC e il creatore, ma per qualche motivo la gente ci ha creduto ed ha iniziato a fare mining. anche il blocco 1 è stato fatto dal founder. all'inizio c'era solo una transazione, poi sono aumentate. inoltre all'inizio ogni transazione richiedeva al client una small fee che si prendeva il miner che faceva approvare il blocco (o anche adesso??)\n\nora, devo in qualche modo tollerare i byzantine nodes.\n\n- potrei mai inserire transazioni false? No, perché per realizzarla mi serve la signature del vecchio owner. \n- se un gruppo di miner ha una buona potenza computazionale, questi possono vincere spesso.\n- se sono un governo o una banca posso DDOSsare il sistema. inizio a mettere blocchi vuoti uno dietro l'altro bloccando la rete (???????????? chiedi a Davide che cavolo ha detto)\n- se ho molta potenza computazionale posso fare un pagamento, ricevere quello che mi viene dato e poi ritirare il pagamento invalidando la vecchia chain con una fork\n\nethereum introduce codice dentro i blocchi (smart contracts), una sorta di trusted worldwide virtual computer. algorand fa transactions ogni pochi secondi. in generale ogni blockchain ha funzionamento unico. ogni blockchain ha diversi token. il token più importante è il coin della blockchain.\n\nprivacy - l'associazione tra le public key (wallet) e l'essere umano non è ovvia.\nma ogni tot qualcuno converte i BTC in soldi veri. le transazioni sono visibili a tutti, e quando converto si scopre che io tizio caio ho incassato i bitcoin ricevuti da una certa public key. ma chi è sta public key? boh! ma poi magari anche quello converte i bitcoin, e si scopre chi c'è dietro alla public key.\n\nbinance?? i wallet sono di binance? (emptygocks???)\n\nin pratica c'è molta poca anonymity. come faccio?\n\n- faccio un botto di wallet\n- money laundries, ci metti i soldi e al costo di una fee ti fanno cheee? è difficile tracciare chi lo fa perché ci sono un botto di flussi ed è difficile risalire a chi ha fatto cosa. \n\nall'inizio la gente non si era resa conto che erano tracciabili (cf. silkroad). \n\ngiven a transaction, come faccio a controllare che sia in un certo blocco j? one-by-one. quindi devo conservare l'intero blocco. ... oppure conservo solo l'hash. ma da quello come faccio a capire sta cosa?\n\n... in generale, non è possibile. Posso però risistemare il problema per renderlo possibile. Ogni T ha un hash. prendo coppie di hash e ne faccio un hash. e poi di nuovo. e poi di nuovo. insomma, costruisco un albero binario completo per costruire la radice, che custodisco nel blocco. mi basta salvare questo. ma non mi basta questo! mi serve ogni fratello di quel coso che non sia nel path lungo la radice. Quindi mi servono logn hash. ***merkle tree*** in cui devo verificare che un pezzetto di informazione sia parte di una big picture. questa roba è usatissima, perché verifico senza salvare il bigger file.","x":9910,"y":-3640,"width":780,"height":2520},
		{"id":"b8b2b46fda424547","type":"text","text":"# What Cash Digitale?\n\nL'entità \"analogica\" più vicina ad una moneta digitale è un assegno: erogo un biglietto con la mia firma che indica che il possessore può restituirmelo per ottenere l'equivalente in dollari. Questo assume fondamentalmente due cose:\n\n- È difficile replicare la mia firma (i.e. è difficile falsificare l'assegno);\n\t- È invece molto facile copiare un file. Se a tale file è associato un \"assegno digitale\" (i.e. un coin) potrei duplicarlo e usarlo più volte. Questo problema prende il nome di ***double spending***, e la sua risoluzione ha stuzzicato la curiosità di diversi esponenti del mondo della crittografia;\n- Chi accetta l'assegno si fida del fatto che a fronte di esso qualcuno mi pagherà.\n\t- Questo è equivalente a dire che qualcuno deve garantire che la transazione avvenga. Di norma questo compito spetta alle banche, che fungono da entità garante centrale. L'obiettivo di un DS, tuttavia, è quello di evitare centralizzazioni. Deve quindi essere il DS stesso a fare da garante, obiettivo raggiungibile tramite il ***consenso*** da parte di tutti i server (***miner***) sull'approvazione delle transazioni presenti nei singoli ***blocchi*** della ***blockchain***, i.e. il \"libro mastro\" (***ledger***) di tutte le transazioni.","x":8880,"y":347,"width":780,"height":489,"color":"4"},
		{"id":"df039763a41c9ec8","type":"text","text":"# Simultaneità e Fork\n\nNonostante sia probabilisticamente raro, cosa succede se due miners risolvono insieme?\n\nEscludiamo il caso in cui uno dei due è più veloce a comunicarlo e a farlo approvare, e diciamo che il server `A` convince alcuni miner ad allungare la blockchain con il proprio blocco e che `B` convince i restanti con il suo. D'altronde \"randomized consensus\" significa proprio questo: \"`è talmente improbabile che qualcuno risolva che quando succede mi convinco che sia un caso isolato e accetto, assumendo implicitamente che tutti quanti accetteranno questo mio stesso blocco`\". Questa situazione rompe esattamente questa assunzione.\n\nQuesto porta in pratica alla creazione di una ***fork*** (i.e. ci sono due diversi ***branch***). O meglio, questa è una cosa di cui mi rendo conto solo io in quanto osservatore esterno: il singolo miner vede solo la propria branch. Chi ha approvato prima il blocco di `A` ignorerà il blocco di `B` (per questi miner il blocco proposto da `B` è semplicemente \"sbagliato\": usando come input quel nonce che propone e l'hash del blocco `A` che ho appena approvato, chiaramente non può uscire l'hash del blocco `B`!) e viceversa. Come si ***riconcilia***?\n\nL'idea è che una delle due catene si estenderà prima dell'altra, i.e. uno dei due gruppi di miner troverà il blocco successivo prima degli altri. Facendo broadcast (gossiping...) a tutti, quelli dell'altro gruppo si renderanno conto dell'incongruenza. Chi ha in memoria la blockchain più corta (e.g. chi ha approvato solo `XXXA`) farà marcia indietro e si adeguerà a chi ha approvato il maggior numero di blocchi (e.g. `XXXBC`). Ciò si chiama ***regola della catena più lunga***.\n\nPotrebbe ovviamente succedere di nuovo che due miner nei due blocchi risolvano insieme e che quindi generino nello stesso sottogruppo due chain `XXXAD` e `XXXBC`, ma l'idea è che a livello probabilistico prima o poi uno dei due si renda conto dell'errore.\n\nLa *rule of thumb* è che prima di approvare in via definitiva le transazioni si aspetta di impilare altri 6 blocchi. Questo riduce quasi del tutto (ma non annulla!) la possibilità di sbagliare (e.g. come caso isolato, nel 2013 a causa di un errore SW c'è stata una fork di 24 blocchi).\n\nConsiderando che si aggiunge circa un blocco ogni circa $10$ minuti, prima di essere certi di un pagamento in Bitcoin bisogna aspettare all'incirca un'ora.","x":9910,"y":-811,"width":780,"height":836,"color":"4"},
		{"id":"290b7892ed1efb0d","type":"text","text":"# Byzantine Nodes ed Attacco del 51%\n\nIn un sistema del genere, è ovviamente un problema se ci stanno dei ***byzantine nodes***.\n\nTuttavia, Bitcoin ha una discreta tolleranza. Ad esempio, non è possibile per un miner inserire transazioni false ed indirizzare a se stessi i coin altrui. Questo perché per realizzare una transazione serve sempre la chiave privata del vecchio proprietario del coin. Non è neanche possibile fare double spending, dato che ogni coin è unico. Né è possibile modificare una transazione approvata, perché romperebbe tutti gli incastri successivi degli hash.\n\n... o meglio, se sono un singolo miner o un piccolo gruppo di miner che collabora posso fare poco. Se però raggruppo una discreta potenza computazionale (***hashrate***) posso iniziare a vincere spesso. A seconda di quanto ne ho possono succedere diversi scenari simpatici.\n\n- Se ne ho tanto (e.g. sono un governo o una banca) posso provare a ***DDOSsare*** il sistema.\n\t- ***Come?*** - Inserendo blocchi vuoti nella BC per aumentare il carico computazionale, *DDOSsando* direttamente i miner.\n\t- ***Perché?*** - Una banca potrebbe avere interesse a destabilizzare le criptovalute.\n\t- ***È un problema?*** - $\\BTC$ è abbastanza resistente, e non sembra sia mai successo.\n- Se ne ho ***più della metà*** posso fare un pagamento, ricevere quello che mi viene dato e poi ritirare il pagamento invalidando la vecchia chain con una fork (***Attacco del*** $51\\%$).\n\t- ***Come?*** - Avere più della metà dell'hashrate significa essere in grado di costruire un branch alternativo più velocemente rispetto al restante $49\\%$ del sistema. Posso così prima o poi (potenzialmente mi servono tanti blocchi) sovrascrivere il branch del resto del sistema (in cui ho speso $100\\BTC$) con il mio (in cui non l'ho fatto).\n\t- ***Perché?*** - Posso fare double spending: mando agli altri una transazione in cui spendo $100\\BTC$ nel blocco `A` che forma la catena `XXXA` per poi sovrascrivere con `XXXBC`. Particolarmente problematico se supero la catena dopo oltre $6$ blocchi (i.e. dopo che il sistema ha già \"ratificato\" la transazione): posso di nuovo spendere i coin che qualcuno aveva già accettato come pagamento (inviandomi magari la cocaina).\n\t- ***È un problema?*** - Difficile ottenere il $51\\%$ dell'hashrate in Bitcoin. Forse è più fattibile con blockchain più piccole.","x":11000,"y":-811,"width":780,"height":836,"color":"4"},
		{"id":"6a9218175a8a7feb","type":"text","text":"# Privacy\n\nBitcoin è un sistema trasparente, i.e. le public key coinvolte nelle transazioni sono sempre in bella mostra sulla blockchain. Tuttavia, l'associazione tra le public key e l'identità della persona fisica dietro quel wallet non è ovvia.\n\nQuindi Bitcoin è anonimo? ... non proprio. Quando uso un ***Exchange*** per convertire i Bitcoin in ***moneta fiat*** (i.e. dollaro, euro, ...) la piattaforma tipicamente richiede una verifica dell'identità nota come ***Know Your Customer*** (***KYC***) nel rispetto delle normative anti-riciclaggio. Questo significa che nel momento in cui converto viene fatta l'associazione tra il nome e il wallet.\n\nQuesto può innescare un processo in cui via via che la gente converte si crea un puzzle in cui diventa sempre più chiaro a chi si estende il giro di soldi, e diventa più facile riempire i vuoti.\n\nIn pratica, Bitcoin non è molto anonimo (piuttosto è ***pseudonimo***, nel senso che finché il gioco regge io sono mascherato dalla mia public key). Ovviamente ci sono tecniche per aumentare il livello di sicurezza di tutto questo.\n\n- Creo tanti wallet per diversificare le attività che svolgo;\n- Al costo di una fee faccio fare le montagne russe ai miei $\\BTC$, dividendoli in milioni di parti e creando flussi di decine di milioni di transazioni (***coin mixing***). In mezzo a questa enorme quantità di flussi è difficile risalire a chi ha fatto cosa. In pratica, è ***Money Laundering***.\n\nQuesto meccanismo di pseudo-anonimato non proprio chiarissimo ha portato nel corso del tempo diverse persone ad essere scoperta con le mani nella marmellata (cfr. ***Silkroad***). \n\nIn generale, se non vuoi farti scoprire mentre traffichi cocaina ci sono blockchain create con in mente la privacy come focus principale (e.g. ***Monero***, ***Zcash***).","x":7840,"y":-798,"width":780,"height":823,"color":"4"},
		{"id":"5c8dfb6c456d2f6b","type":"text","text":"# Ben-Or Pseudocode\n\n```\npreference <- input\nround = 1\nwhile true do\n\tsend(1, round, preference) to everybody\n\t\t// 1 è il tipo di messaggio\n\t\t\n\twait a number n-t of answers like (1, round, *)\n\t\t// Devo stare attento a questo tipo di comandi, perché se questi messaggi\n\t\t// non arrivano non termino (è scritto per definizione nella tolleranza)\n\t\n\tif received > n/2 messages like (1, round, v) then:\n\t\tsend (2, round, v, ratify) to everybody\n\t\t\t// i.e. posso testimoniare di aver visto una maggioranza su v\n\telse:\n\t\tsend (2, round, ?) to everybody\n\n\twait n-t (2, round, *) messages\n\tif received even a single (2, round, v, ratify):\n\t\tpreference <- v\n\t\tif received more than t (2, round, v, ratify) messages:\n\t\t\toutput <- v\n\telse:\n\t\tpreference <- flip a coin (0 or 1)\n\t\tround ++\n```","x":3293,"y":2195,"width":860,"height":703,"color":"4"},
		{"id":"e4ed5177975b21e0","type":"text","text":"# Failure Detectors\n\nSarebbe bello sapere per certo se un certo $p_j$ è morto o no. Possiamo immaginare che ad ogni processo sia associato un modulo non condiviso, detto ***failure detector*** $D_p$ (i.e. associato al processo $p$), che come un oracolo è in grado di sapere questa informazione. Che proprietà richiediamo a sto coso?\n\n- Completeness - if a process fails, $D$ detects it (i.e. no false negatives).\n\t- è completo un $D$ che dice sempre che $p_j$ è morto (though not very useful);\n\t- Assumiamo che i fail siano tutti fail-stop. Data una run $\\s$ e i processi $crashed(\\s,t)$ e $up(\\s,t)$ definiamo $D_q(t',\\s)$ il set dei processi che il $D$ del processo $q$ sospetta essere crashati al tempo $t'$. Distinguiamo\n\t\t- strong completeness - when some guy crashes, at some point ***all the other guys*** realize it has crashed:$$\\forall\\,\\s\\forall\\,p\\in crashed\\,\\,\\forall q\\in up(\\s)\\quad \\exists\\,t\\,:\\,\\forall\\,t'\\geq t'\\,\\,p\\in D_q(t',\\s)$$\n\t\t- weak completeness when some guy crashes, at some point ***some other guy*** realizes it has crashed:$$\\forall\\,\\s\\forall\\,p\\in crashed\\,\\,\\exists q\\in up(\\s)\\quad \\exists\\,t\\,:\\,\\forall\\,t'\\geq t'\\,\\,p\\in D_q(t',\\s)$$\n- Accuracy - if $D$ tells me some $p_j$ died, then $p_j$ really died (no false positives).\n\t- è completo un $D$ che dice sempre che $p_j$ è morto (though not very useful);\n\t- Anche qui facciamo distinzioni, in ordine di strongness (ogni proprietà implica tutte quelle sotto)\n\t\t- strong accuracy - se p e q sono processi vivi, nessuno dei due sospetta mai la morte dell'altro. $$\\forall\\,\\s\\,\\,\\forall\\,t\\,\\,\\forall\\,p,q\\in up(t,\\s)\\,\\,p\\notin D_q(t,\\s)$$\n\t\t\t- eventual strong accuracy - dopo un certo tempo si ha strong accuracy$$\\forall\\s\\,\\,\\exists t : \\forall\\, t'\\geq t\\,\\,\\forall p,q... poi\\, è\\, uguale$$\n\t\t- weak accuracy - esiste un p vivo di cui nessun q sospetta la morte.$$\\forall\\,\\s\\,\\,\\exists p\\in up \\,:\\,\\forall\\,t\\forall q\\in up(t,\\s)\\,\\,p\\notin D_q(t,\\s)$$\n\t\t\t- eventual weak accuracy - come eventual strong.\n\nOra, a seconda di queste definizioni abbiamo una tassonomia di $D$. (guarda risma fogli)\n\nSe qualcuno mi dà un ***perfect failure detector*** posso risolvere leader election? sì! in ogni momento so chi è vivo, quindi se ad esempio la logica è il più basso ID non posso avere due leader. potrei avere 0 leader se il vecchio leader muore e gli altri processi ancora non lo sanno, ma solo fino ad un certo tempo $t'$ (dopodiché la strong completeness mi assicura che tutti lo sanno).\n- Se risolvo la leader election posso rendere live paxos ovviamente, quindi paxos è live se esiste un perfect D. ma esiste nella realtà? No!\n\nin effetti, se il sistema è asincrono nessuna di queste proprietà e garantita come coppia!!\nse però mi danno un weak completeness D posso costruire uno strong completeness D. Come? Esiste almeno un q che sa del fallimento di qualcuno, quindi gli faccio fare broadcast a tutti gli altri:\n\n```\nDp <- 0  (qua intende dead processes mi sa, o forse no, intende proprio failure detector con l'idea che all'inizio è vuoto e quando si aggiorna prendo le info da lui e aggiorno la mia lista sospetti)\nloop forever\n\tsuspects <- Dp\n\tsend(p, suspects(p)) to everybody  (p è il sender)\nwhen I receive the list (q, suspects(q)):\n\tDp <- Dp U suspect(q) - {q}   (q è chiaramente vivo se mi manda le cose)\n```\n\n- schemino per i vari \"X può essere usato per costruire Y\"\n\nE se il sistema è sincrono? posso costruire direttamente anche il perfect mi sa. posso mandare i `ping` o schedulare dei broadcast ogni dT (heartbeat). se non ricevo un battito entro dT sospetto.\nc'è solo un problema: non conosco il dT. allora uso un dt.\n\n- se dt > dT va tutto bene, il D è perfetto\n- se dt < dT la completeness funziona, ma si rompe l'accuracy.\n\t- se però ricevo il messaggio dopo dT > dt potrei rimuovere q dalla lista, riottengo una qualche accuracy? no, perché l'eventual accuracy (sia essa strong or weak) deve essere vera da un certo punto in poi. qua se dT>dt continuo a fare questo aggiungi-rimuovi all'infinito. posso però fare di meglio. se mi arriva il messaggio dopo il mio timeout semplicemente mi rendo conto che devo aumentare il timeout. diciamo che lo raddoppio. dopo un certo numero di \\*2 riesco a superare dT e ad ottenere eventual strong accuracy. quindi posso costruire \n\nun modello reale per gestire i fail è avere master-slave in cui slave ascolta i battiti del master e lo sostituisce se muore.\n\n- è possibile che la replica si attiva anche se il master è vivo, se questa non ha accuracy\n- o che non si attivi quando il master muore\n\nmeglio mettere la replica in un DB lontano dal main (e.g. se esplode il DB si friggono entrambi, viceversa la probabilità che entrambi diventino unreachable è bassa. in genere li si mette in posti che hanno proprio rischi diversi. rip per quell'ingegnere che ha messo main in una twin tower e replica nell'altra (dice true story))\n\nIRL i sistemi sono sincroni, smettono di esserlo per un tot per via di problemi sulla rete, ma poi si fixano e si torna sincroni. nel protocollo però continuo a raddoppiare dt! questo potrebbe portare a dei tempi di reazione lunghissimi (e.g. la replica si attiva dopo troppo tempo dalla effettiva morte del master). Che ci inventiamo?\n\n- riduco dt se ricevo l'heartbeat dopo meno della metà del dt attuale (diciamo che lo dimezzo);\n- uso dt diversi per diverse coppie di processi.","x":4760,"y":-2840,"width":1540,"height":1712},
		{"id":"d1b73c391592333f","type":"text","text":"# Ma in pratica?\n\nUn modello reale che implementa un failure detector è il seguente: ogni processo (***master***, o ***main***) ha un \"backup\" (***slave***, o ***replica***) che ascolta i suoi battiti e lo sostituisce se muore.\n\nSe l'implementazione non è un PFD, è possibile che la replica si attivi anche se il main è vivo, o che non si attivi quando il main muore.\n\nÈ in generale meglio mettere la replica in un DB lontano dal main. Perché? Se esplode il DB si friggono entrambi, viceversa (se li metto lontani) la probabilità che entrambi diventino unreachable è bassa. in genere li si mette in posti che hanno proprio rischi diversi.\n\nrip per quell'ingegnere che ha messo main in una twin tower e replica nell'altra (dice true story)\n\nIRL i sistemi sono sincroni, smettono di esserlo per un tot per via di problemi sulla rete, ma poi si fixano e si torna sincroni. nel protocollo però continuo a raddoppiare `t`! questo potrebbe portare a dei tempi di reazione lunghissimi (e.g. la replica si attiva dopo troppo tempo dalla effettiva morte del master). Che ci inventiamo?\n\n- riduco `t` se ricevo l'heartbeat dopo meno della metà del `t` attuale (diciamo che lo dimezzo);\n- uso `t` diversi per diverse coppie di processi.","x":4575,"y":-682,"width":860,"height":518,"color":"3"},
		{"id":"9e09ec64904ea3d2","type":"text","text":"# Mai fidarsi dei Nodi (Merkle Trees)\n\nLa blockchain è un oggetto pesante (ad oggi $\\sim 1\\text{ Tb}$ di dati). I ***nodi completi*** la tengono tutta in memoria, ma un utente tiene il proprio wallet su smartphone si chiaramente non può. Allora si limita a salvarne una versione compatta, costruita nel seguente modo.\n\nUn blocco è una sequenza di transazioni, ad ognuna delle quali è associato un ***hash***. Prendo allora coppie contigue di hash e ne faccio un unico hash (i.e. ogni hash rappresenta due transazioni). Continuo così fino a costruire un ***albero binario completo*** che prende il nome di ***merkle tree***. La sua radice rappresenta quindi tutte transazioni del blocco, e viene salvata nell'header di quest'ultimo. Un ***light client*** (nodo leggero) conserva solo l'header del blocco.\n\nUn light client potrebbe avere interesse a verificare se un suo pagamento è stato o meno accettato dalla rete, i.e. se si trova in uno dei blocchi. Questo si traduce nel problema \"data una transazione, controlla che questa sia all'interno di un certo blocco\".\n\nOra, ci vorrebbe poco a chiedere ad un nodo completo \"scusa, questa transazione è davvero nella blockchain?\". Non è così facile. Perché? Il punto è che ***il light client in genere non si fida al 100% del nodo completo***, quindi preferisce verificare da sé. Come? Con una ***Merkle Proof***!\n\nInvece di rispondere semplicemente \"sì\", il nodo completo deve dimostrarmelo. Quindi mi manda tutte le informazioni per permettermi di ricostruire la ***Merkle Root*** (... la radice del Merkle Tree salvata nell'header del blocco). Questo però viene fatto in modo intelligente: invece di mandarmi tutte le $O(N)$ transazioni del blocco, mi manda solo gli $O(\\log N)$ hash dei fratelli con i quali devo via via accoppiare il mio hash per arrivare alla radice.\n\nIn pratica, il Merkle Tree è utile ogni volta che voglio ***verificare che un pezzetto di informazione sia parte di una \"big picture\"***, senza però scaricare interi Terabyte di file!","x":11000,"y":248,"width":780,"height":687,"color":"4"},
		{"id":"6ac21c68e718730b","type":"text","text":"# Ethereum Request for Comments - ERC (i.e. Token)\n\nGli Smart Contracts permettono di creare asset secondari scambiabili entro la blockchain stessa, detti ***Token***. Ora, questo significa che chiunque può creare qualsiasi cosa in qualunque formato, ed è proprio per mettere un freno a questa potenziale follia che a qualcuno è venuto in mente di ***standardizzare*** i formati di questi Token.\n\nGli ***standard ERC*** garantiscono che i token siano compatibili con i vari strumenti di DeFi (e.g. wallet, exchange). Ci sono grossomodo due tipologie di Token su Ethereum:\n\n- ***ERC-20*** (***Fungible Tokens***) - Sono token che si comportano come una moneta secondaria rispetto ad $\\ETH$. Sono intercambiabili e divisibili come le valute fiat, e si possono creare particolari token (detti ***stablecoins***) ancorati ad esse (e.g. ***Tether*** (***USDT***) con il ***dollaro***);\n- ***ERC-721*** (***Non-Fungible Tokens***) - Ognuno di questi token è unico e non divisibile. Li si può associare ad un'opera d'arte o ad un oggetto da collezione.\n\nEsistono poi standard come ***ERC-1155***, che unifica i concetti di FT ed NFT per situazioni in cui servono asset diversi (e.g. un gioco ha sia una valuta sia una collezione di oggetti, skin, ...).\n\nTrasferire o creare un ERC su ***Ethereum*** ha un costo che si aggira tra $O(10^4)$ e $O(10^5)$ gas, dipendentemente dallo standard utilizzato.","x":13480,"y":248,"width":780,"height":687,"color":"4"},
		{"id":"590a5841f2242f04","type":"text","text":"# Rug Pull\n\nCreare una mia moneta alternativa in un DEX come Ethereum è relativamente semplice.\n\n- Creo uno Smart Contract che rispetta lo standard ERC-20 che chiamo ***Truffa Coin*** (***TC***);\n- Creo uno Smart Contract che definisce una ***Liquidity Pool*** per scambiarlo con gli $\\ETH$. Imposto quindi un valore iniziale, i.e. metto nella Pool $1000\\, \\ETH$ e $1000$ TC. Questo significa dire \"un mio coin vale esattamente quanto un $\\ETH$\".\n\t- Nota che se qualcun altro crea in modo indipendente un altro token chiamato TC, non potrà semplicemente venderlo alla LP in cambio di ETH. Gli scambi nella LP sono vincolati ai soli token originali.\n\nOra, se qualcuno compra $100$ TC al costo di $100\\,\\ETH$, nella LP restano $900$ TC e $1100\\,\\ETH$. Ma il rapporto è sempre $1:1$, quindi ora $1\\, TC={1100\\over900}\\,\\ETH\\sim 1.22\\,\\ETH$. Il fatto che qualcuno abbia dato ***fiducia*** al mio TC acquistandolo lo ha fatto salire di valore, come atteso. Il prossimo che vuole $100$ TC dovrà darmi $122\\,\\ETH$, e così via. L'idea, ovviamente, è che poi possano rivendermi quegli stessi TC ad un prezzo di mercato maggiore, guadagnando con la speculazione.\n\nPiù fiducia darà la gente al TC più $\\ETH$ mi ritroverò sulla mia LP, finché ad un certo punto... la chiudo e sparisco con tutti i soldi. Diciamo che avevo investito $1000$, la gente aveva comprato un po', quindi incasso $4000$, e al netto di $100\\,\\ETH$ di fees ho guadagnato un sacco di $\\ETH$ in barba a chi mi ha ritenuto affidabile. Questa simpatica azione prende il nome di ***Rug Pull*** (i.e. tu metti i soldi sul tappeto e io te lo sfilo da sotto al naso).","x":13480,"y":3334,"width":780,"height":590,"color":"4"},
		{"id":"a9a6fe3bf7a96bbe","type":"text","text":"# Rug Pull - Uno Studio Comparativo\n\nIl paper analizza le manipolazioni di mercato di tipo Rug Pull confrontando due diverse BC:\n\n- ***Ethereum*** - Muove somme di denaro maggiori. Le gas fee tendono ad essere un po' più alte per via della decentralizzazione: avendo migliaia di ***nodi validatori***, la latenza di rete fa sì che venga aggiunto un nuovo blocco ogni $\\sim 13\\,s$, e il ***sync time*** (i.e. quanto bisogna aspettare perché tutti i nodi della rete concordino su una transazione) può arrivare fino ad una settimana. Creare un token ERC-20 costa quindi $O(100\\$)$, arrivando quasi a $O(1000\\$)$ in periodi di congestione, sfavorendo di fatto gli scam a basso prezzo;\n- ***Binance Smart Chain*** (***BSC***) - Ha un'architettura più snella, in cui vengono scelti solo $21$ nodi validatori alla volta, riducendo l'overhead computazionale complessivo. Ne deriva un minore tempo di blocco rispetto ad Ethereum ($\\sim3\\,s$), ma un maggiore sync time ($\\sim 3$ settimane). Da questo derivano costi di creazione molto minori, tipicamente $O(1\\$)$.\n\nPosto che puoi voler creare ERC-20 per svariati motivi, dai dati dello studio emerge che\n\n- Il $60\\%$ dei token ha una vita media inferiore alle $24$ ore. Chiamiamoli ***1-day tokens***;\n- Di questi, su Ethereum il $30\\%$ ha una ***LP associata***, mentre su BSC saliamo al $95\\%$. Questo è facilmente spiegabile in virtù di quanto detto prima: Ethereum costa di più;\n- L'$1\\%$ degli account crea il $20\\%$ dei token. Chiamiamoli ***Token Spammers***.\n\t- La maggior parte di questi sono 1-day tokens;\n\t- Quello più attivo su BSC ha creato $20\\text{k}$ token in un anno, il che significa che ha speso almeno $20\\text{k }\\$$. Perché abbia senso fare questa cosa, è logico pensare ad un guadagno effettivo maggiore, il che ci porta a dedurre che c'è un giro di soldi ancora maggiore.\n\nDa tutto questo si arriva a concludere che ***circa l'$80\\%$ delle LP contenenti 1-day tokens sono rug pulls***. Il success rate si aggira intorno al $62\\%$ su Ethereum e al $40\\%$ su BSC (forse per un meccanismo di tipo \"mi fido di più se ci sono investimenti di grosse somme, chissà), con un average gain di $2500\\$$ su Ethereum e $33\\$$ su BSC (le fees complessive sono in media $380\\$$ e $9\\$$). ","x":13480,"y":4060,"width":780,"height":880},
		{"id":"a3670eeeea6adde8","type":"text","text":"### Ethereum vs Binance Smart Chain (BSC)\n\nEntrambe le reti supportano Smart Contracts, ma con alcune differenze:\n\n- Su **Ethereum** ogni transazione può costare anche 100$.\n    \n- Su **BSC** il costo è inferiore a 1$.\n    \n- Ethereum ha blocchi ogni ~13 secondi, BSC ogni ~3 secondi.\n    \n- La sincronizzazione completa di Ethereum può richiedere una settimana, BSC anche tre.\n    \n- Ethereum ha circa 300.000 token, BSC ne ha molti di più perché è più economico.\n    \n\nQuindi, creare un token su Ethereum costa molto, il che limita la speculazione casuale. Su BSC, invece, chiunque può farlo con pochi centesimi, il che ha portato a un'esplosione di token effimeri e scam.\n\n---\n\n### Rug Pull: Il Manuale dello Scam\n\nCreare un token senza valore reale e farci sopra soldi è banale:\n\n1. Creo un token.\n    \n2. Creo una LP con un token legittimo (es. ETH o BNB).\n    \n3. Aggiungo un po’ di liquidità per attirare investitori.\n    \n4. Aspetto che qualcuno compri, sfruttando hype e FOMO.\n    \n5. Ritiro tutta la liquidità e sparisco con i soldi.\n    \n6. Gli investitori rimangono con un token senza valore.\n    \n\nQuanto è diffusa la pratica?\n\n- Su **Ethereum**, il 30% dei token ha una LP.\n    \n- Su **BSC**, il 95% (perché è più economico).\n    \n- Il 60% dei token dura meno di 24 ore.\n    \n- L'80% delle LP associate ai token di un giorno sono Rug Pull.\n    \n- Su Ethereum, il 62% dei Rug Pull ha successo, su BSC il 40%.\n    \n\nChi crea questi token? L’1% degli account genera il 20% dei token. Il top creator ha fatto 20.000 token in un anno, spendendo circa 20.000$ solo in fee. Questi sono i **token spammers**, il cui obiettivo è trovare il primo allocco che investe per poi sparire.\n\n---\n\n### Bot e Sniper Bots: La Guerra Automatica\n\nChi compra questi token truffa? Spesso non sono nemmeno umani, ma **Sniper Bots**: programmi automatici che acquistano i token appena vengono lanciati per rivenderli istantaneamente a un prezzo maggiorato.\n\nCome fanno?\n\n1. Monitorano la mempool (lo spazio dove le transazioni aspettano conferma).\n    \n2. Vedono un nuovo token in arrivo.\n    \n3. Lo comprano prima che venga ufficialmente listato.\n    \n4. Appena qualcuno compra, lo rivendono con profitto.\n    \n\nAlcuni sviluppatori creano token apposta per ingannare questi bot: ad esempio, rimuovendo la funzione “sell”, impediscono la rivendita e fregano gli Sniper Bots stessi.\n\n---\n\n### Considerazioni Finali\n\n- Se crei un token e vuoi che sopravviva, **aspetta almeno una settimana** prima di dichiararlo valido.\n    \n- Il mercato delle criptovalute è una giungla senza regolamentazione: ogni transazione ha una fee, quindi truffare non è istantaneo, ma nemmeno impossibile.\n    \n- I token principali di una blockchain sono quelli con cui si pagano le fee (es. ETH su Ethereum, BNB su BSC), mentre i token secondari esistono solo nel mercato interno della blockchain.\n    \n- Le grandi cripto hanno un mercato globale, dove si confrontano tra di loro e con le valute tradizionali.\n    \n\nIn sostanza: chi ha soldi li mette sulle cripto perché non ci sono tasse immediate, ma quando decide di prelevare deve pagare. È un gioco a lungo termine, pieno di rischi e di trappole. Ma soprattutto, **se qualcosa sembra troppo bello per essere vero, probabilmente lo è.**","x":14600,"y":5323,"width":1940,"height":1490},
		{"id":"d841c661688a66bc","type":"text","text":"# Pump and Dump in Cryptocurrencies (Francesco Sassi)\n\n5/12 no classroom, poi due lezioni ed è finita\n\ncon i token, i saldi dei vari utenti sono interni allo smart contract corrispondenti.\n\ncome viene creato uno SC? C'è un codice che esegui il quale ti dà una roba che fai aggiungere alla blockchain. Servono delle funzioni base per avere (ERC20) nome, symbol, decimal?, total supply?, balanceOf?, transfer?.\nLe LP si aspettano che un token sia fatto così per accettarlo (o comunque secondo uno standard diverso da ERC20)\n\nmentre per i coin bob manda una transaction ad alice direttamente sulla blockchain, per i token bob manda la transazione allo smart contract, ed è quest'utlimo che tiene i conti.\n\nCome faccio le transazioni? Due modi:\n\n- CEX (Centralized Exchange) - c'è una organizzazione come ad esempio Binance.\n\t- Custody - Gli asset degli utenti sono stored in un exchange wallet (i.e. li gestisce binance). Se faccio trading con un coin, lo do fisicamente a Binance \n\t- single point of failure. come banca, può andare in bancarotta e tu perdi i soldi\n\t- mi pare di capire che sta cosa funziona se ti fai tu il portafoglio su binance\n\t- the platform knows who you are (proprio con la real identity)\n\t- posso commerciare Coins, Tokens e FIAT\n\t- order book model\n- DEX (Decentralized ...) - Viene gestita da un gruppo di smart contracts. \n\t- Your assets are stored into your own wallet, cioè mi tengo i btc??\n\t- rischio front running: in questo caso la transazione è pubblica nel mempool, un bot può sfruttare il mio acquisto per\n\t\t- fare un acquisto prima di me sulla stessa cosa (mette più fees)\n\t\t- io compro ad un prezzo maggiorato;\n\t\t- lui rivende subito dopo e ci guadagna. parliamo di ordine anche di millisecondi (sta roba si faceva anche prima delle crypto!).\n\t- non è necessario matchare il portafoglio con l'identità reale\n\t- posso commerciare solo token o Wrapped Bitcoin (or ETH), i.e. a token-representation of BTC o ETH. (Peg?) Una compagnia crea tanti WBTC quanti BTC ha e fa da banca con garanzia i coin che ha (anche WFIAT)\n\t- liquidity pools\n\n#### How a CEX works (order book)\n\n- candlesticks - per ogni giorno metto una candela che inizia al prezzo di partenza e finisce al prezzo di arrivo. Verde se ho guadagnato, rosso se ho perso.\n- order book - diviso in sell (rosso) e buy (verde). Ogni colonnina ti dice quanto paga la gente per N bitcoin. L'idea è che quando si toccano sell e buy, quello è il valore di BTC.\n\t- quando alice vuole vendere un BTC e Bob vuole comprarlo, devono concordare sul prezzo. quando concordano (i.e. quando le due cose se toccano) vengono rimoesse dall'order book e si va avanti\n\t- in pratica è un modo di quantificare un \"di quanto si abbassa il prezzo del BTC se ne vendo 10?\"\n\t- posso quindi buy/sell in due modi:\n\t\t- limit orders \"i want to buy/sell 5 BTC  for 10\\$\"\n\t\t- market order \"i want to buy/sell 5 BTC, whatever price\". di norma te lo fa ad un prezzo ragionevole, ma non si sa mai\n\nQuindi come faccio pump and dump?\n\n- compro qualcosa che non vale niente \n- convinco gli altri a comprare, il valore cresce\n- a un certo punto vendo e ci guadagno un botto\n\novviamente è illegale, conflitto di interesse. ... a meno che non lo faccio su un CEX, che non ha regolamentazioni. ci sono pump&dump channels su Telegram, che però non ti dicono subito il target coin (che è una crypto dal basso valore e dal basso market). chi legge questo messaggio prova a comprare il prima possibile (i.e. al più basso valore possibile), perché in pochi secondi il prezzo esplode.\n\ngithub.com/SystemsLab-Sapienza/pump-and-dump-dataset\n\nlaggente diffonde anche fake news per pumpare ancora di più (e.g. `Sapienza Coin comprato dal governo italiano!!`). La fase di dump è poco più lenta (ordine di minuti anziché di secondi).\n\ngli unici a sapere a priori quale coin verrà pumpato (quidi ad accumularlo) sono gli admin, che sono quasi gli unici a guadagnare. Qualche briciola ad un paio di utenti veloci, gli altri perdono soldi.\nci sta tutto un sistema gerarchico (vip channels, affiliation programs) che puoi scalare per ottenere la notizia della cripto target qualche secondo prima.\n\na questo punto la loro idea è stata sviluppare un modello di ML per monitorare il mercato e dire entro pochi secondi se c'è una pump in action. si fa sul numero di ordini come parametro principale. random forest classifier.\n\n- se usi solo thresholds cattivi risultati\n- ottimali con random forest\n- medio-buoni con ada boost\n\nin genere si usa cryptopia invece di binance perché quota crypto sconosciute\n\nma ci si può proteggere da questa cosa? cioè, creare un coin che non risente di questo meccanismo? ...ni. Se faccio lo smart contract qualcosa posso fare, ma con i CEX quelli ci guadagnano un sacco se la gente compra. poi magari pubblicizzano delle policy che non applicano. insomma, il mondo, la gente. in genere chi crea una blockchain in modo genuino e poi gli capita sopra una pump and dump non è contento (fa perdere credibilità dalla BC).","x":8930,"y":4880,"width":1760,"height":1600},
		{"id":"b851c5ce805e4031","type":"text","text":"# NFT Wash Trading\n\n(che diavolo è ERC??????????????????????????)\n\nwash trading = metto in vendita qualcosa e lo compro io stesso per aumentarne il valore (in generale, creare artificial activity, fake interest on something). A e B si scambiano un asset che sulla carta vale 10 ma che loro si scambiano a 100. C vede questa roba e pensa che se lo compra a 30 è un affare.\n\nGli asset possono essere NFT (non-fungible token). MA che è un non-fungible?\n\n- fungible token sono tutti uguali ed ognuno di essi vale uguale, sono intercambiabili\n- gli NFT sono unici, delle robe da collezione.\n\nInsomma, la gente colleziona NFT tramite Smart Contract\n\n- MinT - crea l'NFT\n- Burn - distrugge l'NFT\n- Transfer - obv\n\nposso identificare un NFT con due elementi: address dello smart contract e con il token ID (entro la collezione (???) che intende di preciso per collezione)\n\nin ethereum si usa lo standard ERC-721. \"Ho fatto una roba, è un NFT?\" \"vediamo se rispetta ERC-721\". che prevede?\n\n- transfer event deve specificare starting_account, destination_account, token_ID\n- Interfaccia ERC-165. NFT supporta interfaccia? funzione che, dato un NFT in input, restituisce vero o falso\n\nraccogliere i dati per lo studio significa\n\n- raccogliere tutti gli eventi transfer per ERC-721\n\t- c stanno keark(?) che prende roba, non ho capito\n- filtro per gli smart contract\n- nartra roba che non ho capito\n\nvabbè hanno preso un sacco di dati\n\ncostruisco per ogni NFT_i un grafo G_i i cui vertici sono gli account e gli edge sono le transazioni con associati (timestamp, valore) della transazione. mi puzzano le strong connected components e i nodi con i loop. visto che lo faccio per ogni NFT, il grafo non è troppo grande e riesco a trovare facilmente le SCC\n\nse il ciclo è AXBXA e X è un service account (tipo Binance, Coinbase,...) o smart contracts (liquidity pools) devo ignorarli. Questo perché quando interagisco con una liquidity pool questa mi restituisce un NFT per identificare la mia posizione nella pool (??? succede solo quando voglio ritirare dalla liquidity pool (cosa? il token secondario o primario??)), ma non c'è alcun male in questo.\n\nfatti spiegare la zero risk position, common founder, e common exist\n\n- common founder dovrebbe essere una roba del tipo che un nodo esterno allo scambio compulsivo AB fornisce ad A o a B l'NFT. Poi A e B si scambiano l'NFT con costi $A\\to B = t_1$ e al contrario $t_2>t_1$ (quindi aumentano il valore percepito). Ovviamente anche qui se il founder è una roba importante tipo Binance o simili (come prima) assumiamo che difficilmente sia corrotto e che quindi non ci interessa\n\t- ha messo un vincolo per cui i fondi che arrivano sono minori delle transazioni dell'NFT, non ho capito perché\n\t- AH NO, t è il timestamp!!!\n- common exit è uguale al contrario: A e B mandano a C quello che hanno guadagnato\n\nin pratica ci sono account che ricevono soldi da Sender, fanno fake traffic e poi danno i soldi a Receiver. \n\nsi è scordato di dire che ci stanno degli NFT Marketplaces, e che uno di questi è coinvolto molto più degli altri in questi giri di riciclaggio (LooksRare ha oltre 80%, contro altri tipo OpenSea che hanno 0.5 o meno). Also, LooksRare tende a muovere molti più soldi per transazione.\n\nqueste attività ci mettono giorni (20%) o 10 giorni (60%) $\\so$ la maggioranza dei washing dura poco. esisono cicli di un solo account (self loop) perché nella BC puoi venderti le cose da solo (ofc paghi le fees)\n\n\ncapisci token reward system dei NFTMs (marketplaces) - dovrebbe essere una sorta di fedeltà: se traffichi un volume di 100 su un traffico totale del market di 1000 ottieni 10 per il numero di soldi dedicati a dividere i reward (è praticamente un dividendo)\n\nparliamo di un gioco che costa da migliaia a milioni di dollari\n\nin pratica gli account di fanno i milioni su LooksRare. O meglio: facevano. Questo perché gli alti reward erano per lanciare la piattaforma, ormai non vale la pena.\n\nRiassumendo: il gioco che vale la candela è sfruttare i marketplace che offrono reward alti, e servono volumi di traffico enormi (flashloans sono prestiti che devi ripagare entro lo stesso blocco in cui ti fanno il prestito, la fee è close to 0, se non ripaghi la transazione è semplicemente rifiutata)\n\n\"chi di voi usa i check per scambiare crypto?\" Centralized Exchanges gli devi dare i tuoi dati (quindi scambi NFT di per sé sono totalmente anonimi?? mezzo, fanno una specie di TOR, poi ci sta tornado cash (cerca, se sei un nodo in US e approvi una roba del genere sei perseguibile legalmente)) coin join?\n\nci sono blockchain che sono più privacy focused, tipo monero","x":12420,"y":5000,"width":1660,"height":1600},
		{"id":"d804d42ee844e62b","type":"text","text":"# Gas Fee\n\nSe in Bitcoin e BC simili il costo della fee è semplicemente proporzionale alla lunghezza in byte della transazione, ***Ethereum*** e BC simili usano un meccanismo per le ***fee*** che dipende dalla ***potenza computazionale*** che verrà utilizzata e dal livello di ***congestione*** della rete.\n\nLa fee è quindi divisa in due parti:\n\n- ***Gas limit*** - Quante unità di gas servono per eseguire la transazione. Dipende dalla complessità computazionale dello SC, solitamente si aggira tra i $20\\text{k}$ e i $300\\text{k}$;\n- ***Gas price*** - Il prezzo della singola unità di gas, tipicamente espresso in Gwei. Esiste un prezzo minimo, ma può essere scelto dall'utente per alzare la fee (i.e. acquisire priorità).\n\nLa ***gas fee*** è quindi data dal prodotto di queste due grandezze. È poi possibile aggiungere una ulteriore ed esplicita ***priority fee*** (anche detta ***tip***) per acquisire ancora più priorità.\n\nLa ***base fee*** (i.e. il gas price minimo sotto al quale l'utente non può scendere) è determinata in automatico dal protocollo in base alla congestione della rete.\n\nIn sostanza, l'idea è \"comprare\" la potenza computazionale dei nodi che eseguono lo SC.","x":12240,"y":248,"width":780,"height":687,"color":"4"},
		{"id":"401ce8238044133c","type":"text","text":"# Sottomultipli\n\nTutte le BC hanno un livello di granularità oltre il quale il coin non è ulteriormente divisibile (... in prima approssimazione, vbb).\n\nPer ***Bitcoin*** è il ***satoshi*** ($10^{-8}\\,\\BTC$), in Ethereum è il ***wei*** ($10^{-18}\\,\\ETH$), spesso usato come ***Gwei*** ($10^9\\text{wei}=10^{-9}\\,\\ETH$, il wei è troppo piccolo).","x":12330,"y":-75,"width":600,"height":200,"color":"4"},
		{"id":"9e3a519a38dbadbf","type":"text","text":"\n\"Cos'è emptygocks? O qualcosa di simile, credo sia correlato alle blockchain\"\n\n\"Probabilmente ti riferisci a **Mt. Gox**, uno dei primi e più noti exchange di Bitcoin. Fondato nel 2010 e con sede a Tokyo, Mt. Gox gestiva oltre il 70% delle transazioni globali di Bitcoin al suo apice. Nel 2014, l'exchange ha dichiarato bancarotta dopo aver perso centinaia di migliaia di Bitcoin, presumibilmente a causa di un hack. Questo evento ha avuto un impatto significativo sulla fiducia nel mercato delle criptovalute.\"","x":11470,"y":1800,"width":780,"height":198,"color":"#b48383"},
		{"id":"42fff89cab6aa1be","type":"text","text":"# Seminari II (di un phd?) Federico cernera - Liquidity Pool\n\nmarket manipulations on ethereum and binance smart chain\n\nblockchain = distributed data structure at high level. ogni blocco consta di transazioni Tx {sender, receiver, value, Tx Hash, ...}. Token = smart contract (SC, sort of program that run on blockchain)\n\n\nDEX (decentralize Exchanges) - senza autorità centrale, lo SC ne fa le veci. DEXes allow anyone to create a market on a token. tipo, ti fai un account su binance e trovi i well-established tokens. e se voglio creare un sapienza coin? lo faccio e creo un mercato con una Liquiditiy pool.\n\n- creo un token e una liquidity pool, ovvero creo uno SC sulla BC\n- questa roba funziona tipo così: metto a confronto il token B (e.g. Ethereum) e A (e.g. Sapienza Coin) e dico che un ethereum vale 1000 sapienza coin.\n\t- non proprio. all'inizio metto 1000 e 1000 e vado a bilancio. se qualcuno compra 100 sapienza pagando 100 ethereum, nella LP ci stanno 900 SC e 1100 ethereum. ma valgono uguale! quindi SC è più forte di eth\n\t- in pratica è uno strumento finanziario. cioè, compro un ethereum e poi metto sul mercato 1000 SC dicendo che valgono un eth? e poi chi compra mi dà eth e se disinvesto daje ho guadagnato? Boh\n\nevm machines binance smart chain??? hanno gli stessi smart contract? per fare transazione su ethereum servono O(100\\$) , per BSC O(1\\$) se ho capito bene.\n\neth e BSC sono veloci: si aggiunge un blocco ogni (13, 3) secondi rispettivamente, ma il sync è 1 settimana per eth e 3 sett per l'altro. dopodiché vengono generati gli smart contracts. not all SC are about LP and tokens ??????\n\nMa nei periodi di alta congestione il prezzo del gas\nun token su ethereum costa tanto, quindi la gente ne ha fatto pochi (300K), mentre su BSC \n\n\ni token principali della BC sono quelli in cui devi pagare le fees nella blockchain. per ethereum è ETH. ma poi posso inventarmi il mio coin secondario (che è uno SC) all'interno della BC e fare il giochino della LP (... che è uno SC), cioè faccio un investimento mettendo sul mercato dei soldi in ETH, i miei Sapienza Coin e mettendoli a confronto dico che il mio sapienza coin vale un ETH. Se qualcuno compra, io sono tipo la banca e vado nella situazione 900-1100, il che aumenta il valore del Sapienza Coin. Il secondo che vuole 100 Sapienza Coin deve pagare 110 Eth. [[no, 122]]\n\nmentre questi token secondari hanno senso solo nel mercato interno della blockchain, il token principale è quotato in una sorta di borsa cripto che confronta i token principali delle varie BC tra loro (ed eventualmente con i soldi centralizzati?)\n\nin ogni istante il creator può chiudere la LP e prendersi tutti i soldi (quelli accettati, in eth). e gli investitori! se lo prendono là. non c'è legge, hai scelto di fidarti di sto tipo che ha creato il doggo coin, colpa tua. conta però che qualsiasi tua azione sulla BC ha una fee, quindi non è così immediato fregare soldi alla gente\n\nora, quanto dura un token? 60\\% dei token vivono meno di 24 ore. perché? perché probabilmente vogliono solo speculare. quanti di questi coin hanno una LP associata? (30\\% su Eth, 95 su BSC, perché eth costa di più).\n\nchi crea token? l'1\\% degli account crea oltre il 20\\% dei token. il top 1 ha creato 20.000 token in un anno, il che significa solo per questo spendere 20k \\$. most of them are day-1 tokens.  questi sono i token spammers. l'idea è aspettare un allocco che investe per poi sparire (rug pul). è una manipolazione finanziaria senza regolamentazione.\n\nposso creare token con qualsiasi nome e simbolo, anche duplicato (l'unica cosa unica è l'address, ci sono siti che dato il nome ti danno il giusto address con il giusto valore)\n\n##### How to Rug Pull\n\n- creo il token;\n- creo la LP tra il mio token e un token con un vero valore;\n- aggiungo liquidity, ci metto un pò di soldi in modo che per comprare il token la gente deve usare un vero coin\n- aspetto l'allocco che compra (di base è zero-effort sfruttando la fomo, al più fai advertising)\n- disinvesto e guadagno quello che ha investito l'allocco meno le fees.\n- chi ha comprato ha buttato i soldi\n\nperché mai dovrei comprare una cosa del genere? gambling. va forte su telegram, discord, twitter.22k su eth e 270k su BSC sono rug pull che durano meno di 24h, ovvero l'80% delle LP con 1day token sono rug pull. MA quante di queste hanno successo? Eth 62\\%, BSC 40\\% (però costa meno). avg gain 33\\$ and 2500\\$ per ovvi motivi, mentre il fee cost è 9\\$ e 380\\$.\n\nchi compra i token? spesso e veolentieri i bot, detti Sniper Bots. lo vogliono fare appena uscito, così compro al minimo valore di mercato. come? ci sono dei modi per vedere la creazione di un token prima della sua effettiva creazione. ovvio, devo prima metterlo come transazione in un blocco e aspettare che venga approvato. leggo il blocco, vedo che un nuovo token verrà approvato a breve, appena succede lo compro. appena il secondo acquirente lo compra, io rivendo ad un prezzo maggiorato. (they scan the mempool). ovviamente gli sniper bots si pagano, perché garantiscono quasi sempre guadagni. ci sono quelli free che fanno schifo, là perdi soldi. \n\ndice mei che ha senso anche creare un token e venderlo agli sniper bots (?). il gruppo di ricerca mei sospetta che anche chi crea le LP sono bot, e che quinidi è una guerra bot vs bot.\n\n\nconclusione: se crei un token aspetta almeno una settimana.\n\nvedi quello che ha rimosso la funzione sell in modo che nessuno poteva rivendergli i coin falsi. ora infatti i bot comprano il minimo possibile, lo rivendono all'istante, se riescono a rivenderlo allora comprano sul serio\npump and dump\n\n\nse ho tanti soldi li metto sulle cripto perché non ci sono tasse (poi però ci sono quando li riprendo... it's a long-term game)","x":17280,"y":3511,"width":1200,"height":1409},
		{"id":"97480c66250b7fcc","type":"text","text":"# ERC-20 - Fungible Tokens (i.e. Coin Secondari)\n\nC'è un codice che esegui il quale ti dà una roba che fai aggiungere alla blockchain. Servono delle funzioni base per avere (ERC20) nome, symbol, decimal?, total supply?, balanceOf?, transfer?.\n\nGas Fee\n\nCi sono diversi motivi per cui potrei voler creare un ERC-20, ad esempio:\n\n- Implementare un sistema di pagamento interno ad un'***applicazione***;\n- Usarlo per gestire la ***governance***, un po' come avviene con le azioni in borsa: più coin di governance di un determinato tipo possiedo, più ho potere decisionale in quell'ambito;\n\t- Più in generale si parla di ***rappresentanza di asset***. Questo tipo di token può rappresentare il grado di possesso di un'immobile, di una società o anche di un NFT!\n- Implementare uno ***stablecoin***, i.e. un coin il cui valore è legato ad una ***moneta fiat***;\n- Per puro ***fine speculativo***, i.e. spero che la gente per qualche motivo riponga fiducia nel mio token facendolo crescere di valore...\n\t- ... per poi eventualmente disinvestire tutto e scappare con i soldi alle Maldive. Questo chiaramente sarebbe illegale in una finanza controllata, ma (guess what) la DeFi non è affatto regolamentata. Vedi ***Rug Pull***.","x":13480,"y":-811,"width":780,"height":836,"color":"3"},
		{"id":"b12c0b1740440efa","x":14440,"y":4060,"width":780,"height":620,"type":"text","text":"# How to Rug Pull\n\n- Creo il token e la LP associata, usando come coin comparativo una roba forte (e.g. $\\ETH$);\n- Aspetto l'allocco che compra. Questa fase in linea di principio è ***zero-effort***: di norma, è facile che arrivi qualcuno a comprare, un po' perché la gente c'ha la ***FOMO*** (\"e se questo coso diventa una roba che vale tantissimo?! Che fai, non compri?!\". Sì, davvero), un po' perché in ogni caso ci stanno gli ***sniper bot***;\n\t- Al più puoi fare ***advertising***, i.e. pubblicizzare questo nuovo incredibile token su Telegram, Discord o X (notoriamente dei bei posti). E la gente? Eh, la gente piazza scommesse sulle corse clandestine di cani, vuoi che non faccia un po' di sano ***gambling*** in criptovalute? Qualcuno magari pensa anche di riuscire a comprare subito ad un prezzo basso per poi rivendere quando l'hype generale avrà fatto salire il prezzo. Probabilmente non sanno che uno sniper bot fa esattamente questo, ma ad una velocità di svariati ordini di grandezza superiore.\n- Quando il valore è cresciuto e mi ritrovo con abbastanza $\\ETH$ nella Poll, disinvesto. Il guadagno sarà dato dagli investimenti dei vari allocchi meno le fees per fare tutte queste transazioni (che su Ethereum sono comunque un po' di soldi, motivo per cui non è proprio immediato fregare soldi al sistema).\n- In ogni caso, chi ha comprato ha buttato i soldi\n"},
		{"id":"a1ceed57020b85fd","type":"text","text":"\nin ogni istante il creator può chiudere la LP e prendersi tutti i soldi (quelli accettati, in eth). e gli investitori! se lo prendono là. non c'è legge, hai scelto di fidarti di sto tipo che ha creato il doggo coin, colpa tua. conta però che qualsiasi tua azione sulla BC ha una fee, quindi non è così immediato fregare soldi alla gente\n\nora, quanto dura un token? 60\\% dei token vivono meno di 24 ore. perché? perché probabilmente vogliono solo speculare. quanti di questi coin hanno una LP associata? (30\\% su Eth, 95 su BSC, perché eth costa di più).\n\nchi crea token? l'1\\% degli account crea oltre il 20\\% dei token. il top 1 ha creato 20.000 token in un anno, il che significa solo per questo spendere 20k \\$. most of them are day-1 tokens.  questi sono i token spammers. l'idea è aspettare un allocco che investe per poi sparire (rug pul). è una manipolazione finanziaria senza regolamentazione.\n\nposso creare token con qualsiasi nome e simbolo, anche duplicato (l'unica cosa unica è l'address, ci sono siti che dato il nome ti danno il giusto address con il giusto valore)\n\n##### How to Rug Pull\n\n- creo il token;\n- creo la LP tra il mio token e un token con un vero valore;\n- aggiungo liquidity, ci metto un pò di soldi in modo che per comprare il token la gente deve usare un vero coin\n- aspetto l'allocco che compra (di base è zero-effort sfruttando la fomo, al più fai advertising)\n- disinvesto e guadagno quello che ha investito l'allocco meno le fees.\n- chi ha comprato ha buttato i soldi\n\nperché mai dovrei comprare una cosa del genere? gambling. va forte su telegram, discord, twitter.22k su eth e 270k su BSC sono rug pull che durano meno di 24h, ovvero l'80% delle LP con 1day token sono rug pull. MA quante di queste hanno successo? Eth 62\\%, BSC 40\\% (però costa meno). avg gain 33\\$ and 2500\\$ per ovvi motivi, mentre il fee cost è 9\\$ e 380\\$.\n\nchi compra i token? spesso e veolentieri i bot, detti Sniper Bots. lo vogliono fare appena uscito, così compro al minimo valore di mercato. come? ci sono dei modi per vedere la creazione di un token prima della sua effettiva creazione. ovvio, devo prima metterlo come transazione in un blocco e aspettare che venga approvato. leggo il blocco, vedo che un nuovo token verrà approvato a breve, appena succede lo compro. appena il secondo acquirente lo compra, io rivendo ad un prezzo maggiorato. (they scan the mempool). ovviamente gli sniper bots si pagano, perché garantiscono quasi sempre guadagni. ci sono quelli free che fanno schifo, là perdi soldi. \n\ndice mei che ha senso anche creare un token e venderlo agli sniper bots (?). il gruppo di ricerca mei sospetta che anche chi crea le LP sono bot, e che quinidi è una guerra bot vs bot.\n\n\nconclusione: se crei un token aspetta almeno una settimana.\n\nvedi quello che ha rimosso la funzione sell in modo che nessuno poteva rivendergli i coin falsi. ora infatti i bot comprano il minimo possibile, lo rivendono all'istante, se riescono a rivenderlo allora comprano sul serio\npump and dump\n\n\nse ho tanti soldi li metto sulle cripto perché non ci sono tasse (poi però ci sono quando li riprendo... it's a long-term game)","x":12320,"y":3865,"width":1040,"height":1075}
	],
	"edges":[
		{"id":"313f73befec0f227","fromNode":"be570a1920f04449","fromSide":"right","toNode":"23a2f6367ee648bd","toSide":"left"},
		{"id":"ca3b4cbd15b08962","fromNode":"f82b77ddb75926d4","fromSide":"top","toNode":"0ebec2401a36c3ef","toSide":"bottom"},
		{"id":"9a87cb3a68c4a99b","fromNode":"f82b77ddb75926d4","fromSide":"bottom","toNode":"48ebe9989920f9da","toSide":"top"},
		{"id":"867ed467e77ccc6f","fromNode":"f9d9980fa75657d5","fromSide":"top","toNode":"3458d02a2952e9c7","toSide":"bottom"},
		{"id":"4fdd279d6063830f","fromNode":"ffd7c9469255f9c8","fromSide":"bottom","toNode":"00258a5944b2d60e","toSide":"top","label":"Cooperative Protocol\nfor\nFail-Stop Processes"},
		{"id":"c8cb8a307762db2a","fromNode":"4fd496a7b455c76c","fromSide":"top","toNode":"e0ba5ac0341a437d","toSide":"bottom"},
		{"id":"47e11942324f4e4b","fromNode":"3458d02a2952e9c7","fromSide":"right","toNode":"db4411d94cf78039","toSide":"left"},
		{"id":"8176ca8baa3df1d8","fromNode":"db4411d94cf78039","fromSide":"right","toNode":"61752071951b1015","toSide":"left"},
		{"id":"021050bd6bda60c5","fromNode":"db4411d94cf78039","fromSide":"top","toNode":"6dcdd47739ab639e","toSide":"bottom"},
		{"id":"1ebd8a2d6587351a","fromNode":"61752071951b1015","fromSide":"top","toNode":"0b9a16cefb33b26c","toSide":"bottom"},
		{"id":"7f44ba9628882881","fromNode":"61752071951b1015","fromSide":"right","toNode":"0aa59bf09c992c32","toSide":"top"},
		{"id":"0638f572c4714020","fromNode":"0aa59bf09c992c32","fromSide":"bottom","toNode":"9db64449abbcd63a","toSide":"top"},
		{"id":"289181e565bb286c","fromNode":"ffd7c9469255f9c8","fromSide":"left","toNode":"c2b4396b3b84e4de","toSide":"right"},
		{"id":"32f109e47c0d8cff","fromNode":"ffd7c9469255f9c8","fromSide":"bottom","toNode":"93212aed82c21df9","toSide":"top","label":"Cooperative Protocol\nfor\nFail-Stop Processes"},
		{"id":"4fdde0886f2be644","fromNode":"ffd7c9469255f9c8","fromSide":"bottom","toNode":"d582dc2a1cfd5670","toSide":"top","label":"Recovery Protocol\nfor\nFail-Recovery Processes"},
		{"id":"dae91a337ba0d6a9","fromNode":"93212aed82c21df9","fromSide":"left","toNode":"d582dc2a1cfd5670","toSide":"right"},
		{"id":"7dcbbb6e76747b67","fromNode":"93212aed82c21df9","fromSide":"right","toNode":"00258a5944b2d60e","toSide":"left"},
		{"id":"649439e90b638f90","fromNode":"00258a5944b2d60e","fromSide":"bottom","toNode":"98ff8ea31a5611b2","toSide":"top"},
		{"id":"64a03807a065f7f2","fromNode":"98ff8ea31a5611b2","fromSide":"left","toNode":"e69724a397d8eb9b","toSide":"right"},
		{"id":"7a26fd3d9ee0832e","fromNode":"98ff8ea31a5611b2","fromSide":"right","toNode":"df6f8d2bb2d685d0","toSide":"left"},
		{"id":"7c33e5114002150d","fromNode":"98ff8ea31a5611b2","fromSide":"bottom","toNode":"7bd4339e9dc5832c","toSide":"top"},
		{"id":"adbb5bc4ec67a3c4","fromNode":"7bd4339e9dc5832c","fromSide":"left","toNode":"ee7f51e5be213a53","toSide":"right"},
		{"id":"ebaecf82de4e7c0a","fromNode":"db4411d94cf78039","fromSide":"top","toNode":"18e22186b729910c","toSide":"bottom"},
		{"id":"939cc18ee4f9b7a0","fromNode":"4043ace7e5011791","fromSide":"right","toNode":"8d15b0299f305f56","toSide":"left"},
		{"id":"a7ede2cc8bbf9706","fromNode":"48ebe9989920f9da","fromSide":"left","toNode":"ffd7c9469255f9c8","toSide":"right"},
		{"id":"ea39b3505731eb47","fromNode":"48ebe9989920f9da","fromSide":"right","toNode":"f0f698cee3286316","toSide":"left"},
		{"id":"747cfde93541c774","fromNode":"8876ea52fed47dda","fromSide":"left","toNode":"570439671731e83f","toSide":"right"},
		{"id":"051684e10cf8eabf","fromNode":"df6f8d2bb2d685d0","fromSide":"top","toNode":"c604e7d9b7779409","toSide":"bottom"},
		{"id":"7adc25763994d5ba","fromNode":"df6f8d2bb2d685d0","fromSide":"bottom","toNode":"4043ace7e5011791","toSide":"top"},
		{"id":"6913e91fdaba2996","fromNode":"98ff8ea31a5611b2","fromSide":"left","toNode":"c6a15cbad3909019","toSide":"right"},
		{"id":"0e67dca1e55f4b4d","fromNode":"df6f8d2bb2d685d0","fromSide":"right","toNode":"6a09d757776030a8","toSide":"left"},
		{"id":"fee2acc773de912c","fromNode":"c604e7d9b7779409","fromSide":"right","toNode":"6a09d757776030a8","toSide":"left"},
		{"id":"76c2e8e408e77475","fromNode":"48ebe9989920f9da","fromSide":"bottom","toNode":"6a09d757776030a8","toSide":"top"},
		{"id":"4b4428cddd937684","fromNode":"1a4ae7dd4ea788ae","fromSide":"bottom","toNode":"5aaddbdcdf2d4e68","toSide":"top"},
		{"id":"f9f5e6f2f0e0e753","fromNode":"4043ace7e5011791","fromSide":"bottom","toNode":"1a4ae7dd4ea788ae","toSide":"top"},
		{"id":"a5f4a9266a1fd4a3","fromNode":"e69724a397d8eb9b","fromSide":"left","toNode":"d63911fe834dbe22","toSide":"right"},
		{"id":"d38e8054fe033cae","fromNode":"f0f698cee3286316","fromSide":"right","toNode":"59ebe3510cce29a1","toSide":"left","label":"Approssimazioni per risolvere il Consenso"},
		{"id":"5f19fa8cc936087d","fromNode":"59ebe3510cce29a1","fromSide":"bottom","toNode":"41f7f87c3fbd9530","toSide":"top","label":"Non-Determinism"},
		{"id":"ff22c76ec9407ce2","fromNode":"0ebec2401a36c3ef","fromSide":"top","toNode":"f9d9980fa75657d5","toSide":"bottom"},
		{"id":"d0272266083b0bf2","fromNode":"96363645d77cc1f4","fromSide":"right","toNode":"0aa59bf09c992c32","toSide":"left"},
		{"id":"1fc47a9dd4bbd1c9","fromNode":"0ebec2401a36c3ef","fromSide":"top","toNode":"96363645d77cc1f4","toSide":"bottom"},
		{"id":"d2b00c0ec97e0f58","fromNode":"fbc1ef28f3589012","fromSide":"left","toNode":"f9d9980fa75657d5","toSide":"right"},
		{"id":"40e22260f52fa8c5","fromNode":"fbc1ef28f3589012","fromSide":"right","toNode":"96363645d77cc1f4","toSide":"left"},
		{"id":"95e7260ddb880781","fromNode":"0ebec2401a36c3ef","fromSide":"top","toNode":"4fd496a7b455c76c","toSide":"bottom"},
		{"id":"72b926d2089ac6a7","fromNode":"e0ba5ac0341a437d","fromSide":"top","toNode":"fbc1ef28f3589012","toSide":"bottom"},
		{"id":"48199d93340adc2d","fromNode":"fbc1ef28f3589012","fromSide":"top","toNode":"ab73850207f6a30a","toSide":"bottom"},
		{"id":"bd0160ae5a7cac8e","fromNode":"0ebec2401a36c3ef","fromSide":"right","toNode":"f10faebf80b04e16","toSide":"top","label":"?"},
		{"id":"4e9dae26103db5cb","fromNode":"f82b77ddb75926d4","fromSide":"right","toNode":"f10faebf80b04e16","toSide":"left"},
		{"id":"9f75d962fea97867","fromNode":"f10faebf80b04e16","fromSide":"right","toNode":"3611e372ae09c45c","toSide":"left"},
		{"id":"891e2d3482cc0679","fromNode":"f10faebf80b04e16","fromSide":"bottom","toNode":"f0f698cee3286316","toSide":"top","label":"FLP assume che non esista un PFD "},
		{"id":"1a7afc7b3106c505","fromNode":"f82b77ddb75926d4","fromSide":"left","toNode":"7e4b0290f28c62f1","toSide":"right"},
		{"id":"af386fa5153488bf","fromNode":"f10faebf80b04e16","fromSide":"bottom","toNode":"48ebe9989920f9da","toSide":"right","label":"L'assenza di un PFD complica il consenso"},
		{"id":"fbb2117f97be0360","fromNode":"59ebe3510cce29a1","fromSide":"right","toNode":"de9132038252d5ae","toSide":"left","label":"Blockchain"},
		{"id":"074b10ae57882894","fromNode":"41f7f87c3fbd9530","fromSide":"bottom","toNode":"c85d7ed0a40f14b3","toSide":"top"},
		{"id":"e3adf8cd1f162b35","fromNode":"41f7f87c3fbd9530","fromSide":"bottom","toNode":"4c8cb86213c84578","toSide":"top"},
		{"id":"35ac5a7012998eea","fromNode":"59ebe3510cce29a1","fromSide":"top","toNode":"2456e7bc85395650","toSide":"bottom","label":"Perfect Failure Detector"},
		{"id":"3e2d813957b72682","fromNode":"3611e372ae09c45c","fromSide":"bottom","toNode":"22eaf9203b8b3be2","toSide":"top"},
		{"id":"191745da20973a3e","fromNode":"22eaf9203b8b3be2","fromSide":"right","toNode":"2456e7bc85395650","toSide":"left"},
		{"id":"89d0413c303fb638","fromNode":"41f7f87c3fbd9530","fromSide":"bottom","toNode":"bcdd06dbc6b09ef9","toSide":"top"},
		{"id":"a50af75865c50ee0","fromNode":"c85d7ed0a40f14b3","fromSide":"bottom","toNode":"bcdd06dbc6b09ef9","toSide":"top"},
		{"id":"0277e4ae55f76589","fromNode":"7e4b0290f28c62f1","fromSide":"top","toNode":"adcea0fee440c20a","toSide":"bottom"},
		{"id":"c515304671e6c36f","fromNode":"de9132038252d5ae","fromSide":"top","toNode":"b8b2b46fda424547","toSide":"bottom"},
		{"id":"8e3e0e7434c2cdc4","fromNode":"de9132038252d5ae","fromSide":"bottom","toNode":"ac892db9755dc4d6","toSide":"top"},
		{"id":"c661ec05d041aa6d","fromNode":"de9132038252d5ae","fromSide":"bottom","toNode":"2b4be9d6d8ebf9a6","toSide":"top"},
		{"id":"6c1aaefe4f54f118","fromNode":"7e4b0290f28c62f1","fromSide":"left","toNode":"c900344b43c66363","toSide":"right"},
		{"id":"8d629d7d254b9387","fromNode":"48ebe9989920f9da","fromSide":"bottom","toNode":"35db17127a9f9fb0","toSide":"top"},
		{"id":"01db83f734541fac","fromNode":"de9132038252d5ae","fromSide":"top","toNode":"6a2265d74be7e3b8","toSide":"bottom"},
		{"id":"5a42c543447271bb","fromNode":"6a2265d74be7e3b8","fromSide":"top","toNode":"68fff16453912ff2","toSide":"bottom"},
		{"id":"cced06924915e39c","fromNode":"6a2265d74be7e3b8","fromSide":"top","toNode":"df039763a41c9ec8","toSide":"bottom"},
		{"id":"d2a5c51cba8c034e","fromNode":"de9132038252d5ae","fromSide":"right","toNode":"d760a9aae20b7b65","toSide":"left"},
		{"id":"9e3d65eda8260e02","fromNode":"df039763a41c9ec8","fromSide":"right","toNode":"290b7892ed1efb0d","toSide":"left"},
		{"id":"3df6ee764f6dbffd","fromNode":"6a2265d74be7e3b8","fromSide":"top","toNode":"290b7892ed1efb0d","toSide":"bottom"},
		{"id":"233781af7b2dcd5a","fromNode":"de9132038252d5ae","fromSide":"right","toNode":"1e7a53303f8b24f6","toSide":"left"},
		{"id":"eed4fa8f966ea31d","fromNode":"1e7a53303f8b24f6","fromSide":"bottom","toNode":"e9eb9f5f2c7957ad","toSide":"top"},
		{"id":"f9704ef47cde9993","fromNode":"1e7a53303f8b24f6","fromSide":"right","toNode":"d968f873f92b2c28","toSide":"left"},
		{"id":"9a44409aad03ec7b","fromNode":"d760a9aae20b7b65","fromSide":"bottom","toNode":"d968f873f92b2c28","toSide":"top"},
		{"id":"dcfedc36d207dd8f","fromNode":"1e7a53303f8b24f6","fromSide":"right","toNode":"d760a9aae20b7b65","toSide":"left"},
		{"id":"8f3dae3c1a4af729","fromNode":"d760a9aae20b7b65","fromSide":"right","toNode":"3bd19f158ca150ea","toSide":"left"},
		{"id":"c600571b38a703f9","fromNode":"1e7a53303f8b24f6","fromSide":"bottom","toNode":"1e17cf703d6fb30f","toSide":"top"},
		{"id":"1921a1a8adca3ebf","fromNode":"d968f873f92b2c28","fromSide":"bottom","toNode":"590a5841f2242f04","toSide":"top"},
		{"id":"afb37a7ea3ab93c1","fromNode":"d968f873f92b2c28","fromSide":"bottom","toNode":"1e17cf703d6fb30f","toSide":"top"},
		{"id":"4c4a54dea879f894","fromNode":"3bd19f158ca150ea","fromSide":"bottom","toNode":"d968f873f92b2c28","toSide":"right"},
		{"id":"61e4f8f6ac554ac8","fromNode":"68fff16453912ff2","fromSide":"left","toNode":"6a9218175a8a7feb","toSide":"right"},
		{"id":"c21fdc866c2da161","fromNode":"6a2265d74be7e3b8","fromSide":"right","toNode":"9e09ec64904ea3d2","toSide":"left"},
		{"id":"49c52adbb980b900","fromNode":"41f7f87c3fbd9530","fromSide":"left","toNode":"5c8dfb6c456d2f6b","toSide":"right"},
		{"id":"5c02127e636888bf","fromNode":"2456e7bc85395650","fromSide":"top","toNode":"d1b73c391592333f","toSide":"bottom"},
		{"id":"62056290b7a7e6b0","fromNode":"d760a9aae20b7b65","fromSide":"top","toNode":"6ac21c68e718730b","toSide":"bottom"},
		{"id":"19d60ea071319dbd","fromNode":"6ac21c68e718730b","fromSide":"right","toNode":"3bd19f158ca150ea","toSide":"top"},
		{"id":"acf8e87a575c72de","fromNode":"6ac21c68e718730b","fromSide":"top","toNode":"97480c66250b7fcc","toSide":"bottom"},
		{"id":"49aa41dc69402b28","fromNode":"590a5841f2242f04","fromSide":"bottom","toNode":"a9a6fe3bf7a96bbe","toSide":"top"},
		{"id":"4800c4c13b5226c0","fromNode":"e9eb9f5f2c7957ad","fromSide":"right","toNode":"1e17cf703d6fb30f","toSide":"left","label":"Puoi vedere Wash Trading come un Pump and Dump dilazionato nel tempo"},
		{"id":"9f403115ddac34d0","fromNode":"d760a9aae20b7b65","fromSide":"left","toNode":"d804d42ee844e62b","toSide":"bottom"},
		{"id":"beffb9978ad54480","fromNode":"d804d42ee844e62b","fromSide":"top","toNode":"401ce8238044133c","toSide":"bottom"},
		{"id":"24d78570732ddcee","fromNode":"590a5841f2242f04","fromSide":"right","toNode":"b12c0b1740440efa","toSide":"top"},
		{"id":"70c14d0f7167354e","fromNode":"a9a6fe3bf7a96bbe","fromSide":"right","toNode":"b12c0b1740440efa","toSide":"left"}
	]
}