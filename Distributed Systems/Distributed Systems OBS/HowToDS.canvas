{
	"nodes":[
		{"id":"ae64a718f65ccd25","type":"text","text":"to do exercises don't use notion of topological order","x":1197,"y":160,"width":250,"height":120},
		{"id":"f82b77ddb75926d4","type":"text","text":"# Distributed Systems\n\nUn DS è un sistema in cui $n$ processi cooperano per completare una task.\n\nUn processo è definito da una ***timeline*** e dagli ***eventi*** (e.g. creazione di un file, modifica) che avvengono su di essa. Quando si verifica un evento sul processo $i$, questo può informare tramite un ***messaggio*** il processo $j$.\n\nLo scambio di messaggi avviene tramite canali FIFO, assumendo che questi siano ***asincroni*** (i.e non esiste un tempo limite entro il quale sono certo che il messaggio arriverà) e ***unreliable*** (i.e. i messaggi si possono perdere).","x":295,"y":-303,"width":640,"height":303,"color":"6"},
		{"id":"fbc1ef28f3589012","type":"text","text":"# Definizioni Processi\n\nPer l'insieme dei processi che scambiano messaggi definiamo\n\n- ***History*** - $h_i$ è l'insieme di tutti gli eventi sulla timeline del processo $i$;\n\t- $H=\\{h_i\\}$ è la collezione di tutte le storie.\n- ***Prefix*** - $p_k(h_i)$ è l'insieme dei primi $k$ eventi di $h_i$;\n- ***Local State*** (***LS***) - dopo $k$ eventi, il processo $i$ si trova nello stato $\\sigma_i^k$;\n\t- ***Global State*** (***GS***) - Collezione di tutti i LS al tempo $k$;\n- ***Cut*** (***C***) - $C=\\{p_{k_{i}}(h_i)\\}$ è una collezione dei prefissi di ogni processo. Non necessariamente ogni processo deve avere lo stesso numero di eventi $k$ nel proprio prefisso (infatti $k$ è una $k_i$), né è necessario che $k\\neq0$. Ogni C individua un GS;\n- $e_2^1$ è l'evento con cui il processo $2$ manda una `request` al processo $1$, e $e_1^2$ è l'evento con cui il processo $1$ registra il ricevimento di tale `request`, necessariamente $e_2^1\\to e_1^2$ (i.e. $e_2^1$ ***avviene prima di*** $e_1^2$).\n\nDetto questo, l'ordine degli eventi sulla singola $h_i$ viene mantenuto ad ogni ***run*** (i.e. ad ogni esecuzione dell'intero sistema), ma l'ordine relativo degli eventi di diversi processi può variare. Formalmente, \"a ***run*** $k'$ is a reordering of events such that internal order of every process is preserved\", i.e. sono diverse sequenze di computazione che preservano la coerenza del singolo processo.\n\nQuesto da solo non garantisce che, ad esempio, $e_2^1\\to e_1^2$. Una run in cui tutte le relazioni tra eventi di tipo $\\to$ sono rispettate è detta ***consistent run*** (***CR***). In modo analogo, un ***consistent cut*** (***CC***) è un cut tale che se $e\\to e'\\,\\wedge\\,e'\\in C\\Rightarrow e\\in C$.  Una consistent run è una run in cui qualsiasi C è consistente.","x":-1320,"y":-451,"width":880,"height":599,"color":"4"},
		{"id":"4fd496a7b455c76c","type":"file","file":"SchemaGenerale.png","x":-300,"y":-303,"width":444,"height":303},
		{"id":"eead1953cd38ffd8","type":"text","text":"# Lamport Clock\n\nMettiamo processi da 0 a 3. Ogni volta che su un processo da 1 a 3 avviene un e, questo lo notifica a $p_0$, il quale può ricostruire la run (è un osservatore). visto che non so quanto ci mettono i singoli messaggi ad arrivare a p_o, potrei avere delle inconsistenze! Non solo potrebbe non essere consistente, ma potrebbe non essere nemmeno una run!! come risolvo? Con un canale FIFO! weak assumption, very easy to implement. ma questo canale è per ogni coppia $(i;0)$ non mi dice niente sull'ordine dei diversi processi. Quindi così è una run, ma non necessariamente consistente. what if i give you a global clock (aka Real Clock RC), assume every process can use it. very strong assumption, cause no god gives us a clock, but i can use it to build a consistent run.\n\ndiciamo che $\\delta$ è un upper bound per il tempo impiegato da ogni canale per deliverare il messaggio a p_0. Ogni messaggio ha un timestamp, e p_0 ha una finestra di osservazione larga $\\delta$. entro questa finestra ha un buffer in cui aspetta di essere sicuro di ricevere tutti i messaggi di quella finestra.\n\n\n$e\\to e' \\Rightarrow RC(e)<RC(e')$. **Clock condition**. Non serve RC, basta qualsiasi clock con questa proprietà. LA freccia non è al contrario! $e\\to e'$ implica una necessità di ordine, che non è necessaria per **eventi concorrenti** (segnati con una freccia sul grafico, i singoli punti sono eventi indipendenti)\n\nDefiniamo un sapienza clock SC local to every p. se $e_1^1\\to e_2^1$ e il primo ha timestamp 1, il secondo deve avere timestamp maggiore, quindi sarà 2. dopodiché procedo con gli eventi in ordine, finché 7 sulla linea 2 non manda un messaggio alla linea 1. a qualsiasi numero sono arrivato sulla linea 1, ricevere un messaggio da 2 deve avere timestamp maggiore sia della linea 2 che di quella 1.\n\n- se 1 era arrivata a 5, il massimo +1 è 8;\n- se era a 12, è 13.\n\nQuesta roba è il **Lamport (or logical) Clock**\n\nora, sul singolo canale FIFO sono sicuro di avere i timestamp in ordine. ma prima di aggiungere $e_2^3$ devo aspettare tutti gli eventi precedenti? Non proprio, solo quelli che precedono logicamente. vedi grafico. e come entra $\\delta$ in tutto questo? se ricevo $e_2^3$ con timestamp 4, allora devo aspettare tutti gli eventi degli altri processi con timestamp fino a 4. this might make me wait. e se p_3 non ha nessun evento per un bel po'? ...boh si è dimenticato?...\n\n","x":1440,"y":-4440,"width":640,"height":1180},
		{"id":"941a38f0264b5e2e","type":"text","text":"# Lamport vettoriale (vector clock)\n\nse ci sono eventi concorrenti (quindi non c'è un ordine tra loro). i loro timestamp non possono essere numeri. allora posso modificare un po' la definizione dei timestamp, e renderli non numeri ma con la loro history. in questo modo$$e\\to e' \\Leftrightarrow TS(e)\\subseteq TS(e')$$\nin questo modo due eventi senza freccia sono semplicemente due eventi le cui storie non sono una un sottoinsieme dell'altra. ma la history di tutta la run! è un vettore. ad esempio, la timestamp di $e_2^3$ è $[1,4,2]$, ovvero il vettore dei lamport clock di ogni processo, così non pesa 5 terabyte dopo un'ora.\n\nvedi secondo grafico.\n\np_0 riceve $[141]$, quindi sa che deve ricevere prima una notifica da 1, 3 da 2 (ma di questo sono sicuro, perché il singolo canale è FIFO) e 1 da 3. nota che così non serve $\\delta$.\n\nquindi la condizione di correlazione $e_i\\to e_j$ è $$\\forall\\,k\\, VC(e_i)[k] \\leq VC(e_j)[k]\\quad\\wedge\\quad\\exists\\,k': VC(e_i)[k'] < VC(e_j)[k']$$se non metto la seconda condizione potrebbero essere lo stesso evento. ovviamente se so che i due eventi appartengono a processi differenti non serve la seconda condizione, anzi, basta confrontare il clock associato ad un singolo processo $k$ (e.g. \\[310\\] sul processo 1 avviene prima di \\[240\\] sul processo 2. controllando NON SONO CONVINTO DI QUESTA ROBA)\n\nvector clock usato nei DB distribuiti. così vediamo anche i deadlock (how?)\n\nreachable = run at some point is equal to the cut","x":2097,"y":-4311,"width":572,"height":923},
		{"id":"120f425563a323e0","type":"text","text":"run = ogni possibile esecuzione degli eventi in modo che l'ordine del singolo processo sia rispettato\n\ntopological order = consistent run?\n\ncioè tipo che su un grafo in generale non c'è topological order perché ci possono essere cicli (qui proviamo che non possono esserci cicli perché sono diagrammi space-time)","x":2960,"y":-3584,"width":328,"height":393},
		{"id":"e0ba5ac0341a437d","type":"file","file":"RequestAndCut.png","x":-300,"y":-1448,"width":444,"height":370},
		{"id":"f9d9980fa75657d5","type":"text","text":"# Sincronizzazione e Causal Delivery\n\nPer ***ricostruire la run di un DS*** possiamo usare un processo $p_0$ al quale tutti i processi $p_i$ inviano i propri eventi.\n\nTuttavia il sistema è asincrono, quindi quando $p_0$ manda le `request` ai singoli processi questi in generale ricevono e rispondono in momenti diversi. Questo può portare $p_0$ a vedere un ***cut inconsistente*** del sistema, e di conseguenza a ricostruire una ***run inconsistente***.\n\nPotrebbe perfino non essere una run, se ad esempio $e_1^2$ arriva prima di $e_1^1$. Questo problema si risolve assumendo che i singoli canali siano FIFO.\n\nPer ovviare a questo problema è necessario sviluppare un criterio di ordinamento robusto rispetto ad asincronia ed unreliability, realizzato tramite un ***clock*** sul quale tutti i processi concordano.\n\nL'idea generale è che $p_0$ faccia `commit` solo e soltanto se l'evento che riceve è l'immediato successivo dell'ultimo registrato, mantenendo in un ***buffer*** tutti gli eventi ricevuti nell'ordine sbagliato.","x":295,"y":-1515,"width":640,"height":503,"color":"6"},
		{"id":"ab73850207f6a30a","type":"text","text":"# Notazione\n\nL'unica informazione utile da specificare sui messaggi è il processo da cui vengono, quindi $m_j$ è inviato dal processo $j$.\n\nUn evento $e_i^k$ è il $k$-esimo evento del processo $i$.\n\nAl contempo, posso riferirmi a generici eventi e dire che $e_i\\neq e_j$. In questo caso sono semplicemente eventi, senza specificare dove (il processo) o quando (VC) siano avvenuti.","x":1063,"y":-1654,"width":599,"height":279,"color":"4"},
		{"id":"0aa59bf09c992c32","type":"text","text":"# Chandy-Lamport Protocol\n\nEsiste un metodo più leggero rispetto al notificare $p_0$ di ogni evento?\n\nSe è $p_0$ a mandare tutte le `request` per conoscere i LS, non è detto che riesca a ricostruire un GS coerente (il sistema è asincrono). Allora delego ai processi.\n\n- $p_0$ manda una `request` con un `ID` univoco a tutti i processi;\n- Ogni processo che riceve una `request` può fare due cose:\n\t- la inoltra in ***broadcast***, se è la prima volta che la riceve;\n\t- la ignora se l'ha già ricevuta.\n- $p_0$ vede le risposte dei processi, realizzando un ***consistent cut***.\n\t- Come faccio a saperlo? Ipotizziamo non lo sia, e che quindi esiste una coppia di eventi $e\\to e'$ tali che $e\\notin C$ e $e'\\in C$. Segue che il cut deve essere fatto prima di $e$. Dal momento che i canali sono FIFO e il messaggio di `broadcast` è partito prima del messaggio di $e$, arriverà sicuramente prima l'evento di cut rispetto all'evento $e'$, il che implica $e'\\notin C$, da cui la contraddizione.\n\nÈ facile vedere che per $n$ processi vengono inviati $n$ `broadcast`. Il protocollo termina quando tutti i processi hanno ricevuto esattamente $n$ `broadcast`.","x":1800,"y":-1538,"width":665,"height":549,"color":"4"},
		{"id":"3458d02a2952e9c7","type":"text","text":"# Real Global Clock\n\nLa prima tentazione è quella di definire un ***Real Global Clock*** (RC) condiviso da tutti i processi, in modo tale che$$e\\to e' \\Rightarrow RC(e)<RC(e')$$Questa prende il nome di ***Clock Condition***, e notiamo che\n\n- L'implicazione non è un $sse$. Due eventi tali che $RC(e)<RC(e')$ possono banalmente essere scorrelati (***eventi concorrenti***), dunque non c'è alcuna necessità che $e\\to e'$;\n- È possibile costruire clock che rispettino questa condizione senza la strong assumption che siano RC.\n\n\n Se il canale fosse sincrono potrei definire la grandezza $\\delta$ della finestra di buffer per $p_0$ e una ***delivery rule*** del tipo \"At time $t$, deliver all messages in order whose timestamp is smaller than $t -\\delta$\".\n \nUna simile costruzione è problematica non tanto per l'assunzione di avere un clock globale (che si potrebbe risolvere con un Network Time Protocol, NTP), quanto per la necessità di implementare una delivery rule che dipende da una finestra temporale $\\delta$ (dunque siamo costretti ad assumere che il sistema sia sincrono).\n","x":-373,"y":-2559,"width":580,"height":588,"color":"4"},
		{"id":"9db64449abbcd63a","type":"file","file":"ChandyLamport.png","x":1800,"y":-835,"width":665,"height":384},
		{"id":"ffd7c9469255f9c8","type":"text","text":"# Atomic Commits\n\nFare ***commit*** significa aggiornare lo stato di un DB in modo ***irreversibile***.\n\nIn un DS, ogni DB deve concordare su ogni `commit` che viene eseguito. Se non c'è ***consenso*** unanime, la transazione viene rifiutata (i.e. ***abort***). \n\nIl singolo DB deve possedere sempre una ***copia coerente*** del DB distribuito, i.e. una versione della sequenza di modifiche sulla quale tutti concordano.\n\nQuesta necessità collettiva è più importante del `commit` che vorrebbe eseguire (e far eseguire a tutti) il singolo DB.\n\nDue criteri fondamentali per valutare i protocolli:\n\n- ***safeness*** - quanto il protocollo non permette copie incoerenti;\n- ***liveness*** - quanto il protocollo \"fa qualcosa\".\n\nUn protocollo che non fa nulla è safe, ma non live. In generale si richiede ***massima safeness cercando di ottimizzare la liveness.***","x":292,"y":1070,"width":640,"height":499,"color":"6"},
		{"id":"dd46627802841779","type":"text","text":"# L3\n\nesempio foto\n\ndefiniamo $|\\Theta(e_i)|=\\sum_k VC(e_i)[k]$, ovvero la \"misura\" del vector clock.\n\nvoglio sapere se esiste $e_k$ t.c. $e_k$ NON è avvenuto prima di $e_i$, ma è avvenuto prima di $e_j$. In pratica voglio sapere se esiste un evento $e_k$ tra $e_i$ e $e_j$ (non letteralmente nel diagramma spaziotempo, basta che sia vero nella run). A livello di VC lo traduco come? $VC(e_i)[k]$ è il numero di eventi del processo $k$ che sono avvenuti prima dell'evento $e_i$, quindi$$VC(e_i)[k] < VC(e_j)[k]$$\n\nora, inizio a registrare. Se mi arriva $[001]$ posso registrarlo? Sì, perché sono sicuro che non può arrivare un messaggio che sia avvenuto prima di questo. e se poi arriva $[232]$? Ovviamente non posso registrarlo. Cosa discrimina tra i due casi, intuizione a parte? posso deliverare solo se una delle componenti dell'incoming message è esattamente +1 rispetto a quello che ho salvato (nel qual caso la aggiorno). riguardati come funziona il VC, funziona. Nota che è facilitato dal fatto che il singolo canale è FIFO, ma non è strettamente necessario che lo sia.\n\n\n\n- Se è $p_0$ che requesta i local states, non è detto che riesca a ricostruire un global state coerente (i messaggi arrivano ai processi in momenti diversi)\n- potremmo fare che quando il processo $p_i$ riceve la request fa broadcast su tutti gli altri processi. se un processo riceve lo snapshot del local state da un altro processo, fa partire il suo broadcast senza aspettare la notifica di $p_0$.\n- il Cut sui punti in cui i processi fanno broadcast è consistente. questo perché i canali sono FIFO, quindi una qualsiasi freccia che implica una relazione $e_i\\to\\ e_j$ avviene prima della comunicazione broadcast. vabbè lo dimostra per contraddizione con la definizione.\n- sta roba si chiama protocollo chandy-lamport\n- se fai così non ci stanno deadlock (continua ad accennarlo senza entrare nel dettaglio)\n\nTutta questa era la fase 1, poi passeremo agli atomic commit (blockchain e cose varie)","x":2760,"y":-1976,"width":780,"height":923},
		{"id":"48ebe9989920f9da","type":"text","text":"# Consensus Problem\n\nL'Atomic Commit è un caso particolare del più generale problema del consenso.","x":3934,"y":1245,"width":666,"height":150,"color":"6"},
		{"id":"db4411d94cf78039","type":"text","text":"# Lamport's Clock (LC)\n\nPer realizzare un clock possiamo usare dei ***counter*** che fungono da ***timestamp***. Una definizione che rispetta la clock condition è la seguente:\n\n- Sul singolo processo, ogni evento concorrente o di tipo `sending` incrementa di $1$ il counter, i.e. all'evento $n$ segue l'evento $n+1$;\n- Quando si verifica un evento di tipo `receiving`, il messaggio in arrivo porta con sé il timestamp $m$ dell'evento `sending` dal quale è partito. L'evento `receiving` assume allora valore $\\max(n+1, m)$, ovvero\n\t- $n + 1$ come nel caso precedente se il $m<n$;\n\t- $m$, se $m>n$.\n\nIn questo modo posso mettere come delivery rule \"Deliver all received messages that are stable in timestamp order\", dove \"A message $m$ is stable if no future message with timestamp smaller than $TS(m)$ can be received\".\n\nQuesto non risolve però tutti i problemi.\n\n- Esistono diversi eventi con lo stesso timestamp. Questo significa che $p_0$ non ricostruisce esattamente la run, ma solo una possibile run;\n- Eventi concorrenti diventano di fatto dipendenti: secondo questa logica $TS(e_3^3) = 3$ e $TS(e_2^2)=5$, quindi sembra che $e_3^3\\to e_2^2$. Come risolvo?","x":295,"y":-2559,"width":640,"height":588,"color":"4"},
		{"id":"e6fd42bc1097166e","type":"text","text":"exam\n\nmidterm 3 esercizi (+ altro?)\n\nesempio ex\n\n$C_1$ e $C_2$ sono due CC. Show that $C_1\\cap C_2$ is consistent.\n\n$C_1\\cap C_2 \\subseteq C_1$, which is consistent.\n\ndevi usare la definizione per cui un consistent cut è \"se e precede e', ovvero $e \\to e'$ ecc...\n\nanche l'unione è consistente.","x":-200,"y":-3444,"width":430,"height":540},
		{"id":"6dcdd47739ab639e","type":"file","file":"LamportClock.png","x":415,"y":-2904,"width":400,"height":264},
		{"id":"61752071951b1015","type":"text","text":"# Vector Clock\n\nÈ chiaro che a causa degli eventi concorrenti il clock non può essere un semplice numero. Definisco allora una ***strong clock condition***$$TS(e)<TS(e') \\Leftrightarrow e\\to e'$$In pratica voglio che una relazione d'ordine tra i timestamp definisca la dipendenza tra gli eventi. Posso trovare una rappresentazione per i $TS$ che consenta una relazione d'ordine per eventi dipendenti e non la consenta per eventi concorrenti? Sì: usiamo un ***vector clock***.\n\nDiciamo che la nuova condizione di correlazione è$$e_i\\to e_j \\Rightarrow \\forall k\\, VC(e_i)[k] \\leq VC(e_j)[k]\\,\\,\\wedge\\,\\,\\exists\\,k': VC(e_i)[k'] < VC(e_j)[k']$$cioè se $e_i\\to e_j$ allora ogni componente del $VC$ di $e_i$ deve essere minore o uguale del $VC$ di $e_j$, di cui almeno una strettamente minore (se non metto quest'ultimo pezzo è valido anche $e_i\\to e_i$).\n\nIn pratica abbiamo detto che $e\\to e' \\Leftrightarrow TS(e)\\subseteq TS(e')$, ovvero che due eventi sono correlati $sse$ la storia di uno è un sottoinsieme della storia dell'altro (per eventi concorrenti questa condizione è falsa, senza rompere i casi in cui la condizione è vera).\n\nOk, ma come è fatto il $VC$? Esattamente come il $LC$, semplicemente ogni processo ha un vettore con tutti i $LC$ di tutti i processi.\n\nE la delivery rule? Quando $p_j$ invia un messaggio $m_j$ a $p_0$, questo contiene il suo VC, ovvero un $TS(m_j)$. $p_0$ fa una commit se$$D[j] = TS(m_j)[j] - 1\\quad \\wedge\\quad D[k]\\geq TS(m_j)[k]\\quad\\forall k\\neq j$$o più intuitivamente$$TS(m_j)[j] = D[j] + 1\\quad \\wedge\\quad TS(m_j)[k]\\leq D[k]\\quad\\forall k\\neq j$$\ndove $D$ è il VC dell'ultimo `commit`, ovvero se all'arrivo di $m_j$\n\n- il suo $VC[j]$ (i.e. il suo stesso LC) è ***esattamente $+1$ rispetto al dato salvato*** da $p_0$ in $D[j]$. In questo modo, $p_0$ è sicuro di aver già visto tutti gli eventi che precedono questo lungo $p_j$;\n- $p_0$ deve anche essere ***sicuro di conoscere già tutti gli eventi degli altri processi $k\\neq j$ che sono precedenti a quello corrente***, il che si traduce nella disuguaglianza.\n\t- Segue che $D$ non si aggiorna solo quando viene eseguito un `commit`, ma anche quando un messaggio finisce nel buffer di attesa (i.e. basta che $p_0$ ne sia a conoscenza).","x":1063,"y":-2806,"width":599,"height":1082,"color":"4"},
		{"id":"0b9a16cefb33b26c","type":"file","file":"VectorClock.png","x":1163,"y":-3166,"width":400,"height":262},
		{"id":"7e4b0290f28c62f1","type":"text","text":"# Security","x":3934,"y":-303,"width":666,"height":303,"color":"6"},
		{"id":"b2214078d2cc0c48","type":"text","text":"# L4 (Two Phase Commits)\n\nCINECA?\nnello schemino in cui (nome, voto) viene splittato in (nome) e (voto) su 3 computer diversi serve ovviamente che ci siano protocolli locali tipo login e rollback localmente su ognuno di essi, ma è un sistema distribuito in cui le operazioni {splitta nomevoto, salva nome, salva voto, cancella nomevoto} deve essere atomico, cioè o tutto o niente. Se non implemento nessun protocollo per assicurarmi di ciò, rischio un'inconsistenza (e.g. il nome viene perso, posso votare di nuovo).\n\nquesta roba si realizza con two phase commit: quando parte la transazione, o tutti i siti la fanno o non la fa nessuno.\n\nassumiamo che i processi siano either crash fail, crash stop or crash recovery. sistema asincrono, no byzantine nodes (?).\n\n- A1 - if processes reach a decision (either commit or abort) it must be the same for all of them;\n\t- versione alternativa scritta sul libro: all processes that reach a decision, reach the same one\n- A2 - A process cannot reverse its decision after reaching one;\n- A3 - The commit decision can be reached only if all processes voted `yes`.\n\t- non dico iff perché voglio lasciarmi aperta la pista di abortire anche se tutti hanno votato `yes`.\n\nnota che anche un protocollo che non fa letteralmente nulla soddisfa queste proprietà, quindi mi servono condizioni aggiuntive.\n\n- safety propriety - nothing bad should happen\n- liveness property - \"ok, but do something\"\n\nse hai solo safety è ok se non fai nulla (e.g. A1 2 3), se sei pazzo sei liveness. tipicamente richiediamo di essere safe ma essere as live as you can.\n\n- A4 - ***If there are no failures*** and all processes voted `yes` then decision is `commit`.\n","x":-2010,"y":1810,"width":880,"height":761},
		{"id":"c60727cb76e10ee4","type":"text","text":"\nin un processo del genere solitamente c'è un coordinatore e dei partecipanti. questi ultimi votano, il primo no (oppure sì, not a problem if coordinator is also participant). quindi: sistema distribuito è così\n\n- coordinatore C manda `voteRequest` ai partecipanti P\n- i quali decidono `yes` or `no`\n\t- se votano `no` fanno `abort`, e ciò è safe.\n- e mandano il voto a C\n\t- e se il messaggio si perde o è molto lento? C deve essere sicuro di avere tutti i voti `yes` per, in caso, fare commit. Quindi è safe.\n- se C riceve TUTTI `yes`, allora può decidere di fare commit. sta di fatto che qualcosa decide\n- allora manda la decisione a tutti, i quali eseguono.\n\nse il protocollo non ha failures e sincrono (i.e. i messaggi arrivano in tempi utili) è perfetto. In una situazione reale non è così.\n\nma posso mai aspettare per sempre? ovviamente no, metto un timeout al termine del quale mando a tutti una decisione di abort.\n# Failures\n\n- se si perde `voteRequest`?\n\t- la prima volta al partecipante scade il timeout e chiede a C di mandare `voteRequest`\n\t- la seconda fa un `abort`.\n- se si perde il voto?\n\t- C manda di nuovo la request\n\t- la seconda volta che scade il timeout decido di abortire.\n\t\t- eh ma magari arriva un attimo dopo ed erano tutti `yes`. \"eh, such is life\"\n- se si perde la decisione che C manda a P?\n\t- se ho votato no, easy, tanto ho già abortito\n\t- se ho votato yes non è per niente safe abortire, quindi mi tocca aspettare. magari gli rimando un \"`aoo sta decisione??`\" sperando che arrivi. e magari lo chiedo tante volte e nessuno mi risponde. eh. magari C è morto. allora magari chiedo agli altri P se hanno ricevuto la decisione (assumo che non mentano). gli altri P possono rispondere in due modi\n\t\t- ho ricevuto la decisione, fine\n\t\t- non l'ho ricevuta, e\n\t\t\t- ho votato no - P fa un `abort`\n\t\t\t- ho votato `yes`. cazzo. chiedo a un altro. se chiedo a tutti e TUTTI hanno votato `yes` e NESSUNO ha ricevuto la decisione, tutti fanno `abort`.\n\nin sostanza, two-phase commit è safe ma non live. scopriremo che nessun protocollo è sia safe che live. c'è un teorema di impossibilità di avere entrambe. per questo chiediamo protocolli safe e as live as can be. è anche un cooperative protocol\n","x":-2010,"y":2640,"width":880,"height":1180},
		{"id":"b51bcda4f72fdde5","type":"text","text":"### Origine del nome \"Byzantine\"\n\nIl termine deriva dal cosiddetto **\"Problema dei Generali Bizantini\"** (_Byzantine Generals Problem_), un problema teorico descritto in un famoso articolo di Leslie Lamport, Robert Shostak e Marshall Pease nel 1982.\n\nL'idea dietro il nome è legata a uno scenario in cui un gruppo di generali di un esercito bizantino deve coordinarsi per attaccare o ritirarsi da una città. Tuttavia, alcuni di questi generali potrebbero essere traditori e potrebbero inviare informazioni false agli altri. Il problema diventa come garantire che i generali leali possano giungere a un accordo su una strategia comune, nonostante la presenza di traditori (ovvero, processi difettosi o maliziosi).\n\nQuesto nome pittoresco viene usato per descrivere l'aspetto complesso e potenzialmente confuso dei fault che possono accadere in un sistema distribuito quando uno o più processi non si comportano in modo affidabile o coerente.","x":-1520,"y":1099,"width":640,"height":442},
		{"id":"93212aed82c21df9","type":"text","text":"# Two-Phase Commit\n\nL'idea è di per sé semplice, poi si complica per farla funzionare in caso di ritardi e/o failures: dividiamo i nodi (processi) in ***coordinatori*** e ***partecipanti***, quindi abbiamo i seguenti step:\n\n- Il coordinatore `C` inizia il protocollo mandando una `voteRequest` ai partecipanti `P`;\n- Ogni partecipante decide cosa votare tra `yes` e `no`;\n\t- Se vota `no` fa direttamente `abort`.\n- La decisione di voto viene inviata a `C`;\n- Se `C` riceve le risposte di tutti i `P` e queste sono tutte `yes`, allora ***può*** prendere la decisione di fare `commit`.\n\t- Notare che non è obbligato a fare `commit`, sta di fatto una decisione viene presa.\n- `C` invia la decisione a tutti, i quali eseguono.\n\nTutto troppo bello. In un sistema reale ***i messaggi possono essere persi***, o fare talmente tanto ritardo che si danno per persi (tramite un timer). Cosa succede in questo caso?\n\n- Se si perde la `voteRequest`,\n\t- la prima volta che scade il suo timer, `P` chiede a `C` di rimandare la `voteRequest`;\n\t- la seconda assume che `C` sia morto e nel dubbio fa un `abort`.\n- Se si perde il voto, a `C` scade il timer per la risposta.\n\t- la prima volta `C` manda di nuovo la `voteRequest`;\n\t- la seconda assume che `P` sia morto e nel dubbio decide per un `abort`.\n- Se si perde la decisione che `C` manda a `P`\n\t- se io `P` ho votato `no` non è un problema, tanto ho già fatto `abort`;\n\t- se ho votato `yes` non è per niente safe scegliere di fare `abort`, quindi mi tocca aspettare. Magari mando a `C` un gentile sollecito come \"`aoo sta decisione??`\" sperando che arrivi. Ma magari lo chiedo tante volte e nessuno mi risponde. Magari `C` è morto. Allora chiedo agli altri `P` se hanno ricevuto la decisione. Questi possono rispondere in tre modi:\n\t\t- `ho ricevuto la decisione` - applico la decisione;\n\t\t- `non l'ho ricevuta, e...`\n\t\t\t- `... ho votato no` - faccio un `abort`;\n\t\t\t- `ho votato yes` - brutta situazione, non posso concludere niente e mi tocca chiedere ad un altro. Se chiedo a tutti, ***tutti*** hanno votato `yes` e ***nessuno*** ha ricevuto la decisione, tutti fanno `abort`.\n\nNotare che è un protocollo safe, ma è molto poco live (ci sono molti più modi per fare `abort` che per fare `commit`). Questa fratellanza tra `P` lo rende un ***cooperative protocol*** nel quale anche un singolo fail (tutti i processi sono fail-stop) fa fare `abort` a tutti.\n","x":-776,"y":2068,"width":753,"height":1030,"color":"4"},
		{"id":"23a2f6367ee648bd","type":"text","text":"è anche possibile che C faili mentre manda i messaggi, tipo che lo dice a qualcuno e poi crasha. legge il log e rimanda a tutti.","x":629,"y":5132,"width":345,"height":201},
		{"id":"be570a1920f04449","type":"text","text":"\n# Log\n\ntutto questo ha senso se i processi sono fail-stop. e se sono fail-recover? quando recoverano perdono memoria! Tranne? Il ***LOG*** (sulla singola macchina. ovviamente puoi farlo sul DS ma servirebbe un TPC per il log condiviso, il che significa provare a risolvere il problema con il problema).\n\n\"eh ma il log sta su disco, può fallire\". Ho capito, ma mica puoi risolvere tutto. magari fai più dischi ma muoiono tutti. non solo. quando chiamo una syscall per scrivere su disco, magari questa scrive in RAM. allora devo forzare a scrivere su disco (tipo con una `flush`, su linux si chiama `fsync`?). ma magari il disco ha un buffer e salta la corrente, perdo il buffer. si usa un disco solo per il log e lo configuro in modo che non usi il buffer. (ha fatto un commento su journaled filesystem? non ho sentito!).\n\nil punto di tutto sto pippone è che mi serve assumere che il log sia safe in modo che quando recovero leggo il log.\n\ndetto questo. è meglio scrivere sul log e poi mandare il messaggio o viceversa? in entrambi i casi potrei morire nel mezzo! ma è meglio la seconda scelta. se rivivo e leggo sul log che ho mandato il messaggio non lo manderò mai. meglio mandare e loggare, alla peggio mando messaggi duplicati. però puoi ricevere voti di una request che non sai di aver mandato. tu nel dubbio vedi sta roba e dici `nono che è sta roba, abortite tutti`. mi sa che posso scegliere entrambi a seconda delle necessità\n\nstesso problema per P. mando il voto e loggo o viceversa?\n- voto e loggo, morendo in mezzo - quando rivivo non so che ho votato. né posso rimandare il voto, perché potrebbe essere diverso. \n- viceversa - quando rivivo penso di aver mandato il messaggio, ma non posso essere sicuro, quindi nel dubbio lo rimando.\nquindi se ho votato no abort, altrimenti aspetto.\n\ncapiamo che in pratica se voto no non cambia niente, se voto `yes` è meglio log e poi send, alla peggio mando lo stesso messaggio due volte perché so qual è.\n\nStesso problema per C quando manda la decisione. prima safety, quindi\n- log then send is ok, if you die in the middle you see the decision, not sure if has been sent, send it again\n- send then log - safe only if it's an abort.\n\nse quando vedi la decisione presa la rimandi, se non vedi nulla mandi un abort.\n\nyou should be very careful what you log and when.","x":-161,"y":4077,"width":753,"height":1804},
		{"id":"c2b4396b3b84e4de","type":"text","text":"# Overview\n\nIl problema è totalmente descritto da queste proprietà che deve possedere:\n\n1. All processes that reach a ***decision***, must reach the same one.\n2. Processes cannot reverse their decisions.\n3. The `commit` decisions can only be reached if all processes voted `yes`.\n4. If there are no ***failures*** and all processes voted `yes`, decision must be to `commit`.\n\nI `failure` possono essere divisi in due macro-categorie:\n\n- `crash failure` - il processo muore (***fail-stop***), poi eventualmente si risveglia (***fail-recovery***);\n- `byzantine failure` - il processo continua a funzionare, ma lo fa in modo imprevedibile, inconsistente o malevolo.\n\nAssumiamo in questi protocolli che non vi sia alcun `byzantine node`.\n\nEssendo il sistema asincrono, è impossibile distinguere un processo che ha avuto un `crash` da uno che è solo molto lento a rispondere. In virtù del Teorema FLP, questo rende il problema in generale irrisolvibile. Tuttavia, nelle situazioni pratiche questo è raramente un problema.\n\nAnche un processo che sulla carta sembra avere liveness molto bassa può rivelarsi efficiente, perché il tasso di `failures` è generalmente basso.","x":-720,"y":997,"width":640,"height":645,"color":"4"},
		{"id":"d582dc2a1cfd5670","type":"text","text":"# DTLog (Distributed Transaction Log)\n\nSe i processi sono ***fail-recover***, quando resuscitano perdono tutta la memoria tranne il ***log***.\n\nAssumendo che il log non possa avere `fail` e dividendo i nodi in coordinatori e partecipanti, la domanda è: ***meglio scrivere sul log e poi mandare il messaggio o viceversa?*** Se riesco ad eseguire entrambe le operazioni va tutto bene, ma in entrambi i casi potrei morire nel mezzo. L'idea è che, se succede, quando torno operativo leggo il log e riprendo da dove mi ero fermato. Ma è safe fare così?\n\n- `C - voteRequest` - Quando inizia il protocollo scrive sul log `Start2PC`, che contiene la lista dei partecipanti. ***L'ordine è indifferente***:\n\t- se prima scrivo sul log e poi muoio, la lista dei partecipanti è ancora là e nel dubbio rimando la `voteRequest`.\n\t- Viceversa, se risvegliandomi non vedo nulla nel log la scelta migliore è mandare `abort` in `broadcast`;\n- `P - vote` - Qui è cruciale distinguere due casi:\n\t- ***Se decido di votare `yes` devo prima scriverlo sul log***. Viceversa, se mando e muoio poi non so cosa ho votato. Dovrei fare `abort`, ma non posso. Insomma, no.\n\t- **Se decido di votare `no` l'ordine è indifferente**, ma a sto punto faccio come `yes`.\n- `C - Decision` - Anche qui devo distinguere\n\t- ***Se decido `commit` devo prima scriverlo sul log***, per gli stessi motivi;\n\t- **Se decido `abort` l'ordine è indifferente**.\n\nIn pratica per una decisione negativa l'ordine è indifferente, mentre per una positiva è meglio `log then send`, alla peggio mando lo stesso messaggio due volte.\n\n###### Obiezione! Il log è su disco, può fallire!\n\nHo capito, ma mica puoi risolvere tutto. Magari fai più dischi e muoiono tutti. Non solo: quando chiamo una `syscall` per scrivere su disco, magari questa scrive in RAM. Allora dovrei forzare a scrivere su disco (tipo con una `flush`, su linux si chiama `fsync`?). Ma magari il disco ha un buffer e salta la corrente, perdo il buffer. Potresti usare un disco solo per il log e configurarlo in modo che non usi il buffer.\n\nIl punto è che è ragionevole assumere che il log sia safe per sviluppare il protocollo","x":230,"y":2068,"width":764,"height":1030,"color":"4"},
		{"id":"604f69980144ca26","type":"text","text":"# Paxos\n\nÈ generale come Two-Phase Commit, ma ha più fault tolerance e più liveness.\n\nProtocollo maggioritario in cui dividiamo i nodi in tre categorie:\n\n- Proposers (`P`) - non una buona idea averne uno solo, se muore il protocollo è morto. non serve una maggioranza però, quindi ne basta uno vivo.\n- Acceptors (`A`) - Sono i nodi più importanti, perché decidono a maggioranza su quale valore deve fare `commit` l'intero sistema;\n\t- Se ci sono $n$ `A`, la tolleranza del protocollo è il fail di $f=\\text{floor}({n-1\\over2})$;\n\t\t- se ci sono più di $f$ failures, il protocollo si ferma (so it's safe)\n\t- In genere $n$ è dispari, perché la tolleranza è la stessa per $n-1$ e a sto punto ne metto uno in più.\n\nin molte applicazioni pratiche, ogni nodo è sia `A` che `P` che `L`.\n\na inizio protocollo vengono distribuiti staticamente tra i `P` un infinito numero di round in modo che nessuno abbia lo stesso.\n\nprimo messaggio: `prepare(1)`. scelgo il round in modo che sia maggiore dei precedenti, se sono anche `A` e so a cosa siamo arrivati sono facilitato. In generale provo a massimizzare la liveness, perché il protocollo in sé già mi garantisce safeness.\n\ngli `A` raccolgono le prepare, votano e mandano le `promise`.\n\ni `P` raccolgono le promise e decidono quale valore deve essere accettato.\n- se si raggiunge il ***quorum*** $Q\\geq n-f$ (i.e. un numero $Q$ di di `A` ha votato lo stesso valore `x`) allora mandano una `accept(i, x)` agli `A`;\n\nQuando gli `A` ricevono una `accept` mandano una `learn` ai `L`, che in pratica è il voto.\n\n- se i `L` vedono un numero $\\geq Q$ di `learn` con lo stesso valore e round, allora fanno `commit` e il valore diventa chosen\n\nl'obiettivo di paxos è che sia impossibile scegliere due valori diversi.\n\nnon è live manco senza failures e con messaggi sincroni. in pratica lo è a meno di non essere unlucky.","x":1244,"y":2068,"width":753,"height":1030},
		{"id":"0351c84f97899850","type":"text","text":"# L5 (Paxos)\n\nconsesus problema dell'agreement (in questo caso commit o abort). stessa cosa risolta nella blockchain (devi sapere se è accettata o no). Two-phase commit è il primo step verso ***paxos*** (perché basta un solo nodo offline e tutto muore). paxos è il nome di una possibile isola greca (in realtà esiste, ma lamport non lo sapeva) perché racconta sta storia di sti tizi greci con un piccolo parlamento di non professionisti. lo manda così e glielo rifiutano. alla fine dopo lunghe lotte lo accettano ma aggiungono delle note esplicative (le fa Keith Marzullo). nessuno lo capisce, allora pubblica un articolo chiamato \"paxos made easy\". nessuno lo capisce. poi ci sta \"paxos for dummies\".\n\nogni nodo ha un DB. in google filesystem every file is replicated in 5 different places. replication system, paxos is a replication system. se tutto viene fatto in parallelo tra 5 server, ognuno di essi deve avere una copia coerente. sui google doc possono lavorare più persone! quindi serve consenso sull'ordine delle singole operazioni.\n\n- serve consenso non sulla serie di valori, ma sul singolo valore. dobbiamo tollerare i crash failures. sistema asincrono con n processi.\n\nci sono n processi più importanti, detti acceptors. nella storia di lamport sono i membri del parlamento\n\npoi ci sono i proposers. \n\nha senso chiedere che tutti gli acceptor debbano votare su ogni singolo valore perché questo sia accettato, ma così torno al two-phase con i relativi problemi. allora facciamo che serve la maggioranza (perché se così non fosse due proposte opposte possono essere approvate contemporaneamente). se serve maggioranza e ci sono due proposte parallele almeno un computer sarà in entrambe le commissioni, e permetterà coerenza. tolerates $floor({n-1\\over2})$ errors?\n\n- primo messaggio - un proposer manda la proposta con un messaggio di `prepare`. perché non è una `voteRequest`? perché che ne so se mi sono appena svegliato e gli altri proposer hanno proposto altro?\n\t- visto che non abbiamo un orologio usiamo il round. ad ogni processo è preassociato un set di numeri che sono i round in cui può iniziare la conversazione. sono statici e mai condivisi (e.g. se il processo 1 ha il round 1, nessun altro ha il round 1).\n\t- a questo punto capiamo che la `prepare` è una `prepare(3)`, cioè `preparati al round 3`\n- l'acceptor risponde con una `promise(3, lastround in cui ho votato, valore per cui ho votato)` in cui comunicano al processo che ha mandato la `prepare`\n\t- cosa hanno votato l'ultima volta\n\t- quello in mezzo è il numero dell'ultimo round per cui ho votato. posso rispondere `null null` se non ho ancora votato\n\t- perché si chiama promise? perché significa `from now on i will never partecipate to a round smaller than 3`\n- Se il proposer riceve n messaggi in risposta, ha un quadro chiaro della situazione. se la maggioranza risponde di aver votato $5$, è inutile che io proponga $7$, tanto non vincerà mai! (Paxos è per il singolo valore!!).\n\t- anche se volevo proporre 7, propongo 5, o in generale x. (?)\n- mando una `accept(3, x)`, cioè al round 3 dico (agli acceptors? o al proposer? A che mi serve sto passaggio??) che ho accettato x\n- gli acceptors mandano un `learn(3, x)` a tutti gli altri (che di base è il loro ***voto***). Se un learner riceve una maggioranza di voti per quel round (altrimenti no?)\n\t- se sono all'inizio e nessuno ha votato parto con una `accept`.\n\nse dopo il round 3 parte il round 1, gli acceptors non rispondono. allora magari il processo prova con il prossimo numero nel suo array statico, che magari è 4 (se sono 3 processi)\n\nse al round 2 c'era la maggioranza (3/5) ma uno degli acceptors muore, arrivano solo 4 promises, magari 2 voti e due nulli. che faccio? non blocco il protocollo, ma non accetto il nuovo valore.\n\nnon è importante che il processo faccia approvare il proprio valore, ma che tutti concordino.\n\nsta dicendo una cosa circa il fatto che non è possibile cambiare il voto una volta che una maggioranza ha preso una decisione\n\nè possibile che nel round 2 qualcuno voti x ma poi nel round 3 la maggioranza vota y. se la minoranza ha votato x ma al round dopo le loro promise si perdono, il nuovo proposer riceve solo dei \"non ho votato\". è totalmente safe fare accept y.\n\nse le accept ci mettono troppo, il prossimo round comincia. gli acceptor promettono di non partecipare a round inferiori. quando le accept arrivano, vengono rifiutate.\n\nnon è live, ma tanto è impossibile esserlo essendo safe. in pratica lo è, nel senso che quando lo implementi i segnali di norma non rallentano tutti insieme, o simili.\n\nnon tollera malicious activities, quindi non usato nelle blockchain.\n\nfino a qualche centinaio di acceptor paxos va bene, ma blockchain ne ha migliaia.","x":1480,"y":3680,"width":850,"height":1800},
		{"id":"bab53e10c3a7bbde","type":"text","text":"choosing the value x (per l'accept?)\n\n- take the promise with the largest \"last time I voted\" j;\n- the value x is the one associated with that promise","x":2840,"y":1835,"width":399,"height":233},
		{"id":"0084479ff459b8fb","type":"text","text":"# L5 (Paxos II)\n\n`THM` - If acceptors vote for `x` in round $i$, then no value $x'\\neq x$ can be chosen in previous rounds.\n\n`Proof` - Se ho votato (i.e. mandato una `learn`) significa che ho ricevuto un `accept`. Se esiste una `accept` significa che è stato raggiunto $Q$. Procediamo per induzione sul round $i$.\n- Base induttiva (`round 0`) - Il teorema è ovvio;\n- Assumiamo vero per $i-1$ e dimostriamo per $i$ - $Q\\subseteq A$. Diciamo che i facenti parte del $Q$ abbiano votato l'ultima volta al più al round $j$. Segue che nessuno dei $Q$ ha votato tra $j+1$ e $i-1$. Se $j$ valesse $-1$ avremmo finito la dimostrazione. Diciamo che al round $j$ all'interno di $Q$ almeno 1 deve aver votato per $x$ (per definizione è il valore associato al maxround). potrebbe essere l'unico? Sì, perché tutti gli altri accept potrebbero essere stati persi. Ma se almeno uno ha votato x significa che un proposer gli ha detto così. quindi nessun altro `A` può aver scelto $y\\neq x$. Ma posso usare l'ipotesi induttiva, quindi nei round $<j$ nessun valore $x'\\neq x$ può essere stato scelto. La chiede all'esame :)\n\nSe tutte le learn si perdono al round i, dipende\n- se c'era Q, quel valore prima o poi verrà approvato (segue da `THM`)\n- altrimenti \n\nin ogni caso mi serve un timeout sul learner, in modo che dicano ai `P` \"inizia un nuovo round\".\n\nse modifico il protocollo in modo che $A_i$ non cambi mai il suo voto, si rompe tutto perché potrebbe non esiste mai più una maggioranza...\n\nha senso dividere i proposer dal resto nel paradigma client-server.\n\ntutta sta roba non funziona se un qualsiasi nodo è byzantine\n\nOra, in genere vogliamo approvare una sequenza di valori (non uno solo). Allora runno diverse instance. di fila? Ma no, viva le pipeline.\n\n- la `prepare(i)` diventa una `prepare(p, i)`, in cui `p` è il numero dell'istanza.\n\t- posso ottimizzare mandando una singola prepare che dice `prepare(1-1000, 1)` ovvero preparati al primo round di 1000 istanze;\n\t- stessa cosa per la prima `promise`, che dice di essere pronto per le istanze `1-1000`, round `1`;\n\t\t- possiamo chiamarle `super-prepare` e `super-promise`\n\t- e le `accept`? Il sistema è solo pronto alle istanze, non le ha ancora ricevute. Quando `P` riceve $T_1$, cioè il primo valore da approvare (tipo dal client) è sufficiente mandare una `accept(1, 1, T1)` ovvero `accept(istanza, round, value)`. Se ne ricevo un'altra posso mandare direttamente `accept(2, 1, T2)`? Sì, perché l'importante è l'univocità della coppia `istanza, round`.\n\t- finora abbiamo assunto che i `P` non fossero in competizione. se c'è un altro `P` a ricevere delle T che faccio?\n\t\t- il secondo proposer si prenota le istanze `1001-2000`. sta roba non è ottimale, perché magari approvo le istanze `1, 2, 1001` ma da qui in poi non posso fare commit perché devo aspettare tutte le istanze tra `2` e `1001`.\n\t\t- altra opzione è che tutti mandino le T al primo che ha fatto le `1000` richieste, che Lamport chiamava ***coordinator***.\n\t\t\t- e se muore? se ne fa un altro , \"come il papa\"\n\t\t\t- questo è il cosiddetto leader problem, che è difficile come il consenso. la soluzione è che qui non deve funzionare per davvero, perché alla peggio Paxos funziona ma non ottimizzato\n\t\t\t- se per magia esistesse un perfetto protocollo di scelta del leader avremmo un solo proposer. in quel caso paxos diventa live (l'unico problema in questo senso era infatti che ci fossero tanti P)","x":2160,"y":2068,"width":800,"height":1552},
		{"id":"0f2b4bbb443afbb1","x":2643,"y":3635,"width":250,"height":60,"type":"text","text":"Esonero novembre 4"}
	],
	"edges":[
		{"id":"313f73befec0f227","fromNode":"be570a1920f04449","fromSide":"right","toNode":"23a2f6367ee648bd","toSide":"left"},
		{"id":"ca3b4cbd15b08962","fromNode":"f82b77ddb75926d4","fromSide":"top","toNode":"f9d9980fa75657d5","toSide":"bottom"},
		{"id":"9a87cb3a68c4a99b","fromNode":"f82b77ddb75926d4","fromSide":"bottom","toNode":"ffd7c9469255f9c8","toSide":"top"},
		{"id":"867ed467e77ccc6f","fromNode":"f9d9980fa75657d5","fromSide":"top","toNode":"3458d02a2952e9c7","toSide":"bottom"},
		{"id":"4fdd279d6063830f","fromNode":"ffd7c9469255f9c8","fromSide":"bottom","toNode":"604f69980144ca26","toSide":"top","label":"Cooperative Protocol\nfor\nFail-Stop Processes"},
		{"id":"6fa98181663b2492","fromNode":"f82b77ddb75926d4","fromSide":"left","toNode":"fbc1ef28f3589012","toSide":"right"},
		{"id":"a3020fc44a176c3a","fromNode":"f9d9980fa75657d5","fromSide":"left","toNode":"e0ba5ac0341a437d","toSide":"right"},
		{"id":"c8cb8a307762db2a","fromNode":"4fd496a7b455c76c","fromSide":"top","toNode":"e0ba5ac0341a437d","toSide":"bottom"},
		{"id":"47e11942324f4e4b","fromNode":"3458d02a2952e9c7","fromSide":"right","toNode":"db4411d94cf78039","toSide":"left"},
		{"id":"8176ca8baa3df1d8","fromNode":"db4411d94cf78039","fromSide":"right","toNode":"61752071951b1015","toSide":"left"},
		{"id":"021050bd6bda60c5","fromNode":"db4411d94cf78039","fromSide":"top","toNode":"6dcdd47739ab639e","toSide":"bottom"},
		{"id":"1ebd8a2d6587351a","fromNode":"61752071951b1015","fromSide":"top","toNode":"0b9a16cefb33b26c","toSide":"bottom"},
		{"id":"0d58e3de10db11ce","fromNode":"61752071951b1015","fromSide":"bottom","toNode":"ab73850207f6a30a","toSide":"top"},
		{"id":"8ef0091e96c8a82c","fromNode":"f9d9980fa75657d5","fromSide":"right","toNode":"0aa59bf09c992c32","toSide":"left"},
		{"id":"7f44ba9628882881","fromNode":"61752071951b1015","fromSide":"right","toNode":"0aa59bf09c992c32","toSide":"top"},
		{"id":"0638f572c4714020","fromNode":"0aa59bf09c992c32","fromSide":"bottom","toNode":"9db64449abbcd63a","toSide":"top"},
		{"id":"9ee451fa3547164c","fromNode":"ffd7c9469255f9c8","fromSide":"right","toNode":"48ebe9989920f9da","toSide":"left"},
		{"id":"46ced27253524808","fromNode":"f82b77ddb75926d4","fromSide":"right","toNode":"7e4b0290f28c62f1","toSide":"left"},
		{"id":"289181e565bb286c","fromNode":"ffd7c9469255f9c8","fromSide":"left","toNode":"c2b4396b3b84e4de","toSide":"right"},
		{"id":"32f109e47c0d8cff","fromNode":"ffd7c9469255f9c8","fromSide":"bottom","toNode":"93212aed82c21df9","toSide":"top","label":"Cooperative Protocol\nfor\nFail-Stop Processes"},
		{"id":"4fdde0886f2be644","fromNode":"ffd7c9469255f9c8","fromSide":"bottom","toNode":"d582dc2a1cfd5670","toSide":"top","label":"Recovery Protocol\nfor\nFail-Recovery Processes"},
		{"id":"0c581301ff9df275","fromNode":"93212aed82c21df9","fromSide":"right","toNode":"d582dc2a1cfd5670","toSide":"left"}
	]
}