{
	"nodes":[
		{"id":"2db738a55dcc2bec","type":"text","text":"# Teoria delle code L 1\n\nquanto ci mette questa coda?\n\nanalisi stocastica per prevedere le cose: \n\n- tempo - i delay di reti\n\t- quantità di dati (throughput coerente)\n\t- jitter (varianza di delay)\n\t- parallelismo? (ricollega throughput per servire più utenti alla volta)\n- spazio ed energia ignorati\n\nresponse time R intende tempo di processamento (coda inclusa). Se la coda è vuota questo coincide con il service time (S=$1\\over v$ dove v è la velocità della cpu). Il waiting time è $T_w$. $R = S+T_w$\n\nv non è il throughput X, perché se non arrivano processi con v non faccio nulla. $\\lambda$ è il rate di arrivo di pacchetti processati a velocità $u$. X è il minimo tra i due (no, perché è sempre medio!! posso parlarne solo in termini di valore atteso!!), $u$ è il massimo di X.\n\nanche se $\\lambda < \\mu$ posso vedere roba in coda, perché $\\lambda$ è sempre una media (quindi posso solo dire che la coda può riempirsi al più con un numero finito di elementi?). Per questo con $\\lambda = \\mu$ la coda si riempie. Indefinitamente? Ci sono N i processi in coda. N(t) = Arrival(t)-Departure(t) $\\geq$ $(\\lambda - \\mu)t$, la disuguaglianza è perché $\\mu t$ è il massimo numero di job uscenti, non quello effettivo. devo sostituire $\\mu$ col throughput, che sarebbe il departed effettivo (nel qual caso ho l'uguaglianza?). la disuguaglianza me la tengo perché è sempre vera. N è sempre una media.\n\n- $\\lambda - X \\geq 0$ ???\n\nstability condition: $\\lambda < \\mu$. Ma se sono uguali? ci sono fluttuazioni, perché queste sono solo medie, quindi torniamo a quello che dicevamo prima. Questo vale per un buffer infinito.\n\nE se è finito? se $\\lambda < \\mu$, X = $\\lambda (1-p_d)$, dove p_d è la probabilità di essere discarded.\n\nSe con coda infinita raddoppio $\\lambda$ quanto deve essere $\\mu$ per mantenere R costante? Sicuro deve essere almeno $2\\lambda$, ma $2\\mu$ è troppo (in questo caso R dimezza!). Quindi è compreso tra i due. Il che dipende dal ritardo in coda immagino, perché R=S+T.\n\nse metto due sistemi queue+cpu in parallelo con probabilità $1\\over2$. Le due CPU hanno $\\mu=1/3$. Che succede se sostituisco uno dei due server con uno con $\\mu$ doppia? Velocizzo tutto? Non è così facile. Se un processore è molto veloce lancia più velocemente processi nella coda dell'altro, rallentando il sistema. il bilanciamento è più importante di aumentare le specifiche a caso. NO!! se ti calcoli R resta identico!! Se ho un parallelo, R è determinato dal più lento.\n\n\np/1-p = mu1/mu2 = 2/3\\*3=2=p/1-p so 2p = 1-p so 3p = 1 cioè se metto delle probabilità proporzionali alle due mu velocizzo? Sì, cioè faccio load balancing. ma questo richiede un router intelligente. E allora? eh allora devo migliorare entrambi i server in parallelo, anche di molto poco.\n\nse ho un solo server con $\\mu = 4 j/s$ oppure 4 server in parallelo con $\\mu = 1 j/s$ ? ... dipende, da quanto sono lunghi i job, se sono tutti uguali, ecc. Il parallelismo è più fair, se ho poco lavoro è meglio il singolo server veloce. Non ce n'è uno migliore.","x":-280,"y":-80,"width":740,"height":1294},
		{"id":"ae13a0ed0a553ebc","type":"text","text":"# L 2\n\nper un buffer infinito in cui $\\mu>\\lambda$, $R=\\frac{1}{\\mu - \\lambda}$.\n\nse ho $X=\\mu$ la coda si riempie indefinitamente. e se faccio una gerarchia in cui dopo il decimo posto in coda reindirizzo su un altro CPU è meglio di un parallelo? direi di no. oppure di fatto è un parallelo con probabilità proporzionali alla velocità...\n\nR è anche il service time per il numero medio di processi in coda + 1 (nella CPU)\n\nnell'esempio del parallelo asimmetrico, X per il server veloce resta uguale a quello lento. Dipende dallo split del router? sì ma non solo. i bottleneck sono dati dalle code. se c'è poco carico non vedo le code, quindi non vedo bottleneck.\n\ntutto questo era con un cavo che mandava la roba in loop. ora non c'è. in questo caso anche migliorare un solo server migliora R e X, perché non c'è \"feedback\".\n\nPoniamo di avere N server in parallelo, tutti con stessa $\\mu$. Due modelli:\n\n- una sola coda comune\n\t- non hai coda se metti $\\lambda$ server in parallelo. in questo caso hai R = S, ma non puoi fare R < S con nessun numero di server paralleli \n- ognuno con una coda\n\ndiverse scelte portano diversi benefici, ovviamente\n\n\nsegue ex 2.1 del libro (quale? perché non ce l'ho?)\n\n","x":660,"y":-80,"width":680,"height":1294},
		{"id":"602a250412f9f8db","type":"text","text":"# L3\n\njobs/sec è una sorta di misura a livello applicazione, posso usare un numero di cicli medi per job per la conversione (e.g. $5\\cdot 10^3$ cicli/job se ho CPU a $15\\cdot10^4$ cicli/sec avrò $\\mu \\sim 30 job/s$).\n\ne se la coda è finita (lunga M)? anche se $\\lambda<\\mu$ non posso dire che $X=\\lambda$, ma solo che $X<\\lambda$ (sempre a causa delle fluttuazioni, la coda si può riempire fino a scartare qualcosa):$$p_{full}=p(N=M+1) \\neq 0$$\n\"I don't have rum for other jobs\". quindi posso definire un ***drop rate*** come$$\\lambda_{drop} = \\lambda\\cdot p_{full}$$\nquindi chiaramente $X = \\lambda-\\lambda_{drop}$. Nota che non è importante che $\\lambda$ sia minore di $\\mu$. Se è maggiore, semplicemente, il drop rate vola alle stelle.\n\n\"I'd like to introduce UTILIZATION.\" Due casi:\n\n- un solo server - data dal ratio tra busy e observation time (i.e. osservo per 100 secondi, in che percentuale di questo tempo il server è busy?), si indica con $\\rho$. misura empirica $\\in [0,1]$ $\\sum{B_i}\\over\\tau$\n\t- altrimenti posso definirla come probabilità che il server sia busy (che per grandi numeri è equivalente, ma anche per definizione frequentista di probabilità)\n- m server - $\\rho = \\overline{N}_{\\text{busy servers}}/m \\in [0,1]$. perché è equivalente? è mezzo ovvio, nel senso che è come se stessi sostituendo il parallelo con un solo server assumendo che la media al numeratore faccia statistica dando la probabilità (m è la normalizzazione).\n\t- formalmente, se applico questa definizione al caso con un solo server m = 1 e $N_{BS}$ è $1\\cdot p(server busy) + 0\\cdot p(server not busy)$. così per m=1 trovo il caso a un solo server. e viceversa? invece di osservare da 0 a T osservo da 0 a mT e metto i server in serie, poi applico la definizione ed esce la stessa cosa. (intuitivamente, il fatto che N medio in un dato istante fa una roba equivalente alla definizione sul tempo è una specie di trasformata di fourier...?)\n\nvorrei massimizzare l'utilization (senza raggiungere 1 forse, sembra un rischio). sia C jobs completati entro $\\tau$, moltiplico e divido ottenendo $\\rho=$$B\\over C$$C\\over\\tau$ dove $B\\over C$ è il time di esecuzione medio per job (anche detto service time..., perché B è CPU time). Dunque $$\\rho = S\\cdot X$$che prende il nome di ***utilization law***. Nota che vale per il singolo server, non per N server.$$X = E[X\\,|\\,\\text{System is busy}]\\cdot P(\\text{System is busy}) + E[X\\,|\\,\\text{System is idle}]\\cdot P(\\text{System is idle})$$\nil secondo termine è zero per definizione, quindi $$X= E[X\\,|\\,\\text{System is busy}]\\cdot P(\\text{System is busy})= \\mu\\rho = \\frac{\\rho}{S}$$\ncioè $\\rho = SX$, che è quello che ho trovato prima, ma stavolta con la probabilità invece che con osservazioni empiriche. Yeee.\n\n","x":1600,"y":-80,"width":760,"height":1294},
		{"id":"a73718e36cac73fd","type":"text","text":"# L4 (Probabilità)\n\nCose importanti sono solo le seguenti:\n\n- $E[XY] = E[X]E[Y]$ ***solo se gli eventi sono indipendenti***;\n- $E[X+Y] = E[X]+E[Y]$ ***sempre***.\n\nex 3.14","x":2540,"y":-80,"width":760,"height":880},
		{"id":"a2cb1b62ac8e14a7","x":3420,"y":-80,"width":660,"height":880,"type":"text","text":"# L5\n\nmettiamo in relazione popolazione $E[N]$ del sistema, X e tempo di permanenza all'interno del sistema $E[T]$ (Little Law):$$E[T]={E[N]\\over X}$$questo indipendentemente da cosa sia il sistema: è una black box. una condizione è che il sistema sia ergodico.\n\nper il semplice sistema con una coda/processore l'unica condizione è quella di stabilità (non serve coda FCFS)"}
	],
	"edges":[]
}