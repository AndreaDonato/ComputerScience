{
	"nodes":[
		{"id":"7cb40482ce744e2d","type":"text","text":"# Cryptography\n\nLa crittografia moderna si propone di risolvere principalmente due problemi:\n\n- ***Secrecy*** - Solo chi invia e chi riceve deve essere in grado di accedere al contenuto informativo del messaggio (i.e. ***confidentiality***). Questo di norma si risolve con la ***crittografia simmetrica***;\n- ***Authenticity/Integrity*** - Chi riceve deve essere sicuro che il messaggio gli giunga inalterato per come è stato scritto dal mittente atteso. Questo si risolve tipicamente con le ***funzioni di hash*** e con la ***crittografia asimmetrica***, e include due scenari:\n\t- Eve non deve essere in grado di modificare un messaggio di Alice per poi inoltrarlo a Bob facendogli credere che questo sia originale (***integrity***);\n\t- Eve non deve essere in grado di inviare un proprio messaggio facendo credere a Bob che questo provenga da Alice (***Authenticity***).\n\nEssendo la crittografia una scienza, si basa su proprietà statistico-matematiche dimostrabili.\nI risultati più generali non fanno alcuna assunzione sulle capacità computazionali di chi attacca (***unconditional proofs***), ma producono schemi e algoritmi inefficienti dal punto di vista pratico.\nPer ottenere risultati applicabili è spesso necessario aggiungere delle ***ipotesi ragionevoli*** su ciò che un attaccante è in grado di fare (***conditional proofs***). Ad esempio, un risultato generale vale anche se l'attaccante dispone di un computer quantistico fault-tolerant che può girare per miliardi di anni, ma dal punto di vista pratico è inutile avere uno schema crittografico così forte.\n\nA livello di glossario, ragioneremo sempre con i seguenti elementi di base:\n\n- ***Crittografia*** - $m$ è un ***messaggio in chiaro*** scelto tra tutti i possibili messaggi $\\calM$. Chiamiamo $M$ la generica ***distribuzione*** secondo la quale estraggo $m$ dal set $\\calM$. Stessa cosa con la ***chiave*** $k$ tra tutte le chiavi $\\calK$ estratta con distribuzione $K$ e con il ***ciphertext*** $c$ tra tutti i $\\calC$ ottenuti con distribuzione $C$;\n- ***MACs*** - Al messaggio $m$ viene affiancato un ***tag*** $\\t$ (i.e. un ***hash*** del messaggio $m$), con l'idea che essendo difficile risalire alla chiave usata dalla funzione di Tag per Eve sia difficile associare il giusto tag $\\t'$ al messaggio $m'$ (che può essere sia una modifica di $m$ sia un nuovo messaggio scritto da Eve). Questa difficoltà è detta ***unforgeability***.","x":-220,"y":260,"width":759,"height":802,"color":"6"},
		{"id":"1525b2c7f8eec6d1","type":"text","text":"# Cryptography\n\n- (Un)conditional proofs - algoritmi (in)efficienti facendo (zero) assunzioni\n\t- aggiungere ipotesi ragionevoli\n- Schema tipico del corso: Teorema dimostrazione del tipo\n\t- THM - Cryptosystem X is \"secure\" if factoring is $\\NPH$;\n\t- PROOF - Assume X is note secure, i.e. exists efficient machine B che \"rompe\" X, trova la contraddizione.\n\n","x":-220,"y":-500,"width":759,"height":320},
		{"id":"3bb988d1bfcbf2a4","type":"text","text":"# Distribuzioni\n \n Seguono le distribuzioni che devi usare per calcolare la probabilità che\n \n- si verifichi uno tra due eventi mutuamente esclusivi, rispettivamente con probabilità $p$ (successo) e $q=1-p$ (insuccesso) - ***Distribuzione di Bernoulli***;\n- si verifichino $k$ successi su una sequenza di $n$ eventi di Bernoulli indipendenti - ***Distribuzione Binomiale*** avente espressione$$P(X=k)=\\binom{n}{k}p^kq^{n-k}$$\n\t- Quante prove $k$ devo fare prima di ottenere il primo successo? Sono date dalla ***Distribuzione Geometrica*** avente espressione$$P(X=k) = pq^{k-1}$$\n- si verifichino $k$ eventi (rari) in un lungo (formalmente $t\\to\\infty$) periodo di osservazione - ***Distribuzione di Poisson***, cioè$$P(X=k)={\\lambda^ke^{-\\lambda}\\over k!}$$\n\t- Quanto tempo devo aspettare tra due eventi di Poisson? Me lo dice la ***Distribuzione Esponenziale***$$pdf(x)=\\lambda e^{-\\lambda x}$$\n\n$\\lambda$ è in generale un rate di arrivo, mentre nel caso continuo bisogna sostituire la variabile discreta $k$ con una continua $x$. Questo non mi restituisce propriamente una probabilità di un evento, ma la ***probability density function*** (***pdf***), nel senso che su un dominio continuo il singolo valore ha una probabilità di verificarsi $=0$. La pdf ha quindi senso solo sotto integrale in un certo intervallo. La funzione integrale della pdf è detta ***cumulativa***.","x":-3660,"y":-3160,"width":759,"height":868,"color":"4"},
		{"id":"bdd4244555c9c774","type":"text","text":"# Richiami di Probabilità\n\nSe $\\Omega$ è lo spazio di tutti i possibili eventi ed $E\\subseteq \\Omega$ è un singolo evento, la probabilità di $E$ è in generale definita come il rapporto tra la misura di $E$ e la misura di $\\Omega$. Abbiamo quindi che$$P(E\\cup F) = P(E) + P(F) - P(E\\cap F) \\leq P(E) + P(F)$$L'uguaglianza vale solo se $P(E\\cap F)=0$, ovvero se gli eventi $E$ ed $F$ sono ***mutuamente esclusivi***. In modo analogo possiamo scriverlo con gli insiemi, ovvero $E\\cap F = \\emptyset$.\n\n- Dato un set di eventi $\\{E_i\\}$ tali che $E_i\\cap E_j = \\emptyset$, se $\\bigcup_iE_i = F$  allora $\\{E_i\\}$ è una ***partizione*** per $F$ (i.e. $F$ è totalmente ricoperto da eventi disgiunti $E_i$);\n- Eventi mutuamente esclusivi sono ***dipendenti***, in quanto per definizione il verificarsi di un evento influenza il verificarsi dell'altro (i.e. se si verifica $E$ allora $F$ non può verificarsi);\n\t- Viceversa, $E$ ed $F$ sono ***indipendenti*** ($E\\perp F$ ) ***sse*** $P(E\\cap F) = P(E)\\cdot P(F)$.\n\nDefiniamo la ***probabilità condizionata*** come$$P(E|F)={P(E\\cap F)\\over P(F)}$$che significa cercare un elemento $\\in E$ dentro l'insieme $F\\subseteq\\Omega$. In pratica, $F$ diventa il nuovo $\\Omega$ ed $E\\cap F$ diventa il nuovo $E$. Da questa definizione seguono diverse cosette utili.\n\n- ***Teorema della Probabilità Totale*** - Sia $\\{F_i\\}$ una partizione per $\\Omega$. $P(E) = \\sum_i P(E|F_i) \\cdot P(F_i)$.\n\t- Una partizione per $\\Omega$ è $F_1 = F$ ed $F_2= F^C$, dove $F^C=\\{x\\in\\Omega\\,|\\,x\\notin F\\}$ è il ***complemento*** di $F$.\n\t- Consente di trovare una probabilità come somma delle probabilità sui singoli elementi di una partizione di $\\Omega$.\n- ***Teorema di Bayes*** - Consente di invertire la definizione di probabilità condizionata. Essendo $\\cap$ un operatore simmetrico abbiamo che $$P(E\\cap F)=P(F\\cap E) =P(E|F)\\cdot P(F)=P(F|E)\\cdot P(E)$$Da questo segue che$$P(F|E) = {P(E|F)\\cdot P(F)\\over P(E)}={P(E|F)\\cdot P(F)\\over \\sum_i P(E|F_i) \\cdot P(F_i)}$$","x":-4669,"y":-3160,"width":759,"height":868,"color":"4"},
		{"id":"80ff12045395380f","type":"text","text":"# Somme e Prodotti di Variabili Casuali\n\nIndichiamo con $f_X(x)$ la *pdf* associata al processo $X$ scritta nella variabile $x$.\n\nSegue che $f_{XY}(x,y)$ è pdf associata alla ***joint probability***$$P_{XY}(x,y) = P(X=x,\\,\\, Y=y)=\\int_{\\Omega_X}\\int_{\\Omega_Y}f_{XY}(x,y)dxdy$$È possibile risalire alla $f_X$ tramite una ***marginalizzazione***, ovvero integrando su $y$$$f_X(x) = \\int_{\\Omega_Y}f_{XY}(x,y)dy$$e viceversa. Tutto questo serve a dimostrare che l'operatore ***valore atteso***$$E[X] := \\int_{\\Omega_X}xf_X(x)dx$$***commuta sulla somma di variabili, ma in generale non sul prodotto***, ovvero$$\\begin{array}\\\\ E[X+Y] = E[X]+E[Y]\\\\ \\\\ E[XY] \\neq E[X]\\,E[Y]\\leadsto E[XY] \\overset{X\\perp Y}{=} E[X]\\,E[Y]\\end{array}$$***Il valore atteso distribuisce sul prodotto se e solo se $X$ e $Y$ sono indipendenti***.\n\nPiù in generale, la pdf della somma di due variabili indipendenti è data dall'***integrale di convoluzione*** definito come$$f_{Z=X+Y}(z=x+y) = (f_X*f_Y)(z)\\int_{\\mathbb{R}}f_X(x)f_Y(z-x)dx$$mentre la formula per il prodotto esiste ma è inutilmente complicata e non la scrivo.","x":-2651,"y":-3160,"width":759,"height":868,"color":"4"},
		{"id":"0f1dad420fab77a0","type":"text","text":"# Dimostrazione Teorema di Shannon\n\nPer assurdo, prendiamo $M$ uniforme e un qualsiasi $c$ tale che $\\P[C=c]>0$, i.e. posso ottenere $c$ come $\\Enc(k,m)$ per una qualche scelta di $k$ ed $m$, ed assumiamo $|\\calK|<|\\calM|$. Definiamo$$\\calM'=\\bigg\\{\\Dec(k,c)\\,\\text{ t.c. }\\,k\\in\\calK\\bigg\\}$$i.e. l'insieme di tutti i messaggi ricostruibili a partire da una chiave $k\\in\\calK$. È chiaro che $|\\calM'|\\le|\\calK|$: a parità di $c$, non posso ricostruire più messaggi $m'$ di quante sono le chiavi $k$. Segue che $$|\\calM'|\\le|\\calK|<|\\calM| \\quad\\so\\quad |\\calM'|<|\\calM|$$Essendo strettamente minore, deve esistere un messaggio $\\in\\calM$ ma $\\notin\\calM'$. Per tale $m$ abbiamo$$\\P\\big[M=m\\big]={1\\over|\\calM|}$$in quanto $\\in\\calM$, ma anche $$\\P\\big[M=m\\mid C=c\\big]=0$$\nin quanto $\\notin \\calM'$. Essendo queste due $\\P$ diverse non vale la condizione di PS, quindi non esiste un SKE tale che $|\\calK|<|\\calM|$.","x":-4256,"y":-500,"width":759,"height":525,"color":"4"},
		{"id":"a056b827849ac92f","type":"text","text":"# Dimostrazione Lemma di Equivalenza\n\nIndichiamo le definizioni equivalenti come $I$, $II$ e $III$ e dimostriamo che\n\n- $I\\so II$  : Prendi per buona la definizione di PS, usi Bayes e ti ritrovi con$$\\P\\big[M=m\\big]={\\P\\big[M=m,\\,C=c\\big]\\over \\P\\big[C=c\\big]}\\quad\\so\\quad\\P\\big[M=m,\\,C=c\\big]=\\P\\big[M=m\\big]\\cdot\\P\\big[C=c\\big]$$che è la definizione di variabili indipendenti.\n- $II\\so III$ : Fissa la distribuzione $M$ e due elementi $m\\in\\calM$ e $c\\in\\calC$ e considera$$\\P\\aq\\Enc(K,m)=c\\cq=\\P\\aq\\Enc(K,M)=c\\mid M=m\\cq=\\P\\aq C=c\\mid M=m\\cq$$dove ho sostituito $m$ con $M$ + la condizione che $M=m$ e notato che $\\Enc(K,M)=C$. Ma per ipotesi $m$ e $c$ sono indipendenti, quindi quella roba è solo $\\P\\aq C=c\\cq$. Facendo lo stesso ragionamento per un altro messaggio $m'$ ottengo la stessa cosa, i.e. sto dicendo che$$\\forall c\\in\\calC \\,\\,\\P[\\Enc(K,m)=c]=\\P[\\Enc(K,m')=c]$$che è proprio la condizione $III$.\n- $III\\so I$ : Manipoliamo con la probabilità totale e con Bayes$$\\prob{C=c}=\\sum_{m'}\\prob{C=c,\\,M=m'}=\\sum_{m'}\\prob{C=c\\mid M=m'}\\cdot\\prob{M=m'}$$Stesso gioco di prima, ma al contrario. $\\prob{C=c}$ diventa la probabilità di $\\Enc(K,M)=c$, che riassorbendo la condizione $M=m'$ diventa $\\Enc(K,m')=c$, che grazie alla condizione $III$ da cui partiamo possiamo rendere un $\\Enc(K,m)=c$. A questo punto essendo indipendente da $m'$ possiamo portarlo fuori dalla somma, ritrovandoci con$$\\prob{C=c} = \\prob{\\Enc(K,m)=c}\\sum_{m'}\\prob{M=m'} = \\prob{(K,M)=c\\mid M=m}=\\prob{C=c\\mid M=m}$$Ma a noi serve al contrario, quindi usiamo un Bayes al volo:$$\\prob{M=m\\mid C=c}\\cdot\\prob{C=c} = \\prob{C=c\\mid M=m}\\cdot\\prob{M=m}$$da cui otteniamo in definitiva$$\\prob{M=m}={\\prob{M=m\\mid C=c}\\cdot\\cancel{\\prob{C=c}}\\over\\cancel{\\prob{C=c\\mid M=m}}}\\stackrel{\\text{per il risultato di prima}}{=}\\prob{M=m\\mid C=c}$$","x":-3247,"y":-900,"width":759,"height":925,"color":"4"},
		{"id":"4294c40eec01cd5f","type":"text","text":"# Encryption and Decryption\n\nUno schema crittografico a chiave segreta (***Secret Key Encryption***, ***SKE***) è definito da una coppia di funzioni di ***encryption*** e ***decryption***, i.e.$$\\Pi = (\\text{Enc}, \\text{Dec)}\\quad\\text{t.c.}\\quad C=\\Enc(k, M)$$\nQuesta cosa vale sia per le distribuzioni $(C, M)$ che per gli elementi $(c, m)$ estratti secondo esse. In modo analogo si definisce la funzione inversa $\\Dec$ (nota che in un ***cifrario simmetrico*** anche $\\Dec$ usa $k$, mentre in uno ***asimmetrico*** usa un $k'\\ne k$) . Dal momento che è più facile tenere segreta una singola chiave rispetto ad un intero schema di cifratura, $\\Pi$ di norma è ***pubblico***. L'idea è che anche conoscendo $\\Pi$ sia ***troppo difficile risalire alla chiave usata*** per la comunicazione sicura.\n\nIl pilastro che definisce la crittografia come scienza lo mette ***Shannon*** nel 1950: una SKE è ***sicura*** se rispetta la condizione matematica di ***Perfect Secrecy*** (***PS***)$$\\P[M=m] = \\P[M=m\\mid C=c]$$i.e. conoscere il ciphertext $c$ non modifica in alcun modo la probabilità a priori del messaggio $m$. In altre parole, una SKE è sicura se guardare $c$ non permette di inferire nulla su $m$, tolto ovviamente quello che già sapevamo già prima di guardare $c$. Possiamo definire la PS in modo equivalente:$$\\text{PS}\\iff M\\text{ and }C\\text{ independent}\\iff \\forall m,m'\\in\\calM, \\,\\,\\forall c\\in\\calC \\,\\,\\P[\\Enc(K,m)=c]=\\P[\\Enc(K,m')=c]$$\nNella terza formulazione $\\P$ è una distribuzione su $K$ e $K$ è una distribuzione su $\\calK$. Queste definizioni sono generali: non c'è nessuna ***assunzione*** su un eventuale attaccante, pertanto la chiamiamo ***unconditional***. Le proposizioni di questo tipo sono appunto più generali, ma di norma sono difficili da realizzare nella pratica. La PS è talmente difficile da realizzare che il ***teorema di Shannon*** ci dice che data una qualsiasi SKE $\\Pi$ si ha che$$|\\calK|\\ge|\\calM|$$\nQuesto disastroso risultato ci dice che la SKE minimale ha $|\\calK| = |\\calM|$. Si chiama ***One Time Pad*** (***OTP***), e nonostante sia mostruosamente inefficiente viene ancora oggi utilizzato in alcune situazioni. Tutti gli altri SKE che utilizziamo hanno $|\\calK| < |\\calM|$, e pertanto non sono PS: il ciphertext $c$ rivelerà ***sempre*** qualcosa sul messaggio in chiaro $m$.","x":-3247,"y":260,"width":759,"height":802,"color":"4"},
		{"id":"5c76b469ef30b644","type":"text","text":"# One Time Pad (OTP) & Two-Times Security\n\nÈ un esempio di cifrario PS minimale (ma comunque poco pratico), in cui $|\\calK| = |\\calM|$. Poniamo$$\\calK=\\calM=\\calC=\\{0,1\\}^n$$\ne stabiliamo come schema $\\Pi$ $$\\Enc(k, m)=k\\oplus m \\quad\\so\\quad \\Dec(k,c)=k\\oplus c$$\ndove $\\oplus$ è l'operatore di $\\text{bitwise XOR}$. È facile verificare la *correctness* di questo SKE, visto che$$\\Dec(\\Enc(k,m))=\\Dec(k\\oplus m) = k\\oplus(k\\oplus m) = m \\quad(\\text{since }k\\oplus k = 1)$$\nE la PS? Proviamo a calcolare\n$$\\P[\\Enc(k,m)=c] = \\P[k\\oplus m = c] = \\P[k=m\\oplus c]=2^{-n}$$\nQuesto perché $m\\oplus c$ è una delle $2^n$ stringhe $\\{0,1\\}^n$, in modo equiprobabile. Ma essendo una costante che non dipende da $m$ otteniamo lo stesso risultato anche usando $m'$, per cui$$\\prob{\\Enc(k,m)=c} = \\prob{\\Enc(k,m)=c}\\quad \\forall m,m'\\in\\calM, \\,\\,\\forall c\\in\\calC$$che è esattamente la definizione equivalente $III$ di PS.\n\nSiamo contenti? ... meh. Questo perché ***la PS vale su un solo messaggio***, e non dice nulla sulla situazione in cui ne mando diversi. Ad esempio, OTP ha il problemino che se l'attaccante intercetta due diversi ciphertexts $c=k\\oplus m$ e $c'=k\\oplus m'$ può tranquillamente calcolare$$c\\oplus c' = k\\oplus m\\oplus k\\oplus m'= m\\oplus m'$$i.e. se mai dovessi risalire anche ad un solo messaggio $m$ potrei decrittare a cascata tutti gli altri messaggi cifrati con la stessa chiave $k$. Questo ci dice che ***OTP non è Two-Times Secure***.\n\nSi può dimostrare che ***in generale non esiste un SKE Two-Times Secure***. Come conseguenza, ogni SKE è PS solo se viene usato una sola volta, i.e. se la chiave cambia ad ogni messaggio.","x":-4256,"y":260,"width":759,"height":802,"color":"4"},
		{"id":"6bcda7da0b8d4ae6","type":"text","text":"# Message Authentication Codes (MACs)\n\nQui ignoriamo la secrecy e ci interessiamo solo all'***integrità*** del messaggio. Lo schema è semplice:\n\n- Alice usa una funzione $\\text{Tag}(k,m)$ per ottenere un ***tag*** $\\t$ associato al messaggio $m$. Tale funzione mappa $(k, m)$ in uno spazio a dimensione molto minore rispetto a quella di $m$, ed è (anche per questo) difficile da invertire (e.g. se $m$ è lungo `2048 byte` $\\t$ può essere lungo `256 bit`);\n- Alice invia in chiaro la coppia $(m, \\t)$;\n- Bob riceve $(m,\\t)$, prende $m$ ignorando $\\t$ e prova a fare la stessa cosa che ha fatto Alice: applica $\\text{Tag}(k,m)$ e ottiene $\\t'$ (stiamo assumendo che entrambi concordino sulla chiave $k$). A questo punto è facile: se $\\t=\\t'$ il messaggio è autentico, altrimenti è meglio buttarlo.\n\nEve in tutto questo entra in gioco nel secondo punto. Può anche leggere il messaggio, ma se prova a modificarlo ottenendo un $m'$ cambierà anche il tag. Già, ma in che modo? Per calcolarlo dovrebbe conoscere $k$, ma (hopefully) non è così. Quindi la cosa migliore che può fare è ***indovinare*** tra tutti i possibili tag. Se $|\\t|=$`256 bit` la probabilità di associare ad $m'$ il giusto $\\t'$ sono $2^{-256}$. Improbabile, ma comunque possibile (la probabilità non è *esattamente* zero).\n\nÈ per questo motivo che anche se trovassimo una funzione di $\\text{Tag}$ perfetta (***Perfect MAC***) ci sarebbe comunque una probabilità $1/|\\calT|$ che l'attaccante indovini il giusto $\\t'$. Più in generale diciamo che una funzione $\\text{Tag}$ è ***One-Time*** $\\e$***-Statistically Secure*** ($\\e\\text{-SS}$) se$$\\forall m,m'\\in\\calM\\,\\text{t.c. }m\\ne m',\\,\\,\\forall\\t,\\t'\\in\\calT\\qquad\\Prob{\\text{Tag}\\at K,m'\\ct=\\t'\\Bigm| \\text{Tag}\\at K,m\\ct=\\t}\\le \\e$$i.e. data l'osservazione di una coppia $(m,\\t)$, la probabilità di indovinare il tag $\\t'$ per un messaggio $m'\\ne m$ è al più $\\e$. Osservare $\\t$ mi sta quantomeno comunicando la lunghezza $|\\t|$ e l'alfabeto $\\calT$, che sono gli stessi per $\\t'$. Se sono bravo, queste sono le uniche informazioni che fornisco all'attaccante, ed $\\e=1/|\\calT|$ (al Perfect MAC corrisponde il minimo di $\\e$), altrimenti sarà maggiore.\n\nEve può quindi riuscire ad indovinare il tag $\\t'$ per il messaggio $m'$ con probabilità $\\ne0$. Se ci riesce, realizza una ***forgery*** (i.e. ha \"forgiato\" un tag come fosse Alice, e può quindi impersonarla). Uno schema che soddisfa la condizione di One-Time $\\e\\text{-SS}$ ha la proprietà di ***One-Time Unforgeability***.","x":2807,"y":260,"width":759,"height":802,"color":"4"},
		{"id":"4466415b5eb73223","type":"text","text":"# Pairwise Independence\n\nAbbiamo definito la condizione di One-Time $\\e\\text{-SS}$, ma come costruiamo una $\\text{Tag}$ che la soddisfi? \n\nPartiamo dal presupposto che questo è un corso di matematica e che non sempre è facile trovare una logica lineare ai risultati che escono fuori. Quindi ci mettiamo l'anima in pace e definiamo anzitutto una famiglia di funzioni dalla quale prendere la magica $\\text{Tag}$ che realizza il Perfect MAC:\n$$\\calH=\\{h_k: \\calM\\to\\calT\\}_{k\\in\\calK}$$\nSe definiamo $p$ come numero primo, un esempio di siffatta funzione (parametrica) può essere$$h_{a,b}(m)=\\AQ am+b\\CQ_{\\text{ mod }p}\\quad\\text{dove }k=(a,b) \\in\\calK=\\Z^2_p \\quad\\text{e}\\quad\\calM=\\cal\\calT=\\Z_p$$Il senso è il seguente: la chiave è una coppia di interi $\\in[0,\\,p-1]$. Detto questo,\n\n- A parità di messaggio $m$, applicare $h_{a,b}$ e $h_{a',b'}$ restituisce $\\t$ e $\\t'\\ne\\t$ fintantoché $(a,b)\\ne(a',b')$;\n- A parità di $h_{a,b}$, l'azione sui messaggi $m$ ed $m'$ restituisce $\\t$ e $\\t'\\ne\\t$ fintantoché $m\\ne m'$.\n\nIn genere quello che cambia non è la chiave, ma il messaggio. Prendiamo quindi la seconda proprietà. Sarebbe bello produrre $\\t$ e $\\t'$ ***statisticamente scorrelati***: in questo modo, osservarli entrambi non permetterebbe di inferire alcuna informazione in più su $K$ rispetto allo scenario in cui se ne osservi una sola delle due. Questo si formalizza nella definizione ***Pairwise Independence***:$$\\forall\\, m,m'\\in\\calM : m\\ne m' \\quad\\so\\quad \\AT h_k(K,m), h_k(K,m')\\CT \\text{ è uniforme su }\\calT^2=\\calT\\times\\calT\\text{ se }K\\text{ è uniforme su }\\calK$$\nBello, ma quindi? C'è un teoremino che ci garantisce che se scegliamo una funzione di $\\Tag=h_k$ da una famiglia di funzioni Pairwise Independent allora $\\Tag$ è un ***Perfect MAC***, i.e.$$\\Tag(k, m) = h_k(m)\\quad\\so\\quad\\Tag\\text{ is One-Time }{1\\over|\\calT|}\\text{-Statistically Secure}$$Anche l'esempietto sopra in $\\Z_p$ è una famiglia Pairwise Independent. Dimostrazioni di queste due cose nei riquadri intorno.","x":3816,"y":260,"width":759,"height":798,"color":"4"},
		{"id":"763bb7889db17350","type":"text","text":"# Mini-Glossario\n\n- ***Correctness/Completeness*** - Se applico prima $\\Dec\\aq\\Enc(k,m)\\cq$ deve fare $m$, la stessa funzione di tag applicata allo stesso input deve restituire lo stesso hash, ecc...","x":780,"y":340,"width":520,"height":198},
		{"id":"3b05a71ed7dade2e","type":"text","text":"# coso viola riassunto critto","x":-1803,"y":590,"width":423,"height":150},
		{"id":"bd42c5684f9511b3","type":"text","text":"# coso viola riassunto MAC","x":1600,"y":584,"width":423,"height":150},
		{"id":"b332de5fd82b6631","type":"text","text":"# \"Non-Perfect\" MAC & $t$-Time Security\n\nAnzitutto notiamo che è facile trovare un esempio in cui uno schema One-Time $\\e\\text{-SS}$ rivela più del quantitativo minimale di informazione. Se ad esempio vediamo che il tag è `hello`, lo spazio di ricerca potrebbe non essere \"`tutte le possibili sequenze di 5 caratteri ASCII`\" (il che corrisponderebbe a $\\e\\sim O(2^{-40})$), ma più probabilmente \"`tutte le possibili parole inglesi di 5 lettere`\". Inutile dire che questa osservazione, se corretta, riduce molto lo spazio di ricerca.\n\nInoltre, in uno schema One-Time $\\e\\text{-SS}$ osservare una coppia $(m, \\t)$ mi mette nelle condizioni di avere al più una probabilità $\\e$ di indovinare il tag $\\t'$ per il messaggio $m'$. Ma se ne raccolgo diverse?\n\nCome la ***Perfect Secrecy***, anche la One-Time $\\e\\text{-SS}$ è definita su una sola coppia di messaggi. Possiamo fare di meglio? In un certo senso sì. Ci sono delle definizioni più vaste che contemplano l'osservazione di $t$ coppie $(m,\\t)_i$ e in cui la probabilità resta ancora$$\\forall(m,\\t)_{i=1,..., t}, \\forall m'\\in \\calM/\\{m_i\\}, \\forall\\t'\\in\\calT\\quad\\Prob{\\text{Tag}(K, m')=\\t' \\Bigm| \\text{Tag}(K, m_i)=\\t\\quad\\forall i\\in[1, t]}\\le\\e$$\n(***$t$-Time $\\e$-Statistical Security***, nota che l'avversario è libero di fare ciò che vuole, anche richiedere *ad hoc* di vedere proprio quelle coppie, i.e. è un risultato ***unconditional***), e si può ricavare che ogni funzione di Tag $t\\text{-Time }2^{-\\l}\\text{-SS}$ deve avere una chiave lunga almeno $(t+1)\\cdot\\l$, il che in pratica significa che per miglioramenti lineari su $\\e$ devo ***allungare esponenzialmente la chiave***. Scomodo.\n\nMa questo era giusto per completezza, non credo lo chieda.","x":2807,"y":-900,"width":759,"height":925,"color":"4"},
		{"id":"91f376c3ee9ec3f9","type":"text","text":"# note\n\ndifferenza tra unforgeability statistica e computazionale","x":6400,"y":661,"width":584,"height":268},
		{"id":"4cf6a821f24185ae","type":"text","text":"# SPACING","x":4575,"y":25,"width":250,"height":235,"color":"5"},
		{"id":"a82c1a10ade4a3e5","type":"text","text":"### Message Authentication Codes\n\n\nTutto il punto del corso è capire cosa voglio garantire con questa funzione di tag, come la costruisco e sotto quali condizioni è sicura.\n\n\"correctness is implicit as long as Tag function is deterministic\". Non ha senso farla non deterministica, no? \"a volte ha senso\".\n\nunforgeability = should be hard to forge valid tag $\\t'$ on message $m'$ (se non conosco la chiave è difficile guessare il giusto hash, il che mi fa pensare che prima ho capito male qualcosa). E se conosco già un'associazione messaggio/tag? E se ne raccolgo un tot? Posso risalire alla chiave?\nma poi ovviamente ad ogni messaggio aggiungo un timestamp in modo da non fare mai due messaggi uguali, quindi per forza devo trovare la chiave, è difficile fare tipo \"prendo un pezzetto da una parte e uno dall'altra, tipo che il messaggio abc ha hash 65 e afg ha 69 e associo 6 ad a (tipo)\". \"AES non è un encryption scheme, ha proprietà diverse, poi certo, lo puoi usare per la secirity o per l'hashing\". ok unforgeability ti fa essere sicuro che eve non può scrivere messaggi e asociare un valid tag convincendo bob che il mess venga da alice.\n\ndef (statistical secure mac) - say $\\Pi =$ tag (perché di nuovo pi???). pi ha una $\\e$-statistical security (unforgeability) se $$\\forall\\,m,m'\\text{ t.c. }m\\ne m'\\quad \\forall\\,\\t,\\t'\\in\\calT\\so \\P[tag(K,m')=\\t'\\mid tag(K,m)]=\\t\\le\\e$$\nex: impossible to get \\e=0 (credo perché con probabilità $1\\over|\\calK|$ posso comunque guessare la chiave, lui dice meglio che con probabilità $1\\over|\\calT|$ posso guessare il giusto \\t che forgia). nota che però qua non c'è l'adversary, quindi ancora unconditional. questa def parla solo di un messaggio: se vedo un'associazione posso guessare un'altra giusta associazione con prob al più \\e. Se già i messaggi sono due non dice più nulla. \"alice, puoi taggare in sicurezza un solo messaggio, se ne tagghi due io me ne tiro fuori\". è una ONE-TIME definition. Si può anche scrivere la TWO-TIME definition ma non credo ci interessi. ok forging is ab\n\n- the notion is achievable\n- it's unefficient - thm: any t-time $\\l$-statistically secure tag has a key of length (i.e. al tempo t, una funzione che si propone di essere $\\l$-SS deve avere una chiave lunga almeno $(t+1)\\l$)\n\ndef pairwise independence\n\n\"sha has no key, it is public. pw independent hanno una chiave\"\nany family of hash functions \"prendo una chiave a caso, critto due messaggi a caso, questi cosi sono indipendenti. già non vale per più messaggi\".\n\n\n\n# lec 30/10\n","x":4825,"y":-904,"width":759,"height":1966},
		{"id":"b0b21b2c6ac681ec","x":3816,"y":-900,"width":759,"height":925,"type":"text","text":"# Dimostrazioni"},
		{"id":"77613768479141c6","x":-220,"y":2168,"width":759,"height":832,"type":"text","text":"# Lec randomness"}
	],
	"edges":[
		{"id":"f7582924cfe4cd0b","fromNode":"bdd4244555c9c774","fromSide":"right","toNode":"3bb988d1bfcbf2a4","toSide":"left"},
		{"id":"d0da0d9d08c86080","fromNode":"3bb988d1bfcbf2a4","fromSide":"right","toNode":"80ff12045395380f","toSide":"left"},
		{"id":"6f82084d2bd83270","fromNode":"7cb40482ce744e2d","fromSide":"right","toNode":"6bcda7da0b8d4ae6","toSide":"left"},
		{"id":"ed747c984688f4dd","fromNode":"4294c40eec01cd5f","fromSide":"top","toNode":"a056b827849ac92f","toSide":"bottom"},
		{"id":"7c32388a92f57de3","fromNode":"4294c40eec01cd5f","fromSide":"left","toNode":"5c76b469ef30b644","toSide":"right"},
		{"id":"eb4167131dfe3c95","fromNode":"7cb40482ce744e2d","fromSide":"left","toNode":"4294c40eec01cd5f","toSide":"right"},
		{"id":"1dfebe1c0c250381","fromNode":"4294c40eec01cd5f","fromSide":"top","toNode":"0f1dad420fab77a0","toSide":"bottom"},
		{"id":"4b43d7149921f8c1","fromNode":"6bcda7da0b8d4ae6","fromSide":"right","toNode":"4466415b5eb73223","toSide":"left"},
		{"id":"d07809ac1b4b2fd4","fromNode":"6bcda7da0b8d4ae6","fromSide":"top","toNode":"b332de5fd82b6631","toSide":"bottom"},
		{"id":"c1eac968d623622a","fromNode":"4466415b5eb73223","fromSide":"top","toNode":"b0b21b2c6ac681ec","toSide":"bottom"}
	]
}