{
	"nodes":[
		{"id":"91f376c3ee9ec3f9","type":"text","text":"# note\n\ndifferenza tra unforgeability statistica e computazionale","x":6400,"y":661,"width":584,"height":268},
		{"id":"7cb40482ce744e2d","type":"text","text":"# Unbounded, Unconditional, Information-Theoretical\n\nSono le basi teoriche su cui poggia tutta la struttura. Di fatto è ***teoria dell'informazione***, di cui vediamo tre applicazioni:\n\n- ***Encryption*** -  $m$ è un ***messaggio in chiaro*** scelto tra tutti i possibili messaggi $\\calM$. Chiamiamo $M$ la generica ***distribuzione*** secondo la quale estraggo $m$ dal set $\\calM$. Stessa cosa con la ***chiave*** $k$ tra tutte le chiavi $\\calK$ estratta con distribuzione $K$ e con il ***ciphertext*** $c$ tra tutti i $\\calC$ ottenuti con distribuzione $C$;\n- ***Hashing*** -  Al messaggio $m$ viene affiancato un ***tag*** $\\t$ (i.e. un ***hash*** del messaggio $m$), con l'idea che essendo difficile risalire alla chiave usata dalla funzione di Tag per Eve sia difficile associare il giusto tag $\\t'$ al messaggio $m'$ (che può essere sia una modifica di $m$ sia un nuovo messaggio scritto da Eve). Questa difficoltà è detta ***unforgeability***.\n- ***Randomness Extraction*** - Lavorare con distribuzioni uniformi è tanto necessario quanto difficile nella pratica. Un ***randomness extractor*** lavora con sorgenti reali che forniscono poca randomicità per provare a ricavarne quanta più possibile. ","x":-220,"y":260,"width":759,"height":802,"color":"6"},
		{"id":"bdd4244555c9c774","type":"text","text":"# Richiami di Probabilità\n\nSe $\\Omega$ è lo spazio di tutti i possibili eventi ed $E\\subseteq \\Omega$ è un singolo evento, la probabilità di $E$ è in generale definita come il rapporto tra la misura di $E$ e la misura di $\\Omega$. Abbiamo quindi che$$P(E\\cup F) = P(E) + P(F) - P(E\\cap F) \\leq P(E) + P(F)$$L'uguaglianza vale solo se $P(E\\cap F)=0$, ovvero se gli eventi $E$ ed $F$ sono ***mutuamente esclusivi***. In modo analogo possiamo scriverlo con gli insiemi, ovvero $E\\cap F = \\emptyset$.\n\n- Dato un set di eventi $\\{E_i\\}$ tali che $E_i\\cap E_j = \\emptyset$, se $\\bigcup_iE_i = F$  allora $\\{E_i\\}$ è una ***partizione*** per $F$ (i.e. $F$ è totalmente ricoperto da eventi disgiunti $E_i$);\n- Eventi mutuamente esclusivi sono ***dipendenti***, in quanto per definizione il verificarsi di un evento influenza il verificarsi dell'altro (i.e. se si verifica $E$ allora $F$ non può verificarsi);\n\t- Viceversa, $E$ ed $F$ sono ***indipendenti*** ($E\\perp F$ ) ***sse*** $P(E\\cap F) = P(E)\\cdot P(F)$.\n\nDefiniamo la ***probabilità condizionata*** come$$P(E|F)={P(E\\cap F)\\over P(F)}$$che significa cercare un elemento $\\in E$ dentro l'insieme $F\\subseteq\\Omega$. In pratica, $F$ diventa il nuovo $\\Omega$ ed $E\\cap F$ diventa il nuovo $E$. Da questa definizione seguono diverse cosette utili.\n\n- ***Teorema della Probabilità Totale*** - Sia $\\{F_i\\}$ una partizione per $\\Omega$. $P(E) = \\sum_i P(E|F_i) \\cdot P(F_i)$.\n\t- Una partizione per $\\Omega$ è $F_1 = F$ ed $F_2= F^C$, dove $F^C=\\{x\\in\\Omega\\,|\\,x\\notin F\\}$ è il ***complemento*** di $F$.\n\t- Consente di trovare una probabilità come somma delle probabilità sui singoli elementi di una partizione di $\\Omega$.\n- ***Teorema di Bayes*** - Consente di invertire la definizione di probabilità condizionata. Essendo $\\cap$ un operatore simmetrico abbiamo che $$P(E\\cap F)=P(F\\cap E) =P(E|F)\\cdot P(F)=P(F|E)\\cdot P(E)$$Da questo segue che$$P(F|E) = {P(E|F)\\cdot P(F)\\over P(E)}={P(E|F)\\cdot P(F)\\over \\sum_i P(E|F_i) \\cdot P(F_i)}$$","x":-8480,"y":-3360,"width":759,"height":868,"color":"4"},
		{"id":"3bb988d1bfcbf2a4","type":"text","text":"# Distribuzioni\n \n Seguono le distribuzioni che devi usare per calcolare la probabilità che\n \n- si verifichi uno tra due eventi mutuamente esclusivi, rispettivamente con probabilità $p$ (successo) e $q=1-p$ (insuccesso) - ***Distribuzione di Bernoulli***;\n- si verifichino $k$ successi su una sequenza di $n$ eventi di Bernoulli indipendenti - ***Distribuzione Binomiale*** avente espressione$$P(X=k)=\\binom{n}{k}p^kq^{n-k}$$\n\t- Quante prove $k$ devo fare prima di ottenere il primo successo? Sono date dalla ***Distribuzione Geometrica*** avente espressione$$P(X=k) = pq^{k-1}$$\n- si verifichino $k$ eventi (rari) in un lungo (formalmente $t\\to\\infty$) periodo di osservazione - ***Distribuzione di Poisson***, cioè$$P(X=k)={\\lambda^ke^{-\\lambda}\\over k!}$$\n\t- Quanto tempo devo aspettare tra due eventi di Poisson? Me lo dice la ***Distribuzione Esponenziale***$$pdf(x)=\\lambda e^{-\\lambda x}$$\n\n$\\lambda$ è in generale un rate di arrivo, mentre nel caso continuo bisogna sostituire la variabile discreta $k$ con una continua $x$. Questo non mi restituisce propriamente una probabilità di un evento, ma la ***probability density function*** (***pdf***), nel senso che su un dominio continuo il singolo valore ha una probabilità di verificarsi $=0$. La pdf ha quindi senso solo sotto integrale in un certo intervallo. La funzione integrale della pdf è detta ***cumulativa***.","x":-7471,"y":-3360,"width":759,"height":868,"color":"4"},
		{"id":"80ff12045395380f","type":"text","text":"# Somme e Prodotti di Variabili Casuali\n\nIndichiamo con $f_X(x)$ la *pdf* associata al processo $X$ scritta nella variabile $x$.\n\nSegue che $f_{XY}(x,y)$ è pdf associata alla ***joint probability***$$P_{XY}(x,y) = P(X=x,\\,\\, Y=y)=\\int_{\\Omega_X}\\int_{\\Omega_Y}f_{XY}(x,y)dxdy$$È possibile risalire alla $f_X$ tramite una ***marginalizzazione***, ovvero integrando su $y$$$f_X(x) = \\int_{\\Omega_Y}f_{XY}(x,y)dy$$e viceversa. Tutto questo serve a dimostrare che l'operatore ***valore atteso***$$E[X] := \\int_{\\Omega_X}xf_X(x)dx$$***commuta sulla somma di variabili, ma in generale non sul prodotto***, ovvero$$\\begin{array}\\\\ E[X+Y] = E[X]+E[Y]\\\\ \\\\ E[XY] \\neq E[X]\\,E[Y]\\leadsto E[XY] \\overset{X\\perp Y}{=} E[X]\\,E[Y]\\end{array}$$***Il valore atteso distribuisce sul prodotto se e solo se $X$ e $Y$ sono indipendenti***.\n\nPiù in generale, la pdf della somma di due variabili indipendenti è data dall'***integrale di convoluzione*** definito come$$f_{Z=X+Y}(z=x+y) = (f_X*f_Y)(z)\\int_{\\mathbb{R}}f_X(x)f_Y(z-x)dx$$mentre la formula per il prodotto esiste ma è inutilmente complicata e non la scrivo.","x":-6462,"y":-3360,"width":759,"height":868,"color":"4"},
		{"id":"a056b827849ac92f","type":"text","text":"# Dimostrazione Lemma di Equivalenza\n\nIndichiamo le definizioni equivalenti come $I$, $II$ e $III$ e dimostriamo che\n\n- $I\\so II$  : Prendi per buona la definizione di PS, usi Bayes e ti ritrovi con$$\\P\\big[M=m\\big]={\\P\\big[M=m,\\,C=c\\big]\\over \\P\\big[C=c\\big]}\\quad\\so\\quad\\P\\big[M=m,\\,C=c\\big]=\\P\\big[M=m\\big]\\cdot\\P\\big[C=c\\big]$$che è la definizione di variabili indipendenti.\n- $II\\so III$ : Fissa la distribuzione $M$ e due elementi $m\\in\\calM$ e $c\\in\\calC$ e considera$$\\P\\aq\\Enc(K,m)=c\\cq=\\P\\aq\\Enc(K,M)=c\\mid M=m\\cq=\\P\\aq C=c\\mid M=m\\cq$$dove ho sostituito $m$ con $M$ + la condizione che $M=m$ e notato che $\\Enc(K,M)=C$. Ma per ipotesi $m$ e $c$ sono indipendenti, quindi quella roba è solo $\\P\\aq C=c\\cq$. Facendo lo stesso ragionamento per un altro messaggio $m'$ ottengo la stessa cosa, i.e. sto dicendo che$$\\forall c\\in\\calC \\,\\,\\P[\\Enc(K,m)=c]=\\P[\\Enc(K,m')=c]$$che è proprio la condizione $III$.\n- $III\\so I$ : Manipoliamo con la probabilità totale e con Bayes$$\\prob{C=c}=\\sum_{m'}\\prob{C=c,\\,M=m'}=\\sum_{m'}\\prob{C=c\\mid M=m'}\\cdot\\prob{M=m'}$$Stesso gioco di prima, ma al contrario. $\\prob{C=c}$ diventa la probabilità di $\\Enc(K,M)=c$, che riassorbendo la condizione $M=m'$ diventa $\\Enc(K,m')=c$, che grazie alla condizione $III$ da cui partiamo possiamo rendere un $\\Enc(K,m)=c$. A questo punto essendo indipendente da $m'$ possiamo portarlo fuori dalla somma, ritrovandoci con$$\\prob{C=c} = \\prob{\\Enc(K,m)=c}\\sum_{m'}\\prob{M=m'} = \\prob{(K,M)=c\\mid M=m}=\\prob{C=c\\mid M=m}$$Ma a noi serve al contrario, quindi usiamo un Bayes al volo:$$\\prob{M=m\\mid C=c}\\cdot\\prob{C=c} = \\prob{C=c\\mid M=m}\\cdot\\prob{M=m}$$da cui otteniamo in definitiva$$\\prob{M=m}={\\prob{M=m\\mid C=c}\\cdot\\cancel{\\prob{C=c}}\\over\\cancel{\\prob{C=c\\mid M=m}}}\\stackrel{\\text{per il risultato di prima}}{=}\\prob{M=m\\mid C=c}$$","x":-2238,"y":-904,"width":759,"height":925,"color":"4"},
		{"id":"0f1dad420fab77a0","type":"text","text":"# Dimostrazione Teorema di Shannon\n\nPer assurdo, prendiamo $M$ uniforme e un qualsiasi $c$ tale che $\\P[C=c]>0$, i.e. posso ottenere $c$ come $\\Enc(k,m)$ per una qualche scelta di $k$ ed $m$, ed assumiamo $|\\calK|<|\\calM|$. Definiamo$$\\calM'=\\bigg\\{\\Dec(k,c)\\,\\text{ t.c. }\\,k\\in\\calK\\bigg\\}$$i.e. l'insieme di tutti i messaggi ricostruibili a partire da una chiave $k\\in\\calK$. È chiaro che $|\\calM'|\\le|\\calK|$: a parità di $c$, non posso ricostruire più messaggi $m'$ di quante sono le chiavi $k$. Segue che $$|\\calM'|\\le|\\calK|<|\\calM| \\quad\\so\\quad |\\calM'|<|\\calM|$$Essendo strettamente minore, deve esistere un messaggio $\\in\\calM$ ma $\\notin\\calM'$. Per tale $m$ abbiamo$$\\P\\big[M=m\\big]={1\\over|\\calM|}$$in quanto $\\in\\calM$, ma anche $$\\P\\big[M=m\\mid C=c\\big]=0$$\nin quanto $\\notin \\calM'$. Essendo queste due $\\P$ diverse non vale la condizione di PS, quindi non esiste un SKE tale che $|\\calK|<|\\calM|$.","x":-3247,"y":-504,"width":759,"height":525,"color":"4"},
		{"id":"5c76b469ef30b644","type":"text","text":"# One Time Pad (OTP) & Two-Times Security\n\nÈ un esempio di cifrario PS minimale (ma comunque poco pratico), in cui $|\\calK| = |\\calM|$. Poniamo$$\\calK=\\calM=\\calC=\\{0,1\\}^n$$\ne stabiliamo come schema $\\Pi$ $$\\Enc(k, m)=k\\oplus m \\quad\\so\\quad \\Dec(k,c)=k\\oplus c$$\ndove $\\oplus$ è l'operatore di $\\text{bitwise XOR}$. È facile verificare la *correctness* di questo SKE, visto che$$\\Dec(\\Enc(k,m))=\\Dec(k\\oplus m) = k\\oplus(k\\oplus m) = m \\quad(\\text{since }k\\oplus k = 1)$$\nE la PS? Proviamo a calcolare\n$$\\P[\\Enc(k,m)=c] = \\P[k\\oplus m = c] = \\P[k=m\\oplus c]=2^{-n}$$\nQuesto perché $m\\oplus c$ è una delle $2^n$ stringhe $\\{0,1\\}^n$, in modo equiprobabile. Ma essendo una costante che non dipende da $m$ otteniamo lo stesso risultato anche usando $m'$, per cui$$\\prob{\\Enc(k,m)=c} = \\prob{\\Enc(k,m)=c}\\quad \\forall m,m'\\in\\calM, \\,\\,\\forall c\\in\\calC$$che è esattamente la definizione equivalente $III$ di PS.\n\nSiamo contenti? ... meh. Questo perché ***la PS vale su un solo messaggio***, e non dice nulla sulla situazione in cui ne mando diversi. Ad esempio, OTP ha il problemino che se l'attaccante intercetta due diversi ciphertexts $c=k\\oplus m$ e $c'=k\\oplus m'$ può tranquillamente calcolare$$c\\oplus c' = k\\oplus m\\oplus k\\oplus m'= m\\oplus m'$$i.e. se mai dovessi risalire anche ad un solo messaggio $m$ potrei decrittare a cascata tutti gli altri messaggi cifrati con la stessa chiave $k$. Questo ci dice che ***OTP non è Two-Times Secure***.\n\nSi può dimostrare che ***in generale non esiste un SKE Two-Times Secure***. Come conseguenza, ogni SKE è PS solo se viene usato una sola volta, i.e. se la chiave cambia ad ogni messaggio.","x":-3247,"y":256,"width":759,"height":806,"color":"4"},
		{"id":"4294c40eec01cd5f","type":"text","text":"# Encryption and Decryption\n\nUno schema crittografico a chiave segreta (***Secret Key Encryption***, ***SKE***) è definito da una coppia di funzioni di ***encryption*** e ***decryption***, i.e.$$\\Pi = (\\text{Enc}, \\text{Dec)}\\quad\\text{t.c.}\\quad C=\\Enc(k, M)$$\nQuesta cosa vale sia per le distribuzioni $(C, M)$ che per gli elementi $(c, m)$ estratti secondo esse. In modo analogo si definisce la funzione inversa $\\Dec$ (nota che in un ***cifrario simmetrico*** anche $\\Dec$ usa $k$, mentre in uno ***asimmetrico*** usa un $k'\\ne k$) . Dal momento che è più facile tenere segreta una singola chiave rispetto ad un intero schema di cifratura, $\\Pi$ di norma è ***pubblico***. L'idea è che anche conoscendo $\\Pi$ sia ***troppo difficile risalire alla chiave usata*** per la comunicazione sicura.\n\nIl pilastro che definisce la crittografia come scienza lo mette ***Shannon*** nel 1950: una SKE è ***sicura*** se rispetta la condizione matematica di ***Perfect Secrecy*** (***PS***)$$\\P[M=m] = \\P[M=m\\mid C=c]$$i.e. conoscere il ciphertext $c$ non modifica in alcun modo la probabilità a priori del messaggio $m$. In altre parole, una SKE è sicura se guardare $c$ non permette di inferire nulla su $m$, tolto ovviamente quello che già sapevamo già prima di guardare $c$. Possiamo definire la PS in modo equivalente:$$\\text{PS}\\iff M\\text{ and }C\\text{ independent}\\iff \\forall m,m'\\in\\calM, \\,\\,\\forall c\\in\\calC \\,\\,\\P[\\Enc(K,m)=c]=\\P[\\Enc(K,m')=c]$$\nNella terza formulazione $\\P$ è una distribuzione su $K$ e $K$ è una distribuzione su $\\calK$. Queste definizioni sono generali: non c'è nessuna ***assunzione*** su un eventuale attaccante, pertanto la chiamiamo ***unconditional***. Le proposizioni di questo tipo sono appunto più generali, ma di norma sono difficili da realizzare nella pratica. La PS è talmente difficile da realizzare che il ***teorema di Shannon*** ci dice che data una qualsiasi SKE $\\Pi$ si ha che$$|\\calK|\\ge|\\calM|$$\nQuesto disastroso risultato ci dice che la SKE minimale ha $|\\calK| = |\\calM|$. Si chiama ***One Time Pad*** (***OTP***), e nonostante sia mostruosamente inefficiente viene ancora oggi utilizzato in alcune situazioni. Tutti gli altri SKE che utilizziamo hanno $|\\calK| < |\\calM|$, e pertanto non sono PS: il ciphertext $c$ rivelerà ***sempre*** qualcosa sul messaggio in chiaro $m$.","x":-2238,"y":256,"width":759,"height":806,"color":"6"},
		{"id":"b0b21b2c6ac681ec","type":"text","text":"# Dimostrazioni","x":2807,"y":-896,"width":759,"height":925,"color":"1"},
		{"id":"4466415b5eb73223","type":"text","text":"# Pairwise Independence\n\nAbbiamo definito la condizione di One-Time $\\e\\text{-SS}$, ma come costruiamo una $\\text{Tag}$ che la soddisfi? \n\nPartiamo dal presupposto che questo è un corso di matematica e che non sempre è facile trovare una logica lineare ai risultati che escono fuori. Quindi ci mettiamo l'anima in pace e definiamo anzitutto una famiglia di funzioni dalla quale prendere la magica $\\text{Tag}$ che realizza il Perfect MAC:\n$$\\calH=\\{h_k: \\calM\\to\\calT\\}_{k\\in\\calK}$$\nSe definiamo $p$ come numero primo, un esempio di siffatta funzione (parametrica) può essere$$h_{a,b}(m)=\\AQ am+b\\CQ_{\\text{ mod }p}\\quad\\text{dove }k=(a,b) \\in\\calK=\\Z^2_p \\quad\\text{e}\\quad\\calM=\\cal\\calT=\\Z_p$$Il senso è il seguente: la chiave è una coppia di interi $\\in[0,\\,p-1]$. Detto questo,\n\n- A parità di messaggio $m$, applicare $h_{a,b}$ e $h_{a',b'}$ restituisce $\\t$ e $\\t'\\ne\\t$ fintantoché $(a,b)\\ne(a',b')$;\n- A parità di $h_{a,b}$, l'azione sui messaggi $m$ ed $m'$ restituisce $\\t$ e $\\t'\\ne\\t$ fintantoché $m\\ne m'$.\n\nIn genere quello che cambia non è la chiave, ma il messaggio. Prendiamo quindi la seconda proprietà. Sarebbe bello produrre $\\t$ e $\\t'$ ***statisticamente scorrelati***: in questo modo, osservarli entrambi non permetterebbe di inferire alcuna informazione in più su $K$ rispetto allo scenario in cui se ne osservi una sola delle due. Questo si formalizza nella definizione ***Pairwise Independence***:$$\\forall\\, m,m'\\in\\calM : m\\ne m' \\quad\\so\\quad \\AT h_k(K,m), h_k(K,m')\\CT \\text{ è uniforme su }\\calT^2=\\calT\\times\\calT\\text{ se }K\\text{ è uniforme su }\\calK$$\nBello, ma quindi? C'è un teoremino che ci garantisce che se scegliamo una funzione di $\\Tag=h_k$ da una famiglia di funzioni Pairwise Independent allora $\\Tag$ è un ***Perfect MAC***, i.e.$$\\Tag(k, m) = h_k(m)\\quad\\so\\quad\\Tag\\text{ is One-Time }{1\\over|\\calT|}\\text{-Statistically Secure}$$Anche l'esempietto sopra in $\\Z_p$ è una famiglia Pairwise Independent. Dimostrazioni di queste due cose nei riquadri intorno.","x":2807,"y":264,"width":759,"height":798,"color":"4"},
		{"id":"a82c1a10ade4a3e5","type":"text","text":"### Message Authentication Codes\n\n\nTutto il punto del corso è capire cosa voglio garantire con questa funzione di tag, come la costruisco e sotto quali condizioni è sicura.\n\n\"correctness is implicit as long as Tag function is deterministic\". Non ha senso farla non deterministica, no? \"a volte ha senso\".\n\nunforgeability = should be hard to forge valid tag $\\t'$ on message $m'$ (se non conosco la chiave è difficile guessare il giusto hash, il che mi fa pensare che prima ho capito male qualcosa). E se conosco già un'associazione messaggio/tag? E se ne raccolgo un tot? Posso risalire alla chiave?\nma poi ovviamente ad ogni messaggio aggiungo un timestamp in modo da non fare mai due messaggi uguali, quindi per forza devo trovare la chiave, è difficile fare tipo \"prendo un pezzetto da una parte e uno dall'altra, tipo che il messaggio abc ha hash 65 e afg ha 69 e associo 6 ad a (tipo)\". \"AES non è un encryption scheme, ha proprietà diverse, poi certo, lo puoi usare per la secirity o per l'hashing\". ok unforgeability ti fa essere sicuro che eve non può scrivere messaggi e asociare un valid tag convincendo bob che il mess venga da alice.\n\ndef (statistical secure mac) - say $\\Pi =$ tag (perché di nuovo pi???). pi ha una $\\e$-statistical security (unforgeability) se $$\\forall\\,m,m'\\text{ t.c. }m\\ne m'\\quad \\forall\\,\\t,\\t'\\in\\calT\\so \\P[tag(K,m')=\\t'\\mid tag(K,m)]=\\t\\le\\e$$\nex: impossible to get \\e=0 (credo perché con probabilità $1\\over|\\calK|$ posso comunque guessare la chiave, lui dice meglio che con probabilità $1\\over|\\calT|$ posso guessare il giusto \\t che forgia). nota che però qua non c'è l'adversary, quindi ancora unconditional. questa def parla solo di un messaggio: se vedo un'associazione posso guessare un'altra giusta associazione con prob al più \\e. Se già i messaggi sono due non dice più nulla. \"alice, puoi taggare in sicurezza un solo messaggio, se ne tagghi due io me ne tiro fuori\". è una ONE-TIME definition. Si può anche scrivere la TWO-TIME definition ma non credo ci interessi. ok forging is ab\n\n- the notion is achievable\n- it's unefficient - thm: any t-time $\\l$-statistically secure tag has a key of length (i.e. al tempo t, una funzione che si propone di essere $\\l$-SS deve avere una chiave lunga almeno $(t+1)\\l$)\n\ndef pairwise independence\n\n\"sha has no key, it is public. pw independent hanno una chiave\"\nany family of hash functions \"prendo una chiave a caso, critto due messaggi a caso, questi cosi sono indipendenti. già non vale per più messaggi\".\n\n\n\n# lec 30/10\n","x":3816,"y":-900,"width":759,"height":1966},
		{"id":"b332de5fd82b6631","type":"text","text":"# \"Non-Perfect\" MAC & $t$-Time Security\n\nAnzitutto notiamo che è facile trovare un esempio in cui uno schema One-Time $\\e\\text{-SS}$ rivela più del quantitativo minimale di informazione. Se ad esempio vediamo che il tag è `hello`, lo spazio di ricerca potrebbe non essere \"`tutte le possibili sequenze di 5 caratteri ASCII`\" (il che corrisponderebbe a $\\e\\sim O(2^{-40})$), ma più probabilmente \"`tutte le possibili parole inglesi di 5 lettere`\". Inutile dire che questa osservazione, se corretta, riduce molto lo spazio di ricerca.\n\nInoltre, in uno schema One-Time $\\e\\text{-SS}$ osservare una coppia $(m, \\t)$ mi mette nelle condizioni di avere al più una probabilità $\\e$ di indovinare il tag $\\t'$ per il messaggio $m'$. Ma se ne raccolgo diverse?\n\nCome la ***Perfect Secrecy***, anche la One-Time $\\e\\text{-SS}$ è definita su una sola coppia di messaggi. Possiamo fare di meglio? In un certo senso sì. Ci sono delle definizioni più vaste che contemplano l'osservazione di $t$ coppie $(m,\\t)_i$ e in cui la probabilità resta ancora$$\\forall(m,\\t)_{i=1,..., t}, \\forall m'\\in \\calM/\\{m_i\\}, \\forall\\t'\\in\\calT\\quad\\Prob{\\text{Tag}(K, m')=\\t' \\Bigm| \\text{Tag}(K, m_i)=\\t\\quad\\forall i\\in[1, t]}\\le\\e$$\n(***$t$-Time $\\e$-Statistical Security***, nota che l'avversario è libero di fare ciò che vuole, anche richiedere *ad hoc* di vedere proprio quelle coppie, i.e. è un risultato ***unconditional***), e si può ricavare che ogni funzione di Tag $t\\text{-Time }2^{-\\l}\\text{-SS}$ deve avere una chiave lunga almeno $(t+1)\\cdot\\l$, il che in pratica significa che per miglioramenti lineari su $\\e$ devo ***allungare esponenzialmente la chiave***. Scomodo.\n\nMa questo era giusto per completezza, non credo lo chieda.","x":1798,"y":-896,"width":759,"height":925,"color":"4"},
		{"id":"6bcda7da0b8d4ae6","type":"text","text":"# Message Authentication Codes (MACs)\n\nQui ignoriamo la secrecy e ci interessiamo solo all'***integrità*** del messaggio. Lo schema è semplice:\n\n- Alice usa una funzione $\\text{Tag}(k,m)$ per ottenere un ***tag*** $\\t$ associato al messaggio $m$. Tale funzione mappa $(k, m)$ in uno spazio a dimensione molto minore rispetto a quella di $m$, ed è (anche per questo) difficile da invertire (e.g. se $m$ è lungo `2048 byte` $\\t$ può essere lungo `256 bit`);\n- Alice invia in chiaro la coppia $(m, \\t)$;\n- Bob riceve $(m,\\t)$, prende $m$ ignorando $\\t$ e prova a fare la stessa cosa che ha fatto Alice: applica $\\text{Tag}(k,m)$ e ottiene $\\t'$ (stiamo assumendo che entrambi concordino sulla chiave $k$). A questo punto è facile: se $\\t=\\t'$ il messaggio è autentico, altrimenti è meglio buttarlo.\n\nEve in tutto questo entra in gioco nel secondo punto. Può anche leggere il messaggio, ma se prova a modificarlo ottenendo un $m'$ cambierà anche il tag. Già, ma in che modo? Per calcolarlo dovrebbe conoscere $k$, ma (hopefully) non è così. Quindi la cosa migliore che può fare è ***indovinare*** tra tutti i possibili tag. Se $|\\t|=$`256 bit` la probabilità di associare ad $m'$ il giusto $\\t'$ sono $2^{-256}$. Improbabile, ma comunque possibile (la probabilità non è *esattamente* zero).\n\nÈ per questo motivo che anche se trovassimo una funzione di $\\text{Tag}$ perfetta (***Perfect MAC***) ci sarebbe comunque una probabilità $1/|\\calT|$ che l'attaccante indovini il giusto $\\t'$. Più in generale diciamo che una funzione $\\text{Tag}$ è ***One-Time*** $\\e$***-Statistically Secure*** ($\\e\\text{-SS}$) se$$\\forall m,m'\\in\\calM\\,\\text{t.c. }m\\ne m',\\,\\,\\forall\\t,\\t'\\in\\calT\\qquad\\Prob{\\text{Tag}\\at K,m'\\ct=\\t'\\Bigm| \\text{Tag}\\at K,m\\ct=\\t}\\le \\e$$i.e. data l'osservazione di una coppia $(m,\\t)$, la probabilità di indovinare il tag $\\t'$ per un messaggio $m'\\ne m$ è al più $\\e$. Osservare $\\t$ mi sta quantomeno comunicando la lunghezza $|\\t|$ e l'alfabeto $\\calT$, che sono gli stessi per $\\t'$. Se sono bravo, queste sono le uniche informazioni che fornisco all'attaccante, ed $\\e=1/|\\calT|$ (al Perfect MAC corrisponde il minimo di $\\e$), altrimenti sarà maggiore.\n\nEve può quindi riuscire ad indovinare il tag $\\t'$ per il messaggio $m'$ con probabilità $\\ne0$. Se ci riesce, realizza una ***forgery*** (i.e. ha \"forgiato\" un tag come fosse Alice, e può quindi impersonarla). Uno schema che soddisfa la condizione di One-Time $\\e\\text{-SS}$ ha la proprietà di ***One-Time Unforgeability***.","x":1798,"y":264,"width":759,"height":798,"color":"6"},
		{"id":"763bb7889db17350","type":"text","text":"# Mini-Glossario\n\n- ***Correctness/Completeness*** - Se applico prima $\\Dec\\aq\\Enc(k,m)\\cq$ deve fare $m$, la stessa funzione di tag applicata allo stesso input deve restituire lo stesso hash, ecc...","x":789,"y":161,"width":550,"height":198},
		{"id":"c13f27d5a3a25a4d","type":"text","text":"# Alfabeti, Simboli, Linguaggi, Furfanti, Licantropi, ...\n\nBoh io ho trovato sta roba un po' confusionaria, quindi facciamo ordine.\n\nNei DMC siamo partiti da un esempio semplice, in cui $\\calX=\\calY=\\{0,1\\}$. Questa scelta di fatto costituisce la quasi totalità delle applicazioni reali. Perché? Ma perché tutto l'HW del mondo è basato sulla codifica binaria, è la codifica che trovi se intercetti una qualsiasi onda EM che trasmette dati. È chiaro che in genere vorrei trasmettere informazioni più complesse (tipo lettere, l'alfabeto inglese è in base $26$), ma l'HW parla in binario, quindi sono costretto a codificare in binario. Sia $\\calX$ che $\\calY$, quindi, nel quotidiano parlano binario.\n\n... ma quindi la trattazione generica con $\\calX\\ne \\calY$ serve solo al risultato formale? ***No***. Il fatto che la ***quasi*** totalità delle codifiche sia binaria non significa che lo siano tutte.\n\nQuello che può però trarre in inganno è l'abitudine ad usare i `bit`, che come abbiamo capito vengono molto facilmente confusi con i $\\bit$. Facciamo quindi un esempio più generale per capire che diavolo sta succedendo.\n\n$\\calX=\\{A, B, C\\}$, $\\calY=\\{\\a,\\b,\\g, \\d\\}$. Il canale potrebbe conoscere più simboli di quelli che l'input gli comunica, o viceversa. In entrambi i casi esce una matrice $W$ non quadrata, tipo\n$$W(Y|X)=\\begin{bmatrix}\nP(\\a \\mid A) & P(\\b \\mid A) & P(\\g\\mid A) & P(\\d\\mid A) \\\\\nP(\\a \\mid B) & P(\\b \\mid B) & P(\\g\\mid B) & P(\\d\\mid B) \\\\\nP(\\a \\mid C) & P(\\b \\mid C) & P(\\g\\mid C) & P(\\d\\mid C) \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.7 & 0.2 & 0.1 & 0.0 \\\\\n0.1 & 0.6 & 0.2 & 0.1 \\\\\n0.0 & 0.1 & 0.3 & 0.6 \\\\\n\\end{bmatrix}$$\nTutto questo ci sta dicendo che se viene inviato il simbolo $A$ è più probabile che in output venga ricevuto il simbolo $\\a$, e via dicendo. Visto che questa roba è suscettibile agli errori non è troppo conveniente inviare un simbolo alla volta. Scegliamo un codice $\\calC^2$, i.e. decidiamo che `AA` significa `A`, `BB` è `B` e `CC` è `C`. Questo ha due effetti:\n\n- Visto che $P(\\a\\mid A)=0.7$, diventa meno probabile che `AA` venga trasformato in due simboli di output che non sono $\\a$ $\\so$ ***ottengo robustezza rispetto all'errore***;\n- Al contempo per inviare un singolo simbolo $A$ ho dovuto inviarne due (`AA`) $\\so$ ***riduco l'efficienza della comunicazione***. Di quanto? Posso quantificarlo con il ***transmission rate***, che però malauguratamente è definito assumendo che $n$ sia la lunghezza della codeword in `bit`. Ma lasciamolo spiegare meglio al riquadro sul Transmission Rate.","x":780,"y":-6080,"width":759,"height":856,"color":"4"},
		{"id":"17e3d30a112f43b2","type":"text","text":"# \"Contenuto di Informazione per Simbolo\"\n\nQuando diciamo che un alto valore di $R$ implica un alta velocità di trasmissione, si intende che ogni singolo simbolo della codeword porta molta informazione.\n\nPrendiamo l'esempio delle `16` codeword con `4 bit`, i.e. il massimo possibile: ogni singolo `bit` è fondamentale per distinguere tra due codeword. Se ricevo `100` e manca l'ultimo bit, questo sta ancora decidendo al $50\\%$ tra `1000` e `1001`, da cui $R=1$ (perché posso fare questo stesso discorso per ogni `bit`.\n\nSe invece abbiamo l'alfabeto inglese e riceviamo `ABB`, l'ultimo `simbolo` magari porta $0$ informazione, perché esiste una sola codeword che inizia per `ABB` (e.g. `ABBA`). Chiaro, non tutti i simboli hanno un contenuto informativo pari a $0$, ma se su $26^4=456976$ codeword ne scelgo solo `16` verosimilmente queste saranno molto distanti, quindi basterà poco per discriminare tra esse. Magari il primo `simbolo` riduce le possibilità da `16` a `2`, mentre nel caso binario ogni `simbolo` (`bit`) prende una scelta al $50\\%$ su un albero binario.\n\nInsomma, nel primo caso ogni `bit` è una scelta con $\\P=0.5$ (massima entropia, da cui nel complesso uscirà $R=1$), mentre nel secondo caso il primo simbolo potrebbe avere $\\P=0.1$ e il  secondo $\\P=0.99$. Da qui otteniamo un'informazione media di $R\\simeq0.21 \\bit/$`simbolo`.","x":-227,"y":-5907,"width":759,"height":511,"color":"4"},
		{"id":"386f7dcf410dbf8b","type":"text","text":"# Correlazione, Rumore e Comunicazione\n\nPer come l'abbiamo definita, quella di Shannon è un'entropia ***marginale*** (i.e. riguarda solo una sorgente, e.g. $X$). Poniamo di averne un'altra, e.g. $Y$. Se vogliamo caratterizzare un evento il cui outcome consta di un pezzo che discende da $X$ ed uno che deriva invece da $Y$, ci sono altre grandezze che dobbiamo introdurre.\n\n- ***Conditional Entropy*** - Se $X$ è l'incertezza tra il lanciare un dado o una moneta e $Y$ un outcome da $1$ a $6$, è chiaro che conoscere il risultato di $X$ influenza la distribuzione di probabilità su $Y$. Quindi la domanda della conditional entropy è: se conosco $X$, qual è la sorpresa residua su $Y$? Matematicamente, questa roba si scrive$$H(Y|X)=\\sum_{x\\in \\calX}\\P(x)\\,H(Y|X=x)=-\\sum_{x,y}\\P(x,y)\\log_2\\P(y|x)$$\n- ***Mutual Information*** - Ma quanto esattamente conoscere $X$ riduce la sorpresa su $Y$?$$I(X;Y)=H(Y)-H(Y|X)=H(X)-H(X|Y)=I(Y;X)$$\n\t- Nota che è sempre $\\ge 0$, l'uguaglianza vale per variabili indipendenti.\n- ***Joint Entropy*** - Quantifica l'informazione media necessaria a descrivere nel complesso l'evento che consta delle due parti $X$ ed $Y$. L'informazione necessaria a descrivere $(X,Y)$ è pari a quella per conoscere $X$ più quella residua (nel senso di conditional entropy) per conoscere $Y$ (e viceversa, perché è chiaramente simmetrica):$$H(X,Y)=H(X)+H(Y|X)= H(Y)+H(X|Y)=-\\sum_{x,y}\\P(x,y)\\log_2\\P(x,y)$$\n- ***Cross Entropy*** - Date due distribuzioni $P$ e $Q$, rappresenta il numero medio di $\\bit$ che usi se codifichi dati generati da $P$ usando un codice ottimizzato per $Q$:$$H(P,Q)=-\\sum_xP(x)\\log_2Q(x)$$\n\t- Se vuoi quantificare la \"penalità\" in $\\bit$ che discende dallo star usando il codice sbagliato, cfr. ***Kullback-Leibler Divergence*** (i.e. $H(P,Q) - H(P)$).\n\nTutto questo mi serve a correlare due sorgenti in particolare: l'input e l'output di un ***Canale di Comunicazione***. Dato un input $x$, il ***rumore*** produrrà in generale un output $y\\ne x$. Segue che il mio compito di ascoltatore sarà di risalire a $x$ dato $y$.\n\nIn pratica, non possiamo a cuor leggero comprimere il messaggio da inviare fino al limite dato dall'entropia: ogni piccolo errore produrrebbe grandi incomprensioni. Per ottenere robustezza rispetto al rumore siamo costretti ad introdurre ***ridondanza*** rispetto alla compressione ottimale.","x":787,"y":-3770,"width":759,"height":1030,"color":"4"},
		{"id":"3b0496f7b217fd6b","type":"text","text":"# `Bit` Fisico vs $\\bit$ Informativo\n\n***Questa è una cosa molto importante.***\n\nLa Teoria dell'Informazione riconosce l'entità logica fondamentale $0/1$ come \"quanto\" di informazione, in quanto costituisce la risposta più semplice che si possa dare (i.e. scelta binaria). Tale quanto di informazione viene chiamato ***$\\bit$ informativo***.\n\n***Il $\\bit$ NON È il `bit`.*** \n\nIl `bit` inteso come entità che può assumere solo valori `0` e `1` si riferisce a questi due oggetti in quanto ***simboli*** di un ***alfabeto binario***. Potrebbero tranquillamente essere `A` e `B`. Insomma, ***i simboli non quantificano niente***.\n\nIpotizziamo che io faccia una domanda ad una macchina che risponde sempre `sì`.\n\n- Per recapitarmi la risposta, la macchina deve mandare un `bit` fisico;\n- Quando la risposta arriva, la mia sorpresa è esattamente $0$. So che rispondi sempre `sì`, questo `bit` che mi hai inviato non mi fornisce alcun $\\bit$ di informazione.\n\nOvviamente e malauguratamente ***questi due concetti a volte coincidono***. Se faccio la stessa domanda ad una macchina che risponde `sì` o `no` al $50\\%$, ogni `bit` che mi arriva in risposta coinciderà con esattamente un $\\bit$ di informazione.\n\nAltre volte è evidente che c'è qualcosa di strano. Se la macchina risponde `sì` il $75\\%$ delle volte e `no` il restante $25\\%$, ogni `bit` porta mediamente $0.81\\bit$ di informazione. Questa strana magia si chiama ***Entropia di Shannon***.\n\nMorale della favola: non tutti i `bit` che invii servono a comunicare qualcosa.","x":787,"y":-2505,"width":759,"height":695,"color":"4"},
		{"id":"93a2273f912d8274","type":"text","text":"# Capacità di Canale\n\nUn ***Discrete Memoryless Channel*** (***DMC***) è definito da una tripla $(\\calX, \\calY, W)$, dove $W$ è la matrice stocastica del ***rumore***. Segue quindi che è $W$ che caratterizza il canale. Possiamo definire un ***rate di trasmissione*** $R$ che ci dà due indicazioni complementari:\n\n- Quanto \"velocemente\" stiamo trasmettendo, i.e. data la lunghezza $n$ della codeword qual è il contenuto informativo (sorpresa) medio per ognuno degli $n$ `simboli`;\n- Quanto stiamo rischiando di non farci capire a destinazione (i.e. più il singolo `simbolo` è informativo, più una sua modifica dovuta al rumore modifica il senso del messaggio).\n\nOgni valore di $R$ rappresenta quindi un compromesso tra velocità e rischio di errori. Ci si potrebbe aspettare quindi che l'unico modo per far sì che l'errore tenda a $0$ è che $R\\to0$. \n\n... e invece no!\n\nAnzitutto $e\\to0$ lo puoi ottenere anche per $n\\to\\infty$. Al crescere di $n$, è più facile creare codeword distanti tra loro (in ogni caso $|\\calC^n|$ deve crescere esponenzialmente con $n$).\n\nA questo punto facciamo una piccola costruzione matematica e diciamo che un certo rate $R_0$ è ***achievable*** su un DMC $(\\calC^n,\\calY^n,\\calW^n)$ se esiste una sequenza di encoder-decoder $\\{\\calC^n,\\varphi^n\\}_{n\\in\\N}$ tali che per $n\\to\\infty$ abbiamo$$e(\\calC^n,\\calY^n,\\calW^n)\\to0\\qquad\\limsup_{n\\to\\infty}{1\\over n}\\log|\\calC^n|\\ge R_0$$i.e. se possiamo trovare un codice tale che l'errore tende a zero pur mantenendo $R\\ge R_0$.\n\nOra, tutto questo sarebbe solo una bella costruzione matematica se non fosse che esiste lo ***Shannon Capacity Theorem for DMCs*** che ci dice che il massimo rate achievable$$C(\\calW)=\\max\\{R\\mid R\\text{ è un achievable rate}\\}$$sul canale rumoroso caratterizzato da $\\calW$ è pari alla ***massima mutual information*** tra il segnale di input $X$ e quello di output $Y$, i.e.$$C^*(\\calW)=\\max_{\\P(Y|X)\\sim\\calW(Y|X)}I(Y|X)\\quad\\so\\quad C(\\calW)=C^*(\\calW)$$dove $\\P(Y|X)\\sim\\calW(Y|X)$ significa che la pdf di $Y$ dato $X$ è data dalla matrice di rumore (ma in pratica l'unica cosa che posso variare è $\\P(X)$, perché il rumore è caratteristico del canale!).\n\nNota che questo è un limite teorico. Data una matrice di rumore, esiste un massimo rate per il quale è safe comunicare senza rischio di errore. Non sta dicendo ***come*** scrivere encoder e decoder. La sfida \"ingegneristica\" qui è appunto trovare codici quanto più possibile vicini al limite teorico per ottimizzare la comunicazione.","x":1796,"y":-3770,"width":759,"height":1030,"color":"4"},
		{"id":"2c4d1b2523b3dc5f","type":"text","text":"# Esempi e Metafore per Convincersi\n\nIl `bit` fisico ha un ***costo reale*** (e.g. tempo, energia, banda). È ciò che _trasporti_.\n\nIl $\\bit$ informativo è un \"quanto siamo sorpresi\" da un simbolo. È un'entità **astratta** che si misura in base 2. È ciò che *comunichi davvero*.\n\n- $1 \\bit$ = `1` evento binario con probabilità $0.5$.   \n- Il rapporto tra `bit` e $\\bit$ la quantifica il ***transmission rate***:$$R = {\\bit \\text{ informativi}\\over \\text{bit fisici trasmessi}}\\le1$$\n- Nota che un ***simbolo*** in generale può portare più di un $\\bit$ di informazione. Se ad esempio uso l'alfabeto inglese con pdf piatta, ricevere una `K` mi genera una sorpresa pari a $\\log_2(26)\\sim4.7$.\n### **Metafora cinematografica** (By AI)\n\n- I bit **fisici** sono le **pellicole** che usi per registrare un film.\n- I bit **informativi** sono la **storia** che stai raccontando.\n    - Se filmi due ore di schermo nero, stai usando tantissima pellicola (bit fisici) ma non stai dicendo nulla (bit informativi ≈ 0).\n    - Se filmi un thriller dove ogni minuto succede qualcosa di imprevedibile, ogni metro di pellicola è carico di informazione.","x":1796,"y":-2505,"width":759,"height":695,"color":"4"},
		{"id":"0e888d4139ca4724","type":"text","text":"# Codici Continui e Rumore Gaussiano\n\nQuando comunico con un'onda EM lo schema concettuale non è \"`simbolo` che diventa un altro `simbolo` a causa del rumore\", piuttosto è un ***segnale continuo*** che viene ***distorto***.\n\nVista la proprietà additiva dell'ampiezza delle onde, il modello sul segnale diventa$$Y=X+Z$$dove $Z\\sim\\calN(0,\\s^2)$ è il ***rumore gaussiano***. i.e. mi aspetto che la distorsione media sia $0$ ma con un certo allargamento $\\s^2$. $X$ e $Z$ sono anche qui indipendenti.\n\nA questo punto dovremmo traslare tutto il discorso dell'entropia al caso continuo.$$h(Z)=-\\int f_Z(z)\\log\\big[f_Z(z)\\big]dz$$è detta ***entropia differenziale***, e guarda caso è massima per $Z\\sim\\calN(0,\\s^2)$. Il massimo vale$$h(Z\\sim\\calN(0,\\s^2))={1\\over2}\\log(2\\pi e\\s^2)$$Ora, per ***massimizzare la mutual information*** nel caso continuo$$I(X;Y)=h(Y)-h(Y|X)$$ho bisogno di massimizzare $h(Y)$. Quindi anche $Y$ deve essere gaussiana. Ma se $Y=X+Z$ e $Z$ è gaussiana allora ***devo scegliere $X$ gaussiana***. Così facendo abbiamo che$$h(Y|X)=h(X+Z\\mid X)=h(Z)$$(la sorpresa di $X$ dato $X$ è zero, dopodiché $X$ e $Z$ sono indipendenti), e facendo i conti$$I(X;Y)=h(Y)-h(Z)={1\\over2}\\log(2\\pi e\\s^2_Y)-{1\\over2}\\log(2\\pi e\\s^2_Y)=[...]={1\\over2}\\log\\bigg(1+{\\s^2_X\\over\\s^2_Z}\\bigg)$$\nDefiniamo quindi il ***Signal to Noise Ratio*** (***SNR***) e usando il ***teorema di Nyquist-Shannon*** sul campionamento senza perdita di informazione su canale con bandwidth $B$ (i.e. devi campionare almeno al doppio della frequenza dell'onda che trasporta l'informazione) arriviamo a formulare il ***teorema di Shannon-Hartley***$$\\SNR={\\s^2_X\\over\\s^2_Z}\\quad\\so\\quad C(\\calW)=B\\log_2(1+\\SNR)$$\nQuesto risultato fornisce la capacità massima del canale in $\\bit/s$.\n\nL'informazione di questo teorema è \"Più banda hai, più segnali puoi inviare. Più forte è il tuo segnale rispetto al rumore, più informazione puoi comprimere in ciascun segnale\".","x":2805,"y":-3770,"width":759,"height":1030,"color":"4"},
		{"id":"3afd2d36cd06d346","type":"text","text":"# Informazione, Sorpresa ed Entropia\n\nL'idea fondamentale alla base di questo modello è che gli eventi rari ci sorprendono più di quelli frequenti. Prendiamo ad esempio un evento binario i cui outcome hanno probabilità $\\P[A]=t$ e $\\P[B]=1-t=q$ e fissiamo $t=0.99$. Non saremo affatto sorpresi se si verifica l'evento $A$. In altri termini, il verificarsi di $A$ non costituisce un'***informazione*** significativa. Se però viceversa si verifica $B$ (che ha probabilità $q=0.01$) le cose cambiano: saremo sorpresi, e in un mare di eventi $A$ il verificarsi di $B$ sarà un'informazione significativa.\n\nIn particolare, più $q$ si avvicina a $0$ più il verificarsi di $B$ sarà un evento sorprendente. Viceversa, quando $q\\to1$ la sorpresa va ad annullarsi. Un buon modo per modellizzare a livello matematico questo comportamento è definire il ***contenuto informativo*** (o ***sorpresa***) di un evento $E$ che si verifica con probabilità $\\P[E]$ come$$\\I[E]=-\\log_2\\bigg(\\P[E]\\bigg)=\\log_2\\bigg({1\\over\\P[E]}\\bigg)$$\nIl logaritmo ha senso, quella base $2$ sembra un po' arbitraria ma sarà chiara a breve. Il punto è che adesso abbiamo la sorpresa per un evento, vediamo che succede se sommiamo su tutte le possibilità. Limitiamoci un attimo ad eventi binari e scriviamo l'***entropia binaria***$$h(t)=t\\log_2\\bigg({1\\over t}\\bigg)+(1-t)\\log_2\\bigg({1\\over 1-t}\\bigg)\\in[0,1]$$dove $t\\in(0,1)$ è la probabilità di uno dei due eventi. Possiamo facilmente estendere per continuità a $t\\in[0,1]$ definendo $h(0)=h(1)=0$, e per com'è scritta ha un massimo in $t=0.5$. Tale massimo vale $1$: la logica della base $2$ del logaritmo è quindi normalizzare la binary entropy. Essendo un oggetto del tipo \"somma di (sorpresa $\\times$ probabilità della sorpresa)\", l'entropia costituisce un ***valore atteso di sorpresa***. Di chi? Della ***sorgente di informazioni***!\n\nQuesto significa che possiamo caratterizzare il grado di imprevedibilità (o caos, se ci piace il parallelismo con la fisica) di una sorgente di eventi probabilistici tramite questa funzione: una sorgente è \"poco sorprendente\" se $t\\sim 0$ o $t\\sim1$ (non ha senso lanciare una moneta se esce sempre testa!), e \"molto sorprendente\" per $t\\sim 0.5$ (massima casualità).\n\nNota che in realtà posso estendere tutto questo anche a sorgenti non binarie. Sia $X$ una random variable (RV) che può assumere valori $\\{x_1, ..., x_n\\}$ con probabilità $p_i = \\P[X=x_i]$ (i.e. $X$ è una sorgente di informazioni). L'***entropia di Shannon*** di $X$ è data da$$H(X)=-\\sum_ip_i\\log_2(p_i)=\\sum_ip_i\\log_2\\bigg({1\\over p_i}\\bigg)\\in[0, \\log_2n]$$che è massima se la pdf è piatta (i.e. una moneta truccata ha meno entropia di una \"pulita\").\n\nRiassumendo: la ***sorpresa*** è legata all'***evento***, l'***entropia*** alla ***distribuzione***.","x":-220,"y":-3770,"width":759,"height":1030,"color":"4"},
		{"id":"192ac17eab8db376","type":"text","text":"# Teoria dell'Informazione\n\nLa Teoria dell'Informazione si occupa dello studio della quantificazione, dell'archiviazione e del trasferimento dell'informazione. Nasce da tre osservazioni sulla comunicazione:\n\n1. Il ***contenuto informativo*** di un messaggio equivale alla ***sorpresa*** che genera;\n2. In ogni ***linguaggio***, le parole di ***uso comune*** sono più corte di quelle non-comuni;\n3. Se ti perdi qualche pezzetto di frase, sei ancora in grado di ***ricostruire il messaggio***.\n\nDa questi tre principi, la modellizzazione matematica tira fuori tre risultati principali:\n\n1. Data una ***sorgente d'informazioni***, l'***Entropia di Shannon*** quantifica il valore atteso della ***sorpresa*** (i.e. del ***contenuto informativo***) del generico ***messaggio*** che essa può produrre, definendo il concetto di $\\bit$ informativo come unità di misura dell'informazione. Costituisce inoltre il limite inferiore alla ***compressione*** di un messaggio;\n- Dato un ***alfabeto*** di ***simboli*** (i.e. ***eventi*** o ***messaggi***), una ***codifica*** mira a trasformare tali simboli in sequenze di simboli (dette ***codeword***) di un altro alfabeto, detto ***codice***;\n\t- Se sembra non avere senso, considera che ***ASCII*** è una ***codifica binaria*** che trasforma simboli dell'alfabeto in sequenze di `bit`;\n\t2. Se utilizzo una ***codifica a lunghezza variabile*** posso applicare una ***compressione***, i.e. sfrutto la pdf della sorgente di simboli per assegnare ai messaggi più frequenti una rappresentazione più breve, e di conseguenza ottimizzare la lunghezza media della codeword. Si capisce anche meglio perché il suo limite inferiore è l'entropia: la compressione mira a ***minimizzare il numero di `bit` per ***$\\bit$;\n\t3. Se utilizzo una ***codifica a lunghezza fissa*** posso costruire codici appositamente per essere robusti rispetto al ***rumore***, utile se devo ***trasferire l'informazione***.","x":-220,"y":-2505,"width":759,"height":695,"color":"6"},
		{"id":"3cd35a2bc4b296c9","type":"text","text":"# Transmission Rate\n\nCos'è il Transmission Rate, e perché è un disastro che si chiami così? (spoiler: perché ci ho messo tre ore a capire che cazzo di trasmissione fosse).\n\nPariamo con il dire che, come al solito, l'esempio fatto con i `bit` può essere fuorviante. Prima ho detto che sceglievo come codeword `1010` e `0101`, dando per scontato che ogni simbolo fosse binario. Potrei benissimo scegliere `ABBA` e `BACA` avendo come alfabeto `A B C`. Non cambierà nulla per quello che segue. O quasi. Ci arriviamo.\n\nIl ***transmission rate*** non è una qualche forma di throughput. O meglio, un po' lo è, ma la definirei piuttosto un'***efficienza***. De che? Della comunicazione, ovviamente.$$R = {\\bit \\text{ informativi}\\over \\text{bit fisici trasmessi}}\\le1$$\nIn pratica, quanti ***bit informativi*** riesco a trasmettere per ogni bit fisico. Ora, nel caso in cui utilizziamo ***sequenze binarie*** di $n$ `bit` possiamo scrivere $R$ come$$R={1\\over n}\\log_2|\\calC^n|\\in[0,1]$$\nHai presente quella legge che ti impone di usare solo sequenze binarie? No, vero? Infatti non esiste. Questa scrittura è infatti un caso particolare di$$R={1\\over n\\log_2|\\calX|}\\log_2|\\calC^n|\\in[0,1]$$\nOk, cambia solo la normalizzazione, ma è importante. Poniamo $n=4$ e $|\\calC^n|=16$.\n\n- Se usiamo l'alfabeto binario, $\\log_2|\\calX|=1$. Questo significa che con $16$ parole binarie ho un'efficienza $R=1$, i.e. sto sfruttando tutte le potenzialità del mio alfabeto. Questo però significa anche che ogni pezzetto di informazione è esattamente dove deve essere, il che la rende una situazione molto fragile e poco robusta rispetto al rumore;\n- Se usiamo l'alfabeto inglese, $\\log_2|\\calX|\\simeq4.7$, quindi con $16$ parole inglesi di lunghezza $4$ ho un'efficienza $R\\simeq0.21$. Non sto sfruttando appieno l'alfabeto, è vero, ma di contro sarà molto più facile distinguere tra `xykz` e `abcd`.\n\nIn pratica, se $R$ è alto significa che sto trasmettendo messaggi molto compressi (codeword vicine tra loro, rischio di fare più errori), viceversa la trasmissione è lenta rispetto al potenziale dell'alfabeto utilizzato, ma è robusta rispetto agli errori.","x":-220,"y":-4920,"width":759,"height":922,"color":"4"},
		{"id":"763688f00992b7f1","type":"text","text":"# Discrete Memoryless Channel (DMC)\n\nPartiamo da uno schema semplice: invio `0` o `1` e il `bit` può essere flippato con probabilità $\\e$. Possiamo descrivere i possibili scenari dovuti al rumore con una ***matrice stocastica***\n$$\n\\calW(Y|X)=\\begin{bmatrix}\nP(0 \\mid 0) & P(1 \\mid 0) \\\\\nP(0 \\mid 1) & P(1 \\mid 1)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 - \\varepsilon & \\varepsilon \\\\\n\\varepsilon & 1 - \\varepsilon\n\\end{bmatrix}\n$$\n\nNessuno però trasmette un `bit` alla volta: in questo modo posso comunicare solo risultati di eventi $X Y$ binari, e il rumore potenzialmente ribalta quello che voglio comunicare.\n\nGeneralizziamo a situazioni in cui l'input consta di ***simboli*** $x\\in\\calX$ e l'output di altri simboli $y\\in\\calY$ (i.e. metti che il tuo alfabeto di partenza è `A B C` ma il rumore ti fa ricevere una `D`). Questo si traduce in una matrice $|\\calY|\\times|\\calX|$ $$\\calW(\\calY|\\calX)=\\P(y_i|x_j)$$Se ricevo una sequenza di $n$ simboli, posso risalire alla probabilità che questi siano esattamente quelli che erano stati inviati in input:$$W^n(Y=\\vec{y}\\mid X=\\vec x)=\\prod_{k=1}^n\\calW(y_k\\mid x_k)$$dove la produttoria indica la proprietà di ***assenza di memoria*** del canale.\n\nDefiniamo un ***encoder*** $\\calC^n$ come un subset $\\sse \\calX^n$ di tutte le possibili sequenze di `n bit` realizzabili sull'alfabeto $\\calX$: $c\\in\\calC^n\\in\\calX^n$ sono le ***codeword***, e vengono inviate sul DMC. In output, un ***decoder*** $\\varphi^n$ prova a ricondurre la sequenza $s\\in\\calY^n$ che osserva ad una $c\\in\\calC^n$. Un ***errore*** si verifica quando il decoder ricostruisce una codeword diversa da quella che era stata inviata dall'encoder.\n\nSbagliare è tanto più facile quanto più le codeword sono vicine. Se le uniche due codeword sono `1010` e `0101` ho scelto solo `2` messaggi su `16` stringhe disponibili (in particolare queste due sono ***ortogonali***), quindi sarà più difficile sbagliarsi. Viceversa, se ne uso `14` su `16` disponibili anche un solo `bit flip` cambierà il significato della parola. Né posso avere un $n$ stratosferico, perché significa spendere troppe risorse per ogni parola.\n\nBisogna quindi trovare un compromesso tra $n$ e $|\\calC^n|$, tramite il malauguratamente detto ***Transmission Rate*** (davvero eh Claude... un nome peggiore non potevi trovarlo).","x":787,"y":-4920,"width":759,"height":922,"color":"4"},
		{"id":"6728fea2c1660c65","type":"text","text":"# Abuso di Notazione\n\nNonostante \"encoder\" e \"decoder\" suggeriscano entità omologhe, $\\calC^n$ è un ***codice*** (quindi un subset di tutte le possibili stringhe realizzabili con $n$ simboli, i.e. $\\calC^n \\sse \\calX^n$) mentre $\\varphi^n$ è una ***funzione*** $\\varphi^n:\\calY^n\\to \\calC^n\\sse \\calX^n$.\n\nUn ***encoder*** in senso stretto dovrebbe essere una funzione $\\calE^n:\\calM\\to\\calC^n\\sse\\calX^n$ che mappa messaggi $m\\in\\calM$ in codeword $c\\in\\calC^n$.\n\nDa quello che leggo questo passaggio logico viene comunemente saltato per alleggerire la notazione, io non sono d'accordo. Chiamalo codice, no? Che ti costa?","x":1796,"y":-4610,"width":759,"height":303,"color":"4"},
		{"id":"3e7056fba0b7c79e","type":"text","text":"# Compressione (lossless) dell'Informazione\n\nPoniamo di dover comunicare in binario codificando ogni lettera dell'alfabeto (`a, ..., z`). Posto che potrei usare `5 bit` (devo contare fino a più di `16` ma a meno di `32`) scegliendo `a = 00000`, `b = 00001`, `...`, esiste un modo più ottimizzato per scegliere la codifica?\n\nAnzitutto, c'è un'importante scelta da fare:\n\n- Se non accetto di perdere informazione nel processo (***compressione lossless***) devo trovare una corrispondenza biunivoca tra il messaggio codificato e la sua decodifica. Per fare ciò mi serve una ***binary encoding*** di tipo ***prefix-free***;\n- Se accetto di perdere informazione nel processo (***compressione lossy***) posso permettermi di ridurre le dimensioni oltre i limiti della versione lossless.\n\nLimitiamoci alla compressione lossless e troviamo il limite di compressione.\n\nPosto di aver trovato un prefix-free encoding e di avere un set di probabilità $\\P$ associate alle parole di $\\calM$ (i.e. sappiamo qual è la frequenza attesa delle varie parole), definiamo$$l(f)=\\sum_{m\\in\\calM}\\P[m]\\,\\bigg|f(m)\\bigg|=\\sum_{m\\in\\calM}\\P[m]\\,l_m$$lunghezza media di $f$ (i.e. della stringa prodotta da $f$ su $m\\in\\calM$ secondo $\\P$). Capiamo bene che il problema originale coincide con il minimizzare $l(f)$. Ma andiamo con ordine. La ***disuguaglianza di Kraft*** stabilisce una relazione per il codici prefix-free:$$\\sum_{m\\in\\calM}2^{-l_m}\\le1$$\nChe significa? Se interpretiamo la codifica PF come un albero binario (ogni nodo è un prefisso, ogni ramo `0/1` porta ad un nuovo prefisso), ogni $f(m)$ è il cammino dalla radice ad una foglia. Kraft dice che se quella somma fa più di $1$, allora stai cercando di usare più foglie di quante ne esistono. Di conseguenza, almeno una $f(m)$ non sarà una foglia, i.e. sarà il prefisso di una qualche $f(m')$.\n\nQuesto lemma si usa per dimostrare il ***Teorema di Codifica della Sorgente***: l'***entropia*** è il ***limite inferiore della compressione lossless***, i.e.$$l(f)\\ge H(\\P)$$dove l'uguaglianza vale solo per distribuzione $\\P$ uniforme.\n\nOra, uno potrebbe pensare che questo limite sia più stringente del dovuto, perché abbiamo preso un encoding prefix-free invece di uno univocamente decodificabile. Invece no: il limite resta questo anche se rilasso quella condizione.\n","x":-1225,"y":-3770,"width":759,"height":1030,"color":"4"},
		{"id":"e0bb1df35ad0b538","type":"text","text":"# Breve Storia della Teoria dell'Informazione\n\nGià durante gli anni '20 ***Nyquist*** e ***Hartley*** avevano pubblicato studi circa la trasmissione di informazioni, giungendo a forme funzionali del tipo $A = \\log B$. L'informazione veniva definita come la ***capacità del ricevitore di distinguere una sequenza di simboli da qualsiasi altra***.\n\n Durante il suo lavoro come crittografo nel corso della WWII, ***Shannon*** comprende che la codifica di messaggi segreti implica di fatto l'aggiunta di rumore ingannevole ai messaggi originali, e che la decodifica consiste nella rimozione di tale rumore. È forse ripensando a questo che quando gli viene chiesto di ottimizzare la comunicazione su canali rumorosi inizia a pensare ad un'astrazione teorica per modellizzare il problema.\n\nNel ***1948*** compare un suo articolo in cui definisce così il processo della comunicazione:\n\n- Dato un insieme di possibili messaggi, una ***sorgente di informazione*** ne seleziona uno; \n- Un ***trasmettitore*** codifica questo messaggio in un ***segnale***;\n- Il segnale viene inviato attraverso un ***canale***, dove può essere corrotto dal ***rumore***;\n- Un ***ricevitore*** decodifica quindi il segnale ricevuto per ricostruire il messaggio originale.\n\nLa rivoluzione di questo modello è l'uso della ***probabilità*** per modellare sorgente e rumore. Di conseguenza, le grandezze qui definite (e.g. l'***entropia dell'informazione***) trovano facili paralleli con la ***meccanica statistica***, e le applicazioni della teoria nel suo complesso spaziano oggi dalla codifica del DNA umano alle grandi teorie di unificazione in fisica.\n\nL'articolo del 1948 segna convenzionalmente l'inizio dell'***Era dell'Informazione***.","x":-1225,"y":-2505,"width":759,"height":695,"color":"4"},
		{"id":"5aa40f1bf7e96287","type":"text","text":"# Variable Length Binary Encodings\n\nIn generale, il concetto di ***alfabeto*** è esteso ad un insieme $\\calX$ di ***simboli*** ($|\\calX|<\\infty$). $\\calM$ è l'insieme delle ***parole*** (o ***codewords***) di un certo ***codice*** costruito su $\\calX$ ($|\\calM|<\\infty$), ed $\\calM^*$ è l'insieme delle ***sequenze di parole*** $m\\in\\calM$ ($|\\calM^*|=\\infty$).\n\nQui ci limitiamo però ad un ***alfabeto binario***, quindi $\\calX=\\{0,1\\}$. Non solo: permettiamo parole di lunghezza variabile (perché stiamo facendo compressione, per i canali rumorosi $l$ si fissa).\n\nUn ***variable length binary encoding*** è una funzione iniettiva $f:\\calM\\to\\{0,1\\}^*$, i.e. ogni $m\\in\\calM$ è mappata in una sequenza binaria distinta da tutte le altre (non è biettiva perché $\\{0,1\\}^*$ è un insieme infinito, non è vero che ogni sequenza binaria codifica una $m\\in\\calM$).\n\nTrovare $f$ sembra sufficiente, ma solo se trasmetto una parola alla volta. Di norma voglio trasmetterne diverse, quindi devo trovare una $f^*:\\calM^*\\to \\{0,1\\}^*$ iniettiva. Il che ovviamente non è facile! Chiaro che $f^*(m^*)=f(m_1)...f(m_n)$, ma proprio per questo se codifico `A = 0`, `B = 1` e `C = 01`, la stringa `001` può voler dire sia `AAB` che `AC`!\n\nSegue che $f$ è ***univocamente decodificabile*** (UD) se $f^*$ è iniettiva. Quindi la domanda è: come si costruisce una $f^*$ iniettiva? Ci serve che nessuna coppia di parole $m,m'\\in\\calM$ sia codificata da $f$ in modo ambiguo, i.e. che $$\\forall\\, m,m'\\in\\calM \\,|\\, m\\ne m'\\to f(m)\\not\\trianglelefteq f(m')$$ovvero che per ogni coppia $m,m'$ $f(m)$ non sia un prefisso di $f(m')$. Se ciò si verifica, $f$ è detta ***codifica prefix-free*** (PF). Intuitivamente, $x$ è prefisso di $y$ se $\\exists\\,z\\in\\{0,1\\}^*$ t.c. $y=zx$. Le codifiche prefix-free sono un ***sottoinsieme*** di quelle UD. Pertanto, $$\\text{PF} \\so \\text{UD}\\qquad\\text{ma}\\qquad \\text{UD} \\not\\so \\text{PF}$$\nOk, ma come costruisco una codifica PF? Intanto la disuguaglianza di Kraft$$\\sum_{m\\in\\calM}2^{-l_m}\\le1$$fornisce una condizione sufficiente: se scelgo delle lunghezze $l_m$ tali che questa viene rispettata, allora esiste una codifica PF avente le $f(m)$ ti tali lunghezze. <span style=\"color:rgb(236, 155, 14)\">Qui è dove avrei voluto trovare un algoritmo reale, ma non ne ho voglia.</span>","x":-1225,"y":-4920,"width":759,"height":922,"color":"4"},
		{"id":"e0fdf60d48349d03","type":"text","text":"# Cryptography\n\nLa crittografia moderna si propone di risolvere principalmente due problemi:\n\n- ***Secrecy*** - Solo chi invia e chi riceve deve essere in grado di accedere al contenuto informativo del messaggio (i.e. ***confidentiality***). Questo di norma si risolve con la ***crittografia simmetrica***;\n- ***Authenticity/Integrity*** - Chi riceve deve essere sicuro che il messaggio gli giunga inalterato per come è stato scritto dal mittente atteso. Questo si risolve tipicamente con le ***funzioni di hash*** e con la ***crittografia asimmetrica***, e include due scenari:\n\t- Eve non deve essere in grado di modificare un messaggio di Alice per poi inoltrarlo a Bob facendogli credere che questo sia originale (***integrity***);\n\t- Eve non deve essere in grado di inviare un proprio messaggio facendo credere a Bob che questo provenga da Alice (***Authenticity***).\n\nEssendo la crittografia una scienza, si basa su proprietà statistico-matematiche dimostrabili. Bisogna però distinguere due \"livelli\" \n\n- I risultati più generali non fanno alcuna assunzione sulle capacità computazionali di chi attacca (***unconditional proofs***), ma producono schemi e algoritmi inefficienti dal punto di vista pratico. Si parla di definizioni, teoremi e dimostrazioni di tipo puramente ***information-theoretical***;\n\t- Questi risultati restano validi anche se l'attaccante dispone di un computer quantistico fault-tolerant che può girare per miliardi di anni. Chiaramente, dal punto di vista pratico non è necessario avere uno schema crittografico così forte.\n- Per ottenere risultati applicabili è spesso necessario aggiungere delle ***ipotesi ragionevoli*** su ciò che un attaccante è in grado di fare (***conditional proofs***). Queste ipotesi rendono le soluzioni ***computationally-bounded***, i.e. valgono fintantoché persistono certe condizioni computazionali (e.g. finché l'attaccante non dispone di una macchina non-deterministica).","x":-220,"y":2332,"width":759,"height":802,"color":"6"},
		{"id":"c51ba17f210187ea","type":"text","text":"# Conditional, Computationally-Bounded\n\nLe stesse cose di prima, ma nella pratica. Se prima potevamo fare\n\n- crypto: one time, |key|=|message|\n- integrity: same\n- randomness: recupera\n\nL'idea è che possiamo fare delle assunzioni e trovare metodi più comodi:\n\n- Adversary has limited computation power\n- there exist hard problems\n\nDa qui in poi schema tipico: Teorema dimostrazione del tipo\n- THM - Cryptosystem \\Pi is \"secure\" vs efficient adversaries if problem X is $\\NPH$;\n- PROOF - Assume \\pi  is not secure, so exists efficient machine B che \"rompe\" \\Pi, trova la contraddizione, tutti felici.\n\nVisto che non possiamo assumere a cuor leggero che $\\P\\ne\\NP$ dobbiamo quantomeno assumere l'esistenza delle one-way functions (easy to compute but hard to invert). Questa è l'assunzione minima per fare critto, poi magari tale owf è il factoring, se $\\P\\ne\\NP$. (anche se owf implicano $\\P\\ne\\NP$, e al contrario non sappiamo disproof l'ipotesi in cui $\\P\\ne\\NP$ e non esistono owf). Da qui nascono i diversi mondi di russel impagliazzo:\n\n- Algorithmic - $\\P=\\NP$ \n- heuristica - $\\P\\ne\\NP$ but no \"average hard puzzles\" (dove i puzzle sono i giochi su cui mi scervellavo l'altro giorno). \"\\*\" significa che puoi generare un puzzle se\n- pessiland - $\\P\\ne\\NP$ but no owf\n- minicrypt - owf\n- cryptomania - owf + public key crypto\n\nper raccontare questi mondi parla di gauss e della somma da 1 a N, e dice \"immagina che la storia continua e la maestra prova a trovare un problema che Gauss non può risolvere. la storia finisce in modi diversi a seconda del mondo in cui siamo:\n\n- algo - non può fare nulla perché non ci sono problemi difficili;\n- pessiland - può creare problema difficile ma lei stessa non sa la soluzione\n- ...\n- cryptomania - scenario migliore per l'insegnante perché posso implementare diffie-hellman e umiliare gauss condividendo la soluzione con l'intera classe in pubblico tranne gauss\"\n\ne mo che si fa? tocca fissare un modello di computazione: Turing Machines, per cui \"efficiente\" significa polynomial time for TM. Adv can use any (polynomial) amount of randomness (pure perfetta dai) -> PPT Adv (Probabilistic Poly-Time Adv), può essere un randomized algorithm\n\n- PPT - at any  step the TM can flip a coin and choose which computation to perform according to it\n\nseguono due possibili approcci:\n\n- concrete security - security holds wrt t-times TM except with prob at most \\e for concrete parameters (e.g. t=2^20 steps, \\e=2^-80). noioso, perché devi tenere traccia dei parametri;\n- asymptotic security - big or small è inteso in modo asintotico. prendo solo $\\l$ ed è il livello di sicurezza che voglio (e.g. $\\l=80$ sono `80 bit` di sicurezza, e.g. la lunghezza dei numeri primi usati in un cifrario factoring). Advs are Poly $\\l$-time TM, ed \\e sarà negligible = negl($\\l$)","x":-220,"y":4404,"width":759,"height":1616},
		{"id":"4cf6a821f24185ae","type":"text","text":"# SPACING","x":-960,"y":1535,"width":250,"height":235,"color":"5"},
		{"id":"69d937b1d939fb1f","type":"text","text":"# Randomness Extraction\n\nNel dare le definizioni teoriche e computazionali assumiamo spesso di avere a disposizione delle variabili distribuite in modo uniforme. Il mondo reale non offre sorgenti di randomicità pura: offre solo processi deterministici complessi al punto da *sembrare* casuali, ma che in realtà hanno un qualche grado di correlazione. Tali processi costituiscono le ***sorgenti deboli di randomicità***, o ***pseudocasuali***. Ad oggi fa eccezione solo la ***meccanica quantistica***: il collasso della funzione d'onda è una ***sorgente forte***, ma a meno di lavorare con i computer quantistici tocca capire come costruirne una che ci si avvicina sfruttando solo fenomeni pseudocasuali.\n\nPossiamo pensare che esista una funzione che prende in input un processo \"debole\" e lo trasforma in un processo \"forte\". Il modo più semplice per farlo è l'***estrattore di Von Neumann***: immaginiamo di avere una sorgente binaria sbilanciata $B$  tale che $\\prob{0}=p\\ne q=\\prob{1}$. Possiamo eliminare il bias con il seguente algoritmo:\n\n- Estrai due valori $b_1,b_2$ secondo $B$;\n- `if` $b_1=b_2$ non restituire nulla;\n- `else`: {`if` $(b_1=0, b_2=1)\\to$ `return 0`; `if` $(b_1=1, b_2=0)\\to$ `return 1`}\n\nLa cosa buona è che `0` e `1` adesso sono equiprobabili perché $\\prob{01}=\\prob{10}=pq$. La cosa cattiva è che c'è una probabilità $2\\cdot\\prob{00} = 2\\cdot\\prob{11} = 1-2pq$ di non estrarre niente. Pur vero che ad ogni step la probabilità cumulativa di non estrarre niente diminuisce esponenzialmente, è inefficiente. Inoltre stiamo assumendo che i lanci siano ***indipendenti***. In generale, in una sorgente debole potrei avere ***correlazione tra le varie estrazioni***. Non solo Von Neumann non basta: si può dimostrare che ***non esiste alcun estrattore in grado di generare un output uniforme a partire da una singola sorgente debole***. Questo vale sia per estrattori deterministici (risultato \"classico\", più semplice da provare) che non-deterministici (qui si generalizza al concetto di sorgente di Santha-Vazirani, è un risultato più profondo e di più difficile dimostrazione).\n\nUn modo per aggirare le difficoltà provate da questi risultati è usare due sorgenti: una debole (i.e. bassa min-entropy, correlazione, distribuzione non-uniforme) e una forte, che si traduce in un \"piccolo\" (rispetto alle dimensioni dell'output finale) ***seed***. Questo crea i ***seeded extractors***.","x":1798,"y":2332,"width":759,"height":802,"color":"6"},
		{"id":"df59b6a477d1cbb7","type":"text","text":"# Dimostrazioni\n\n(... de che???)","x":1798,"y":3369,"width":759,"height":800,"color":"1"},
		{"id":"523f6e733cf9d801","type":"text","text":"# Rényi Entropies\n\nShannon aveva definito la sua entropia come \"media\" su tutti i possibili outcome di una misura. Tale Alfréd Rényi nel 1961 si rende conto che possiamo vedere l'entropia di Shannon come caso particolare della definizione parametrica$$H_\\a(X) = {1\\over1-\\a}\\log\\AQ\\sum_xp(x)^\\a\\CQ\\qquad\\a \\ge 0,\\quad\\a\\ne1$$\nL'idea è usare $\\a$ per pesare quanto mi interessano le divergenze di $\\P$ rispetto alla distribuzione uniforme sullo stesso supporto. ... eh?! Intanto il ***supporto*** di una distribuzione $X$ è l'insieme di valori $x$ che hanno probabilità non nulla di essere estratti secondo $X$ (the more you know...). Detto questo, meglio procedere per esempi sui valori di $\\a$.\n\n- $\\a = 0$ - L'***Entropia di Hartley*** è definita come $H_0(X) = \\log\\aq\\text{supporto}(X)\\cq$, i.e. restituisce solo il $\\log$ del numero di outcome possibili. Nota che coinciderebbe con l'entropia di Shannon se $X$ fosse uniforme, ed è questo che intendevo prima. Pertanto, $H_0\\ge H_\\text{Shannon}$;\n- $\\a\\to1$ - ***Entropia di Shannon***, sorpresa media, $H_1(X)=-\\sum_xp(x)\\log\\aq p(x)\\cq=H_\\text{Shannon}(X)$;\n- $\\a = 2$ - La ***Collision Entropy*** è legata alla sorpresa media relativa all'evento \"faccio due estrazioni indipendenti ottenendo lo stesso $x$\" (o meglio, essendo $H_2(X)=-\\log\\sum_xp(x)^2$  sarebbe formalmente $H_2(X) = \\P(X=Y)$, dove $Y$ è distribuita esattamente come $X$). Intuitivamente, questo evento ha un'entropia più bassa rispetto alla media: $H_1\\ge H_2$;\n- $\\a\\to\\infty$ - La ***Min-Entropy*** è intuitivamente quella più piccola, ed è legata alla probabilità che una singola misura restituisca l'output più probabile. Una distribuzione con un picco di probabilità molto alto ha una $H_\\infty(X) = \\min_x\\aq-\\log p_x\\cq=-\\log\\max_x\\AT\\prob{X=x}\\CT$ molto bassa.\n\nDate queste definizioni, si possono dimostrare due proprietà:$$H_0\\ge H_1\\ge H_2\\ge H_\\infty\\quad \\text{;}\\quad H_\\a\\le2H_\\infty \\quad\\a>1$$\nLa prima ci aiuta a capire per quale motivo ***ai fini della randomness extraction è generalmente più importante avere una alta $H_\\infty$ piuttosto che una alta $H_1$***: mi interessa cosa fa il mio avversario nel caso peggiore, non nel caso medio (che poi, viste le disuguaglianze, sarà comunque \"buono\").","x":1798,"y":1297,"width":759,"height":802,"color":"4"},
		{"id":"9afef0e97ec9d7f2","type":"text","text":"Se hai tempo anche [questo](https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy#R%C3%A9nyi_divergence) è interessante","x":979,"y":1652,"width":380,"height":90,"color":"1"},
		{"id":"98f298d92a403bc8","type":"text","text":"# Seeded Extractors","x":2807,"y":2332,"width":759,"height":802,"color":"3"},
		{"id":"2bc8cb7ada9ead31","type":"text","text":"# Leftover Hash Lemma","x":2807,"y":1297,"width":759,"height":802,"color":"3"},
<<<<<<< HEAD
		{"id":"7a5fa1ef00051172","x":-1240,"y":4404,"width":759,"height":371,"type":"text","text":"# Signora mia, in che mondo viviamo..."},
		{"id":"c614bd5dab277881","x":3816,"y":1297,"width":759,"height":802,"color":"1","type":"text","text":"# Dimostrazione Leftover Hash Lemma\n"},
		{"id":"c36a321426aa0b61","type":"text","text":"# 3/10 - Zoom è una merda\n\nproof di leftover hash in due step: prima un altro lemma e poi quello. visto che sono entrato dopo mezz'ora (perché zoom fa schifo) guarda appunti crypto negativo.\n\n\n\n","x":5220,"y":1297,"width":759,"height":802}
=======
		{"id":"9890179548199db4","type":"text","text":"\n","x":-1698,"y":2548,"width":518,"height":272}
>>>>>>> 2c01a07440787c6c2695670cba227570ffbdbfe8
	],
	"edges":[
		{"id":"f7582924cfe4cd0b","fromNode":"bdd4244555c9c774","fromSide":"right","toNode":"3bb988d1bfcbf2a4","toSide":"left"},
		{"id":"d0da0d9d08c86080","fromNode":"3bb988d1bfcbf2a4","fromSide":"right","toNode":"80ff12045395380f","toSide":"left"},
		{"id":"6f82084d2bd83270","fromNode":"7cb40482ce744e2d","fromSide":"right","toNode":"6bcda7da0b8d4ae6","toSide":"left","label":"Compressione Lossy"},
		{"id":"ed747c984688f4dd","fromNode":"4294c40eec01cd5f","fromSide":"top","toNode":"a056b827849ac92f","toSide":"bottom"},
		{"id":"7c32388a92f57de3","fromNode":"4294c40eec01cd5f","fromSide":"left","toNode":"5c76b469ef30b644","toSide":"right"},
		{"id":"eb4167131dfe3c95","fromNode":"7cb40482ce744e2d","fromSide":"left","toNode":"4294c40eec01cd5f","toSide":"right","label":"Comunicazione"},
		{"id":"1dfebe1c0c250381","fromNode":"4294c40eec01cd5f","fromSide":"top","toNode":"0f1dad420fab77a0","toSide":"bottom"},
		{"id":"4b43d7149921f8c1","fromNode":"6bcda7da0b8d4ae6","fromSide":"right","toNode":"4466415b5eb73223","toSide":"left"},
		{"id":"d07809ac1b4b2fd4","fromNode":"6bcda7da0b8d4ae6","fromSide":"top","toNode":"b332de5fd82b6631","toSide":"bottom"},
		{"id":"c1eac968d623622a","fromNode":"4466415b5eb73223","fromSide":"top","toNode":"b0b21b2c6ac681ec","toSide":"bottom"},
		{"id":"de0905097534438e","fromNode":"c13f27d5a3a25a4d","fromSide":"bottom","toNode":"3cd35a2bc4b296c9","toSide":"top"},
		{"id":"aa172d1a703fba07","fromNode":"763688f00992b7f1","fromSide":"top","toNode":"c13f27d5a3a25a4d","toSide":"bottom"},
		{"id":"59ea6a8baeb3b2bb","fromNode":"17e3d30a112f43b2","fromSide":"right","toNode":"c13f27d5a3a25a4d","toSide":"left","fromEnd":"arrow"},
		{"id":"6def3da1a9075a78","fromNode":"3cd35a2bc4b296c9","fromSide":"top","toNode":"17e3d30a112f43b2","toSide":"bottom","fromEnd":"arrow"},
		{"id":"7fdd4611c113db60","fromNode":"763688f00992b7f1","fromSide":"left","toNode":"3cd35a2bc4b296c9","toSide":"right"},
		{"id":"bf8c59993637058e","fromNode":"763688f00992b7f1","fromSide":"right","toNode":"6728fea2c1660c65","toSide":"left"},
		{"id":"e20c965fc75b3c44","fromNode":"763688f00992b7f1","fromSide":"right","toNode":"93a2273f912d8274","toSide":"left"},
		{"id":"6f0aee89a95e505e","fromNode":"386f7dcf410dbf8b","fromSide":"top","toNode":"763688f00992b7f1","toSide":"bottom"},
		{"id":"357d32acc4432eb7","fromNode":"3afd2d36cd06d346","fromSide":"left","toNode":"3e7056fba0b7c79e","toSide":"right"},
		{"id":"a102f39314093d4c","fromNode":"3afd2d36cd06d346","fromSide":"right","toNode":"386f7dcf410dbf8b","toSide":"left"},
		{"id":"35a1505fb2a81f2a","fromNode":"192ac17eab8db376","fromSide":"top","toNode":"3afd2d36cd06d346","toSide":"bottom","label":"Sorpresa"},
		{"id":"6961f1ac88ec67d5","fromNode":"386f7dcf410dbf8b","fromSide":"right","toNode":"93a2273f912d8274","toSide":"left"},
		{"id":"b4e602e168509e5a","fromNode":"192ac17eab8db376","fromSide":"top","toNode":"386f7dcf410dbf8b","toSide":"bottom","label":"Comunicazione"},
		{"id":"d90dee706b9cb11b","fromNode":"93a2273f912d8274","fromSide":"right","toNode":"0e888d4139ca4724","toSide":"left"},
		{"id":"7249187f7fdb9f3e","fromNode":"192ac17eab8db376","fromSide":"left","toNode":"e0bb1df35ad0b538","toSide":"right"},
		{"id":"e882421c376a363c","fromNode":"192ac17eab8db376","fromSide":"top","toNode":"3e7056fba0b7c79e","toSide":"bottom","label":"Compressione"},
		{"id":"696c1f512f65cf02","fromNode":"192ac17eab8db376","fromSide":"right","toNode":"3b0496f7b217fd6b","toSide":"left"},
		{"id":"f2119d85dd86c713","fromNode":"3b0496f7b217fd6b","fromSide":"right","toNode":"2c4d1b2523b3dc5f","toSide":"left"},
		{"id":"91d16b92439c792b","fromNode":"5aa40f1bf7e96287","fromSide":"bottom","toNode":"3e7056fba0b7c79e","toSide":"top"},
		{"id":"00ccb24228393067","fromNode":"7cb40482ce744e2d","fromSide":"top","toNode":"192ac17eab8db376","toSide":"bottom","label":"From IoT"},
		{"id":"148c259921d3ef5c","fromNode":"e0fdf60d48349d03","fromSide":"top","toNode":"7cb40482ce744e2d","toSide":"bottom"},
		{"id":"3b386d8eaeadb31a","fromNode":"e0fdf60d48349d03","fromSide":"bottom","toNode":"c51ba17f210187ea","toSide":"top"},
		{"id":"ce15c385d49ebb05","fromNode":"7cb40482ce744e2d","fromSide":"bottom","toNode":"69d937b1d939fb1f","toSide":"left","label":"Sorpresa"},
		{"id":"30bdfcf699ed9ece","fromNode":"c51ba17f210187ea","fromSide":"top","toNode":"69d937b1d939fb1f","toSide":"left"},
		{"id":"bf4d07cb46794d27","fromNode":"e0fdf60d48349d03","fromSide":"right","toNode":"69d937b1d939fb1f","toSide":"left"},
		{"id":"46fd2daefe5925b6","fromNode":"523f6e733cf9d801","fromSide":"bottom","toNode":"69d937b1d939fb1f","toSide":"top"},
		{"id":"d9206018f44c7e22","fromNode":"69d937b1d939fb1f","fromSide":"bottom","toNode":"df59b6a477d1cbb7","toSide":"top"},
		{"id":"b27a71039ff34ecf","fromNode":"523f6e733cf9d801","fromSide":"left","toNode":"9afef0e97ec9d7f2","toSide":"right"},
		{"id":"81e704bfb58cf8ee","fromNode":"69d937b1d939fb1f","fromSide":"right","toNode":"98f298d92a403bc8","toSide":"left"},
		{"id":"f2a79c0059053230","fromNode":"69d937b1d939fb1f","fromSide":"right","toNode":"2bc8cb7ada9ead31","toSide":"left"},
		{"id":"e43dfed393c89595","fromNode":"523f6e733cf9d801","fromSide":"right","toNode":"2bc8cb7ada9ead31","toSide":"left"},
		{"id":"adb4fd864b661680","fromNode":"4466415b5eb73223","fromSide":"bottom","toNode":"2bc8cb7ada9ead31","toSide":"top"},
		{"id":"c99aaac60bff60c5","fromNode":"98f298d92a403bc8","fromSide":"top","toNode":"2bc8cb7ada9ead31","toSide":"bottom"},
		{"id":"7ca0c5955f9b3735","fromNode":"2bc8cb7ada9ead31","fromSide":"right","toNode":"c614bd5dab277881","toSide":"left"}
	]
}